{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_collection = \"MSMARCO\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown\n",
    "import ir_datasets\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "if chosen_collection not in [\"vaswani\", \"MSMARCO\"]:\n",
    "    raise ValueError(\"chosen_collection must be one of ['vaswani', 'MSMARCO']\")\n",
    "\n",
    "if chosen_collection == \"MSMARCO\":\n",
    "    url_collection = 'https://drive.google.com/uc?id=1_wXJjiwdgc9Kpt7o7atP8oWe-U4Z56hn'\n",
    "    \n",
    "    if not os.path.exists('../MSMARCO.tsv'):\n",
    "        gdown.download(url_collection, 'MSMARCO.tsv', quiet=False)\n",
    "\n",
    "    #df = pd.read_csv('../collection.tsv', sep='\\t', names=['doc_id', 'text'])\n",
    "   \n",
    "\n",
    "elif chosen_collection == \"vaswani\":\n",
    "    vaswani_dataset = ir_datasets.load(\"vaswani\")\n",
    "    docs = list(vaswani_dataset.docs_iter())\n",
    "    df = pd.DataFrame(docs)\n",
    "    df['doc_id'] = (df['doc_id'].astype(int) - 1).astype(str)\n",
    "    # rimuovi i \\n da ogni documento\n",
    "    df['text'] = df['text'].str.replace('\\n', ' ')\n",
    "    df.to_csv('vaswani.tsv', sep='\\t', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "# import string\n",
    "# import nltk\n",
    "# import re\n",
    "# from tqdm import trange\n",
    "\n",
    "# nltk.download(\"stopwords\", quiet=True)\n",
    "# STOPWORDS = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "# STEMMER = nltk.stem.PorterStemmer()\n",
    "\n",
    "# def analyze_collection(documents):\n",
    "#     char_counter = Counter()\n",
    "#     stopword_counter = Counter()\n",
    "#     contains_acronyms = 0\n",
    "#     total_words = 0\n",
    "#     total_docs = len(documents)\n",
    "\n",
    "#     for doc in trange(total_docs, desc=\"Processing documents\"):\n",
    "#         # Assicurati che 'doc' sia una stringa e non un indice\n",
    "#         document_text = documents.iloc[doc]  # Ottieni il testo del documento tramite l'indice\n",
    "        \n",
    "#         # Conteggio dei caratteri\n",
    "#         char_counter.update(list(document_text))  # Conta i caratteri del testo\n",
    "        \n",
    "#         # Stopword e parole totali\n",
    "#         words = document_text.lower().split()\n",
    "#         total_words += len(words)\n",
    "#         stopword_counter.update(word for word in words if word in STOPWORDS)\n",
    "        \n",
    "#         # Controllo acronimi\n",
    "#         if re.search(r\"\\b\\w+\\.\\w+\\.\\b\", document_text):  # Cerca pattern tipo \"U.S.A.\"\n",
    "#             contains_acronyms += 1\n",
    "\n",
    "#     # Filtra solo i caratteri speciali\n",
    "#     special_chars = {ch: count for ch, count in char_counter.items() if ch in string.punctuation or not ch.isalnum()}\n",
    "\n",
    "#     # Calcola le statistiche\n",
    "#     percent_documents_with_acronyms = (contains_acronyms / total_docs) * 100\n",
    "#     percent_special_chars = (sum(special_chars.values()) / sum(char_counter.values())) * 100\n",
    "#     percent_stopwords = (sum(stopword_counter.values()) / total_words) * 100\n",
    "\n",
    "#     return {\n",
    "#         \"special_chars\": special_chars,\n",
    "#         \"contains_acronyms\": contains_acronyms,\n",
    "#         \"percent_documents_with_acronyms\": percent_documents_with_acronyms,\n",
    "#         \"percent_special_chars\": percent_special_chars,\n",
    "#         \"percent_stopwords\": percent_stopwords\n",
    "#     }\n",
    "\n",
    "# # Esegui l'analisi sulla colonna 'text' di df\n",
    "# result = analyze_collection(df['text'])\n",
    "\n",
    "# # Stampa i risultati\n",
    "# print(\"Special characters:\", result[\"special_chars\"])\n",
    "# print(f\"Documents with acronyms: {result['contains_acronyms']} ({result['percent_documents_with_acronyms']:.2f}%)\")\n",
    "# print(f\"Percentage of special characters: {result['percent_special_chars']:.2f}%\")\n",
    "# print(f\"Percentage of stopwords: {result['percent_stopwords']:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import Stemmer\n",
    "STEMMERNEW = Stemmer.Stemmer('english')\n",
    "\n",
    "\n",
    "\n",
    "nltk.download(\"stopwords\", quiet=True)\n",
    "STOPWORDS = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "#STEMMER = nltk.stem.PorterStemmer()\n",
    "\n",
    "\n",
    "def preprocess(s):\n",
    "    # lowercasing\n",
    "    s = s.lower()\n",
    "    # ampersand and special chars\n",
    "    s = re.sub(r\"[‘’´“”–-]\", \"'\", s.replace(\"&\", \" and \")) # this replaces & with 'and' and normalises quotes\n",
    "    # acronyms\n",
    "    #s = re.sub(r\"\\.(?!(\\S[^. ])|\\d)\", \"\", s) # this removes dots that are not part of an acronym\n",
    "    # remove punctuation\n",
    "    s = s.translate(str.maketrans(string.punctuation, \" \" * len(string.punctuation)))\n",
    "    # strip whitespaces\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    # tokenisation\n",
    "    tokens = [t for t in s.split() if t not in STOPWORDS]\n",
    "    # stemming\n",
    "    #tokens = [STEMMER.stem(t) for t in tokens]\n",
    "    \n",
    "    return STEMMERNEW.stemWords(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import Stemmer\n",
    "\n",
    "# Stemmer di PyStemmer (Porter2)\n",
    "pystemmer = Stemmer.Stemmer('english')\n",
    "\n",
    "# Stemmer Porter originale\n",
    "nltk_porter = PorterStemmer()\n",
    "\n",
    "# Stemmer Porter2 di NLTK\n",
    "nltk_snowball = SnowballStemmer('english')\n",
    "\n",
    "# Parola di esempio\n",
    "word = \"general\"\n",
    "\n",
    "print(\"PyStemmer:\", pystemmer.stemWord(word))\n",
    "print(\"NLTK Porter:\", nltk_porter.stem(word))\n",
    "print(\"NLTK Snowball (Porter2):\", nltk_snowball.stem(word))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "\n",
    "\n",
    "\n",
    "nltk.download(\"stopwords\", quiet=True)\n",
    "STOPWORDS = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "STEMMER = PorterStemmer()\n",
    "\n",
    "\n",
    "def preprocess_old(s):\n",
    "    # lowercasing\n",
    "    s = s.lower()\n",
    "    # ampersand and special chars\n",
    "    s = re.sub(r\"[‘’´“”–-]\", \"'\", s.replace(\"&\", \" and \")) # this replaces & with 'and' and normalises quotes\n",
    "    # acronyms\n",
    "    #s = re.sub(r\"\\.(?!(\\S[^. ])|\\d)\", \"\", s) # this removes dots that are not part of an acronym\n",
    "    # remove punctuation\n",
    "    s = s.translate(str.maketrans(string.punctuation, \" \" * len(string.punctuation)))\n",
    "    # strip whitespaces\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    # tokenisation\n",
    "    tokens = [t for t in s.split() if t not in STOPWORDS]\n",
    "    # stemming\n",
    "    tokens = [STEMMER.stem(t) for t in tokens]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def profile(f):\n",
    "    def f_timer(*args, **kwargs):\n",
    "        start = time.time()\n",
    "        result = f(*args, **kwargs)\n",
    "        end = time.time()\n",
    "        ms = (end - start) * 1000\n",
    "        print(f\"{f.__name__} ({ms:.3f} ms)\")\n",
    "        return result\n",
    "    return f_timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import humanize\n",
    "import os\n",
    "\n",
    "def print_pickled_size(var_name, var):\n",
    "    # If the 'tmp' directory does not exist, we first create it\n",
    "    os.makedirs('./tmp', exist_ok=True)\n",
    "    with open(f\"./tmp/{var_name}.pickle\", 'wb') as f:\n",
    "        pickle.dump(var, f)\n",
    "    print(f'{var_name} requires {humanize.naturalsize(os.path.getsize(f\"./tmp/{var_name}.pickle\"))}')\n",
    "    os.remove(f\"./tmp/{var_name}.pickle\")\n",
    "    os.removedirs('./tmp')\n",
    "\n",
    "\n",
    "def vbyte_encode(number):\n",
    "    bytes_list = bytearray()\n",
    "    while True:\n",
    "        byte = number & 0x7F # Prendi i 7 bit meno significativi -> 0111 1111 = 0x7F\n",
    "        number >>= 7 # Shifta a destra di 7 bit\n",
    "        if number:\n",
    "            bytes_list.append(byte) # Aggiungo i 7 bit al risultato\n",
    "        else:\n",
    "            bytes_list.append(0x80 | byte) # Aggiungo i 7 bit con il bit di continuazione, 0x80 = 1000 0000\n",
    "            break\n",
    "    return bytes(bytes_list)\n",
    "\n",
    "def vbyte_decode(bytes_seq):\n",
    "    number = 0\n",
    "    for i, byte in enumerate(bytes_seq):\n",
    "        number |= (byte & 0x7F) << (7 * i)\n",
    "        if byte & 0x80:\n",
    "            break\n",
    "    return number\n",
    "\n",
    "def decode_concatenated_vbyte(encoded_bytes):\n",
    "    decoded_numbers = []\n",
    "    current_number = 0\n",
    "    shift_amount = 0\n",
    "    \n",
    "    for byte in encoded_bytes:\n",
    "        if byte & 0x80:  # Bit di continuazione trovato, fine del numero\n",
    "            current_number |= (byte & 0x7F) << shift_amount\n",
    "            decoded_numbers.append(current_number)\n",
    "            current_number = 0\n",
    "            shift_amount = 0\n",
    "        else:  # Continuo a comporre il numero\n",
    "            current_number |= (byte & 0x7F) << shift_amount\n",
    "            shift_amount += 7\n",
    "    \n",
    "    return decoded_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "@profile\n",
    "def build_index(file_path, batchsize=10000):\n",
    "    \"\"\"\n",
    "    Costruisce un indice invertito da un file TSV senza intestazione elaborato in batch,\n",
    "    mostrando una barra di avanzamento completa.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Percorso al file TSV senza intestazione.\n",
    "        batchsize (int): Numero di righe da elaborare per batch (default: 10000).\n",
    "        \n",
    "    Returns:\n",
    "        tuple: lexicon (vocabolario), indici invertiti (docids, freqs), doc_index (documenti e lunghezze), stats (statistiche globali).\n",
    "    \"\"\"\n",
    "    lexicon = {}\n",
    "    doc_index = []\n",
    "    inv_d, inv_f = {}, {}\n",
    "    termid = 0\n",
    "\n",
    "    num_docs = 0\n",
    "    total_dl = 0\n",
    "\n",
    "    # Calcolare il numero totale di righe/documenti\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        total_docs = sum(1 for _ in f)\n",
    "    \n",
    "    # Barra di avanzamento con totale definito\n",
    "    progress_bar = tqdm(desc=\"Indexing\", total=total_docs, unit=\"doc\", dynamic_ncols=True)\n",
    "    \n",
    "    # Leggere il file TSV in batch\n",
    "    for df in pd.read_csv(file_path, sep='\\t', header=None, names=['doc_id', 'text'], chunksize=batchsize):\n",
    "        for docid, (doc_id, doc_text) in enumerate(zip(df['doc_id'], df['text'])):\n",
    "            tokens = preprocess(doc_text)\n",
    "            token_tf = Counter(tokens)\n",
    "            #doclen = sum(token_tf.values())\n",
    "            for token, tf in token_tf.items():\n",
    "                if token not in lexicon:\n",
    "                    lexicon[token] = [termid, 0, 0]\n",
    "                    inv_d[termid], inv_f[termid] = [], []\n",
    "                    termid += 1\n",
    "                token_id = lexicon[token][0]  # prendo il termid\n",
    "                inv_d[token_id].append(num_docs)  # aggiungo il docid alla lista dei docid in cui compare il termine\n",
    "                inv_f[token_id].append(tf)  # aggiungo il tf alla lista dei tf in cui compare il termine\n",
    "                lexicon[token][1] += 1  # incremento il df\n",
    "                lexicon[token][2] += tf  # tf è quanto compare il termine nel documento\n",
    "            doclen = len(tokens)\n",
    "            doc_index.append((str(doc_id), doclen))\n",
    "            total_dl += doclen\n",
    "            num_docs += 1\n",
    "\n",
    "            # Aggiorna la barra di avanzamento\n",
    "            progress_bar.update(1)\n",
    "    \n",
    "    # Chiudere la barra di avanzamento\n",
    "    progress_bar.close()\n",
    "\n",
    "    # Compress the inv_d and inv_f lists\n",
    "    \"\"\"for term, (termid, df, _) in lexicon.items():\n",
    "        # Compress the docids\n",
    "        encoded_list = [vbyte_encode(x) for x in inv_d[termid]]\n",
    "        concatenated_encoded = b''.join(encoded_list)\n",
    "        assert inv_d[termid] == decode_concatenated_vbyte(concatenated_encoded), \"Compression/Decompression mismatch!\"\n",
    "        inv_d[termid] = concatenated_encoded\n",
    "        # Compress the frequencies\n",
    "        encoded_list = [vbyte_encode(x) for x in inv_f[termid]]\n",
    "        concatenated_encoded = b''.join(encoded_list)\n",
    "        assert inv_f[termid] == decode_concatenated_vbyte(concatenated_encoded), \"Compression/Decompression mismatch!\"\n",
    "        inv_f[termid] = concatenated_encoded\"\"\"\n",
    "\n",
    "    stats = {\n",
    "        'num_docs': num_docs,\n",
    "        'num_terms': len(lexicon),\n",
    "        'num_tokens': total_dl,\n",
    "    }\n",
    "    return lexicon, {'docids': inv_d, 'freqs': inv_f}, doc_index, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "@profile\n",
    "def build_index_old(file_path, batchsize=10000):\n",
    "    \"\"\"\n",
    "    Costruisce un indice invertito da un file TSV senza intestazione elaborato in batch,\n",
    "    mostrando una barra di avanzamento completa.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Percorso al file TSV senza intestazione.\n",
    "        batchsize (int): Numero di righe da elaborare per batch (default: 10000).\n",
    "        \n",
    "    Returns:\n",
    "        tuple: lexicon (vocabolario), indici invertiti (docids, freqs), doc_index (documenti e lunghezze), stats (statistiche globali).\n",
    "    \"\"\"\n",
    "    lexicon = {}\n",
    "    doc_index = []\n",
    "    inv_d, inv_f = {}, {}\n",
    "    termid = 0\n",
    "\n",
    "    num_docs = 0\n",
    "    total_dl = 0\n",
    "\n",
    "    # Calcolare il numero totale di righe/documenti\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        total_docs = sum(1 for _ in f)\n",
    "    \n",
    "    # Barra di avanzamento con totale definito\n",
    "    progress_bar = tqdm(desc=\"Indexing\", total=total_docs, unit=\"doc\", dynamic_ncols=True)\n",
    "    \n",
    "    # Leggere il file TSV in batch\n",
    "    for df in pd.read_csv(file_path, sep='\\t', header=None, names=['doc_id', 'text'], chunksize=batchsize):\n",
    "        for docid, (doc_id, doc_text) in enumerate(zip(df['doc_id'], df['text'])):\n",
    "            tokens = preprocess_old(doc_text)\n",
    "            token_tf = Counter(tokens)\n",
    "            #doclen = sum(token_tf.values())\n",
    "            for token, tf in token_tf.items():\n",
    "                if token not in lexicon:\n",
    "                    lexicon[token] = [termid, 0, 0]\n",
    "                    inv_d[termid], inv_f[termid] = [], []\n",
    "                    termid += 1\n",
    "                token_id = lexicon[token][0]  # prendo il termid\n",
    "                inv_d[token_id].append(num_docs)  # aggiungo il docid alla lista dei docid in cui compare il termine\n",
    "                inv_f[token_id].append(tf)  # aggiungo il tf alla lista dei tf in cui compare il termine\n",
    "                lexicon[token][1] += 1  # incremento il df\n",
    "                lexicon[token][2] += tf  # tf è quanto compare il termine nel documento\n",
    "            doclen = len(tokens)\n",
    "            doc_index.append((str(doc_id), doclen))\n",
    "            total_dl += doclen\n",
    "            num_docs += 1\n",
    "\n",
    "            # Aggiorna la barra di avanzamento\n",
    "            progress_bar.update(1)\n",
    "    \n",
    "    # Chiudere la barra di avanzamento\n",
    "    progress_bar.close()\n",
    "\n",
    "    # Compress the inv_d and inv_f lists\n",
    "    \"\"\"for term, (termid, df, _) in lexicon.items():\n",
    "        # Compress the docids\n",
    "        encoded_list = [vbyte_encode(x) for x in inv_d[termid]]\n",
    "        concatenated_encoded = b''.join(encoded_list)\n",
    "        assert inv_d[termid] == decode_concatenated_vbyte(concatenated_encoded), \"Compression/Decompression mismatch!\"\n",
    "        inv_d[termid] = concatenated_encoded\n",
    "        # Compress the frequencies\n",
    "        encoded_list = [vbyte_encode(x) for x in inv_f[termid]]\n",
    "        concatenated_encoded = b''.join(encoded_list)\n",
    "        assert inv_f[termid] == decode_concatenated_vbyte(concatenated_encoded), \"Compression/Decompression mismatch!\"\n",
    "        inv_f[termid] = concatenated_encoded\"\"\"\n",
    "\n",
    "    stats = {\n",
    "        'num_docs': num_docs,\n",
    "        'num_terms': len(lexicon),\n",
    "        'num_tokens': total_dl,\n",
    "    }\n",
    "    return lexicon, {'docids': inv_d, 'freqs': inv_f}, doc_index, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if chosen_collection == \"vaswani\":\n",
    "    lex, inv, doc, stats = build_index(\"./vaswani.tsv\")\n",
    "elif chosen_collection == \"MSMARCO\": # requires approx 11 minutes\n",
    "    lex, inv, doc, stats = build_index(\"../MSMARCO.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_lexicon, expected_inv, expected_doc_index, expected_stats = build_index_old(\"../MSMARCO.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    assert lex == expected_lexicon, \"Lexicon does not match expected\"\n",
    "    \n",
    "    # Ordinare le liste prima di confrontarle\n",
    "    # assert sorted(inv['docids']) == sorted(expected_inv['docids']), \"Inverted document index does not match expected\"\n",
    "    # assert sorted(inv['freqs']) == sorted(expected_inv['freqs']), \"Inverted frequencies do not match expected\"\n",
    "    \n",
    "    assert inv['docids'] == expected_inv['docids'], \"Inverted document index does not match expected\"\n",
    "    assert inv['freqs'] == expected_inv['freqs'], \"Inverted frequencies do not match expected\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Ordinare l'indice dei documenti se è una lista\n",
    "    assert doc == expected_doc_index, \"Document index does not match expected\"\n",
    "    \n",
    "    # Confronto per le statistiche, se sono dizionari o strutture simili\n",
    "    assert stats == expected_stats, \"Stats do not match expected\"\n",
    "    \n",
    "except AssertionError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stats, \"\\n\",expected_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_index(lex, inv, doc, stats): \n",
    "    print(\"**********************************************************************************\\nFirst 5 elements of lexicon:\")\n",
    "    print(list(lex.items())[:5])\n",
    "\n",
    "    print(\"\\nFirst 5 elements of inv:\\n\\tFirst 5 elements of inverted index docids:\")\n",
    "    print(\"\\t\",{k: decode_concatenated_vbyte(v)[:5] for k, v in sorted(list(inv['docids'].items()))[:5]})\n",
    "    print(\"\\tFirst 5 elements of inverted index freqs:\")\n",
    "    print(\"\\t\",{k: decode_concatenated_vbyte(v)[:5] for k, v in list(inv['freqs'].items())[:5]})\n",
    "\n",
    "    print(\"\\nFirst 5 elements of document index:\")\n",
    "    print(doc[:5])\n",
    "\n",
    "    print(\"\\nStats:\")\n",
    "    print(stats,\"\\n**********************************************************************************\")\n",
    "\n",
    "print_index(lex, inv, doc, stats)\n",
    "print_index(expected_lexicon, expected_inv, expected_doc_index, expected_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cProfile\n",
    "import pstats\n",
    "import os\n",
    "\n",
    "cProfile.run(\"build_index_old('./vaswani.tsv')\", \"output.prof\")\n",
    "p = pstats.Stats(\"output.prof\")\n",
    "p.sort_stats(\"cumtime\").print_stats(20)  # Mostra le 20 funzioni più lente\n",
    "os.remove(\"output.prof\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mircv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
