{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/giuliocapecchi/IR_project/blob/main/IR_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T17:27:39.294725Z",
     "start_time": "2024-11-19T17:27:39.283186Z"
    }
   },
   "outputs": [],
   "source": [
    "#%pip install torch matplotlib nltk tqdm gdown ir_datasets humanize lfu_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Download and prepare the collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T17:27:39.388287Z",
     "start_time": "2024-11-19T17:27:39.359476Z"
    }
   },
   "outputs": [],
   "source": [
    "# chosen_collection can be one of [\"vaswani\", \"msmarco\"]\n",
    "\n",
    "chosen_collection = \"msmarco\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T17:27:39.561923Z",
     "start_time": "2024-11-19T17:27:39.454360Z"
    }
   },
   "outputs": [],
   "source": [
    "import gdown\n",
    "import ir_datasets\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "if chosen_collection not in [\"vaswani\", \"msmarco\"]:\n",
    "    raise ValueError(\"chosen_collection must be one of ['vaswani', 'msmarco']\")\n",
    "\n",
    "if chosen_collection == \"msmarco\":\n",
    "\n",
    "    os.makedirs('./collection/msmarco', exist_ok=True)\n",
    "\n",
    "    url_collection = 'https://drive.google.com/uc?id=1_wXJjiwdgc9Kpt7o7atP8oWe-U4Z56hn'\n",
    "    \n",
    "    if not os.path.exists('./collection/msmarco/msmarco.tsv'):\n",
    "        gdown.download(url_collection, './collection/msmarco/msmarco.tsv', quiet=False)\n",
    "    \n",
    "    \"\"\"os.makedirs('./pickles', exist_ok=True)\n",
    "    if not os.path.exists('./pickles/stats.pkl'):\n",
    "        gdown.download(url_stats, './pickles/stats.pkl', quiet=False)\n",
    "    if not os.path.exists('./pickles/lex.pkl'):\n",
    "        gdown.download(url_lex, './pickles/lex.pkl', quiet=False)\n",
    "    if not os.path.exists('./pickles/inv.pkl'):\n",
    "        gdown.download(url_inv, './pickles/inv.pkl', quiet=False)\n",
    "    if not os.path.exists('./pickles/doc.pkl'):\n",
    "        gdown.download(url_doc, './pickles/doc.pkl', quiet=False)\"\"\"\n",
    "\n",
    "elif chosen_collection == \"vaswani\":\n",
    "    os.makedirs('./collection/vaswani', exist_ok=True)\n",
    "\n",
    "    vaswani_dataset = ir_datasets.load(chosen_collection)\n",
    "    docs = list(vaswani_dataset.docs_iter())\n",
    "    df = pd.DataFrame(docs)\n",
    "    df['doc_id'] = (df['doc_id'].astype(int) - 1).astype(str)\n",
    "    # rimuovi i \\n da ogni documento\n",
    "    df['text'] = df['text'].str.replace('\\n', ' ')\n",
    "    if not os.path.exists('./collection/vaswani/vaswani.tsv'):\n",
    "        df.to_csv('./collection/vaswani/vaswani.tsv', sep='\\t', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard preprocessing but with the usage of the *PyStemmer* library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T17:27:39.671041Z",
     "start_time": "2024-11-19T17:27:39.649947Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import Stemmer # PyStemmer\n",
    "\n",
    "\n",
    "nltk.download(\"stopwords\", quiet=True)\n",
    "STOPWORDS = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "STEMMER = Stemmer.Stemmer('english')\n",
    "# stemmer = nltk.stem.PorterStemmer().stem # much slower than PyStemmer\n",
    "\n",
    "\n",
    "def preprocess(s):\n",
    "    # lowercasing\n",
    "    s = s.lower()\n",
    "    # ampersand and special chars\n",
    "    s = re.sub(r\"[‘’´“”–-]\", \"'\", s.replace(\"&\", \" and \")) # this replaces & with 'and' and normalises quotes\n",
    "    # acronyms\n",
    "    s = re.sub(r\"\\.(?!(\\S[^. ])|\\d)\", \"\", s) # this removes dots that are not part of an acronym\n",
    "    # remove punctuation\n",
    "    s = s.translate(str.maketrans(string.punctuation, \" \" * len(string.punctuation)))\n",
    "    # strip whitespaces\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    # tokenisation\n",
    "    tokens = [t for t in s.split() if t not in STOPWORDS]\n",
    "    # stemming\n",
    "    return STEMMER.stemWords(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T17:27:39.717258Z",
     "start_time": "2024-11-19T17:27:39.705731Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def profile(f):\n",
    "    def f_timer(*args, **kwargs):\n",
    "        start = time.time()\n",
    "        result = f(*args, **kwargs)\n",
    "        end = time.time()\n",
    "        ms = (end - start) * 1000\n",
    "        print(f\"{f.__name__} ({ms:.3f} ms)\")\n",
    "        return result\n",
    "    return f_timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T17:27:39.810568Z",
     "start_time": "2024-11-19T17:27:39.782650Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO rivedere\n",
    "\n",
    "import pickle\n",
    "import humanize\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def print_pickled_size(var_name, var):\n",
    "    # If the 'tmp' directory does not exist, we first create it\n",
    "    os.makedirs('./tmp', exist_ok=True)\n",
    "    with open(f\"./tmp/{var_name}.pickle\", 'wb') as f:\n",
    "        pickle.dump(var, f)\n",
    "    print(f'{var_name} requires {humanize.naturalsize(os.path.getsize(f\"./tmp/{var_name}.pickle\"))}')\n",
    "    os.remove(f\"./tmp/{var_name}.pickle\")\n",
    "    os.removedirs('./tmp')\n",
    "\n",
    "\n",
    "def vbyte_encode(number):\n",
    "    bytes_list = bytearray()\n",
    "    while True:\n",
    "        byte = number & 0x7F # Prendi i 7 bit meno significativi -> 0111 1111 = 0x7F\n",
    "        number >>= 7 # Shifta a destra di 7 bit\n",
    "        if number:\n",
    "            bytes_list.append(byte) # Aggiungo i 7 bit al risultato\n",
    "        else:\n",
    "            bytes_list.append(0x80 | byte) # Aggiungo i 7 bit con il bit di continuazione, 0x80 = 1000 0000\n",
    "            break\n",
    "    return bytes(bytes_list)\n",
    "\n",
    "def vbyte_decode(bytes_seq):\n",
    "    number = 0\n",
    "    for i, byte in enumerate(bytes_seq):\n",
    "        number |= (byte & 0x7F) << (7 * i)\n",
    "        if byte & 0x80:\n",
    "            break\n",
    "    return number\n",
    "\n",
    "def decode_concatenated_vbyte(encoded_bytes):\n",
    "    decoded_numbers = []\n",
    "    current_number = 0\n",
    "    shift_amount = 0\n",
    "    \n",
    "    for byte in encoded_bytes:\n",
    "        if byte & 0x80:  # Bit di continuazione trovato, fine del numero\n",
    "            current_number |= (byte & 0x7F) << shift_amount\n",
    "            decoded_numbers.append(current_number)\n",
    "            current_number = 0\n",
    "            shift_amount = 0\n",
    "        else:  # Continuo a comporre il numero\n",
    "            current_number |= (byte & 0x7F) << shift_amount\n",
    "            shift_amount += 7\n",
    "    \n",
    "    return decoded_numbers\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "def compress_index(lexicon, inv_d, inv_f):    \n",
    "    compressed_inv_d = {}\n",
    "    compressed_inv_f = {}\n",
    "\n",
    "    for term, (termid, df, _) in tqdm(lexicon.items(), desc=\"Compressing lists\", unit=\"term\"):\n",
    "        encoded_d = bytearray()\n",
    "        for x in inv_d[termid]:\n",
    "            encoded_d.extend(vbyte_encode(x)) \n",
    "        assert decode_concatenated_vbyte(encoded_d) == inv_d[termid]\n",
    "        compressed_inv_d[termid] = encoded_d\n",
    "\n",
    "        encoded_f = bytearray()\n",
    "        for x in inv_f[termid]:\n",
    "            encoded_f.extend(vbyte_encode(x))\n",
    "        assert decode_concatenated_vbyte(encoded_f) == inv_f[termid]\n",
    "        compressed_inv_f[termid] = encoded_f\n",
    "\n",
    "    return compressed_inv_d, compressed_inv_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to build the inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T17:27:39.889433Z",
     "start_time": "2024-11-19T17:27:39.848182Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def build_index(filepath, batch_size=10000):\n",
    "    total_documents = sum(1 for _ in open(filepath)) # get total number of documents\n",
    "\n",
    "    lexicon = {}\n",
    "    inv_d = {}\n",
    "    inv_f = {}\n",
    "    doc_index = []\n",
    "    total_dl = 0\n",
    "    num_docs = 0\n",
    "    termid = 0\n",
    "\n",
    "    with open(filepath, 'r') as file:        \n",
    "        batch = []\n",
    "        \n",
    "        with tqdm(total=total_documents, desc=\"Processing documents\", unit=\"doc\") as pbar:\n",
    "            for line in file:\n",
    "                batch.append(line.strip())\n",
    "                \n",
    "                # when the batch is full, we process it\n",
    "                if len(batch) >= batch_size:\n",
    "                    for line in batch:\n",
    "                        doc_id, text = line.split('\\t', 1) # '1' specifies the number of splits\n",
    "                        doc_id = int(doc_id)\n",
    "                        tokens = preprocess(text)\n",
    "                        token_tf = Counter(tokens)\n",
    "\n",
    "                        for token, tf in token_tf.items():\n",
    "                            if token not in lexicon:\n",
    "                                lexicon[token] = [termid, 0, 0] # termid, df, tf\n",
    "                                inv_d[termid], inv_f[termid] = [], [] # docids, freqs\n",
    "                                termid += 1\n",
    "                            token_id = lexicon[token][0]  # get termid\n",
    "                            inv_d[token_id].append(doc_id)  # add doc_id to the list of documents containing the term\n",
    "                            inv_f[token_id].append(tf)  # add term frequency for this doc\n",
    "                            lexicon[token][1] += 1  # increment document frequency (df)\n",
    "                            lexicon[token][2] += tf  # increment total term frequency (tf)\n",
    "\n",
    "                        doclen = len(tokens)\n",
    "                        doc_index.append((str(doc_id), doclen))\n",
    "                        total_dl += doclen\n",
    "                        num_docs += 1                    \n",
    "                    # update progress bar for each processed document\n",
    "                    pbar.update(len(batch))\n",
    "                    batch = []\n",
    "\n",
    "            # process the remaining documents in the last batch\n",
    "            if batch:\n",
    "                for line in batch:\n",
    "                    doc_id, text = line.split('\\t', 1)\n",
    "                    doc_id = int(doc_id)\n",
    "                    tokens = preprocess(text)\n",
    "                    token_tf = Counter(tokens)\n",
    "\n",
    "                    for token, tf in token_tf.items():\n",
    "                        if token not in lexicon:\n",
    "                            lexicon[token] = [termid, 0, 0]\n",
    "                            inv_d[termid], inv_f[termid] = [], []\n",
    "                            termid += 1\n",
    "                        token_id = lexicon[token][0]  # get termid\n",
    "                        inv_d[token_id].append(doc_id)  # get doc_id to the list of documents containing the term\n",
    "                        inv_f[token_id].append(tf)  # get term frequency for this doc\n",
    "                        lexicon[token][1] += 1  # increment document frequency (df)\n",
    "                        lexicon[token][2] += tf  # increment total term frequency (tf)\n",
    "\n",
    "                    doclen = len(tokens)\n",
    "                    doc_index.append((str(doc_id), doclen))\n",
    "                    total_dl += doclen\n",
    "                    num_docs += 1                    \n",
    "                    pbar.update(1)\n",
    "                    \n",
    "     # Calculate average document length (avdl)\n",
    "    avdl = total_dl / num_docs if num_docs > 0 else 0\n",
    "                    \n",
    "    stats = {\n",
    "        'num_docs': num_docs,\n",
    "        'num_terms': len(lexicon),\n",
    "        'num_tokens': total_dl,\n",
    "        'avdl': avdl  # Add avdl to stats\n",
    "    }\n",
    "    return lexicon, {'docids': inv_d, 'freqs': inv_f}, doc_index, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T17:27:39.982267Z",
     "start_time": "2024-11-19T17:27:39.948216Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import bisect\n",
    "from functools import lru_cache\n",
    "\n",
    "\n",
    "class InvertedIndex:\n",
    "\n",
    "    class PostingListIterator:\n",
    "        def __init__(self, docids, freqs, doc, avdl):\n",
    "            self.docids = docids\n",
    "            self.freqs = freqs\n",
    "            self.pos = 0\n",
    "            self.doc = doc\n",
    "            self.total_docs_number = len(doc)\n",
    "            self.avdl = avdl\n",
    "\n",
    "        def docid(self):\n",
    "            if self.is_end_list():\n",
    "                return math.inf\n",
    "            return self.docids[self.pos]\n",
    "        \n",
    "        def score(self, method='tfidf'):\n",
    "            if method == 'tfidf':\n",
    "                return self.score_tfidf()\n",
    "            elif method == 'bm25':\n",
    "                return self.score_bm25()\n",
    "            else:\n",
    "                raise ValueError(\"Invalid scoring method\")\n",
    "        \n",
    "        ###################################################################################        \n",
    "        # def score_tfidf(self): # TODO : check if correct, this is TF-IDF score\n",
    "        #     \"\"\"\n",
    "        #     Calculate TF-IDF score of the current document in the posting list.\n",
    "        #     \"\"\"\n",
    "        #     if self.is_end_list():\n",
    "        #         return math.inf \n",
    "            \n",
    "        #     tf = self.freqs[self.pos]\n",
    "                        \n",
    "        #     if tf > 0:\n",
    "        #         wtd = 1 + math.log(tf)\n",
    "        #     else:\n",
    "        #         wtd = 0 # avoid log(0)\n",
    "            \n",
    "        #     df = self.len()  # document frequency\n",
    "        #     if df > 0:\n",
    "        #         idf = math.log(self.total_docs_number / df)\n",
    "        #     else:\n",
    "        #         idf = 0  # avoid log(0)\n",
    "            \n",
    "        #     # finally calculate tf-idf score\n",
    "        #     tfidf = wtd * idf\n",
    "\n",
    "        #     return tfidf\n",
    "        @lru_cache(maxsize=512)\n",
    "        def score_tfidf_cached(self, tf, df, total_docs_number):\n",
    "            \"\"\"\n",
    "            Cached version of TF-IDF score calculation.\n",
    "            \"\"\"\n",
    "            if tf > 0:\n",
    "                wtd = 1 + math.log(tf)\n",
    "            else:\n",
    "                wtd = 0  # avoid log(0)\n",
    "            \n",
    "            if df > 0:\n",
    "                idf = math.log(total_docs_number / df)\n",
    "            else:\n",
    "                idf = 0  # avoid log(0)\n",
    "            \n",
    "            return wtd * idf\n",
    "\n",
    "        def score_tfidf(self):\n",
    "            \"\"\"\n",
    "            Non-cached interface for calculating TF-IDF score.\n",
    "            \"\"\"\n",
    "            if self.is_end_list():\n",
    "                return math.inf\n",
    "\n",
    "            tf = self.freqs[self.pos]\n",
    "            df = self.len()\n",
    "            return self.score_tfidf_cached(tf, df, self.total_docs_number)\n",
    "\n",
    "        ###################################################################################\n",
    "        # new score_bm25 function\n",
    "        # TODO: check if correct -> OK rivista\n",
    "        # def score_bm25(self): # Modified to match the BM25 formula from the slides\n",
    "        #     if self.is_end_list():\n",
    "        #         return math.inf\n",
    "        #     else:\n",
    "        #         # Standard BM25 parameters\n",
    "        #         b = 0.75\n",
    "        #         k_1 = 1.5\n",
    "                \n",
    "        #         # Length of the current document\n",
    "        #         dl = self.doc[self.docid()][1]\n",
    "                \n",
    "        #         # Term frequency in the current document\n",
    "        #         tf = self.freqs[self.pos]\n",
    "                \n",
    "        #         # Total number of documents in the collection\n",
    "        #         N = self.total_docs_number\n",
    "                \n",
    "        #         # Number of documents containing the term\n",
    "        #         n = self.len()  # document frequency\n",
    "                \n",
    "        #         # Calculate document length normalization component (B_j)\n",
    "        #         B_j = (1 - b) + b * (dl / self.avdl)\n",
    "                \n",
    "        #         # Calculate the IDF component\n",
    "        #         idf = math.log( N / n)\n",
    "                \n",
    "        #         # Calculate the BM25 score\n",
    "        #         rsv_bm25 = ((tf) / (tf + k_1 * B_j)) * idf\n",
    "                \n",
    "        #         return rsv_bm25\n",
    "\n",
    "        @lru_cache(maxsize=512)\n",
    "        def score_bm25_cached(self, tf, dl, avdl, N, n, b=0.75, k_1=1.5):\n",
    "            B_j = (1 - b) + b * (dl / avdl)\n",
    "            idf = math.log(N / n)\n",
    "            return ((tf) / (tf + k_1 * B_j)) * idf\n",
    "\n",
    "        def score_bm25(self):\n",
    "            if self.is_end_list():\n",
    "                return math.inf\n",
    "\n",
    "            dl = self.doc[self.docid()][1]\n",
    "            tf = self.freqs[self.pos]\n",
    "            n = self.len()\n",
    "            return self.score_bm25_cached(tf, dl, self.avdl, self.total_docs_number, n)\n",
    "             \n",
    "            \n",
    "            ###################################################################################\n",
    "\n",
    "        def next(self, target=None):\n",
    "            if not target:\n",
    "                if not self.is_end_list():\n",
    "                    self.pos += 1\n",
    "            else:\n",
    "                if target > self.docid():\n",
    "                    self.pos = bisect.bisect_left(self.docids, target, self.pos)\n",
    "\n",
    "        def is_end_list(self):\n",
    "            return self.pos == len(self.docids)\n",
    "\n",
    "\n",
    "        def len(self):\n",
    "            return len(self.docids)\n",
    "        \n",
    "\n",
    "    def __init__(self, lex, inv, doc, stats):\n",
    "        self.lexicon = lex\n",
    "        self.inv = inv\n",
    "        self.doc = doc\n",
    "        self.stats = stats\n",
    "\n",
    "    def num_docs(self):\n",
    "        return self.stats['num_docs']\n",
    "    \n",
    "    def avdl(self):\n",
    "        return self.stats['avdl']\n",
    "\n",
    "    def get_posting(self, termid):\n",
    "        return InvertedIndex.PostingListIterator(self.inv['docids'][termid], self.inv['freqs'][termid], self.doc, self.stats['avdl'])\n",
    "    \n",
    "    def get_termids(self, tokens):\n",
    "        return [self.lexicon[token][0] for token in tokens if token in self.lexicon]\n",
    "\n",
    "    def get_postings(self, termids):\n",
    "        return [self.get_posting(termid) for termid in termids]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T17:27:40.060261Z",
     "start_time": "2024-11-19T17:27:40.033105Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO remove?\n",
    "# import cProfile\n",
    "# import pstats\n",
    "\n",
    "# cProfile.run(\"build_index('./vaswani.tsv')\", \"output.prof\")\n",
    "# p = pstats.Stats(\"output.prof\")\n",
    "# p.sort_stats(\"cumtime\").print_stats(10)\n",
    "# os.remove(\"output.prof\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the index on the chosen collection \n",
    "\n",
    "Now build up the index for the chosen collection. It is built only if a pickled version of its components doesn't exist already :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read doc\n",
      "read inv\n",
      "read lex\n",
      "read stats\n",
      "Building index...\n"
     ]
    }
   ],
   "source": [
    "# TODO : rimuovi\n",
    "\n",
    "with open('./pickles/doc.pkl', 'rb') as f:\n",
    "            doc = pickle.load(f)\n",
    "            print(\"read doc\")\n",
    "with open('./pickles/inv.pkl', 'rb') as f:\n",
    "            inv = pickle.load(f)\n",
    "            print(\"read inv\")\n",
    "with open('./pickles/lex.pkl', 'rb') as f:\n",
    "            lex = pickle.load(f)\n",
    "            print(\"read lex\")\n",
    "with open('./pickles/stats.pkl', 'rb') as f:\n",
    "            stats = pickle.load(f)\n",
    "            print(\"read stats\")\n",
    "    \n",
    "print(\"Building index...\")\n",
    "inv_index = InvertedIndex(lex, inv, doc, stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T17:27:41.514455Z",
     "start_time": "2024-11-19T17:27:40.142154Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4adfd9871c1f4a198ce72d43f5046a13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing documents:   0%|          | 0/8841823 [00:00<?, ?doc/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 8\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;66;03m# try to open the pickled files, else build the index\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./pickles/inv_index.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      9\u001b[0m         inv_index \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n",
      "File \u001b[1;32mc:\\Users\\giuli\\miniconda3\\envs\\mircv\\lib\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './pickles/inv_index.pkl'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m         inv_index \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n\u001b[1;32m---> 12\u001b[0m     lex, inv, doc, stats \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_index\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./collection/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mchosen_collection\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mchosen_collection\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m.tsv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;66;03m# Save the lexicon, inverted lists, and document index to disk\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./pickles/lex.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "Cell \u001b[1;32mIn[7], line 28\u001b[0m, in \u001b[0;36mbuild_index\u001b[1;34m(filepath, batch_size)\u001b[0m\n\u001b[0;32m     26\u001b[0m doc_id, text \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# '1' specifies the number of splits\u001b[39;00m\n\u001b[0;32m     27\u001b[0m doc_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(doc_id)\n\u001b[1;32m---> 28\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m token_tf \u001b[38;5;241m=\u001b[39m Counter(tokens)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token, tf \u001b[38;5;129;01min\u001b[39;00m token_tf\u001b[38;5;241m.\u001b[39mitems():\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# If the 'pickles' directory does not exist, we first create it\n",
    "os.makedirs('./pickles', exist_ok=True)\n",
    "\n",
    "if chosen_collection == \"msmarco\":\n",
    "    try: # try to open the pickled files, else build the index\n",
    "        with open('./pickles/inv_index.pkl', 'rb') as f:\n",
    "            inv_index = pickle.load(f)\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        lex, inv, doc, stats = build_index('./collection/'+chosen_collection + '/'+chosen_collection+'.tsv')\n",
    "\n",
    "        # Save the lexicon, inverted lists, and document index to disk\n",
    "        with open('./pickles/lex.pkl', 'wb') as f:\n",
    "            pickle.dump(lex, f)\n",
    "        with open('./pickles/inv.pkl', 'wb') as f:\n",
    "            pickle.dump(inv, f)\n",
    "        with open('./pickles/doc.pkl', 'wb') as f:\n",
    "            pickle.dump(doc, f)\n",
    "        with open('./pickles/stats.pkl', 'wb') as f:\n",
    "            pickle.dump(stats, f)\n",
    "                    \n",
    "        # Compress the inverted lists\n",
    "        #inv['docids'], inv['freqs'] = compress_index(lex, inv['docids'], inv['freqs'])\n",
    "        \n",
    "        inv_index = InvertedIndex(lex, inv, doc, stats)\n",
    "        with open('./pickles/inv_index.pkl', 'wb') as f:\n",
    "            pickle.dump(inv_index, f)\n",
    "else:\n",
    "    lex, inv, doc, stats = build_index('./collection/'+chosen_collection + '/'+chosen_collection+'.tsv')\n",
    "    inv_index = InvertedIndex(lex, inv, doc, stats)\n",
    "\n",
    "\n",
    "print(f\"Numero di documenti: {inv_index.num_docs()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T17:27:41.577071Z",
     "start_time": "2024-11-19T17:27:41.566029Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avdl: 34.687022122021666\n"
     ]
    }
   ],
   "source": [
    "# print avdl\n",
    "print(f\"Avdl: {inv_index.avdl()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T17:27:41.718244Z",
     "start_time": "2024-11-19T17:27:41.709662Z"
    }
   },
   "outputs": [],
   "source": [
    "#print_pickled_size('inv_index', inv_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Download and prepare queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T17:27:41.796711Z",
     "start_time": "2024-11-19T17:27:41.768982Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of queries:  200\n",
      "Number of relevance judgments:  9260\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import os\n",
    "\n",
    "if chosen_collection not in [\"vaswani\", \"msmarco\"]:\n",
    "    raise ValueError(\"chosen_collection must be one of ['vaswani', 'msmarco']\")\n",
    "\n",
    "if chosen_collection == \"msmarco\":\n",
    "    if not os.path.exists('./collection/msmarco/msmarco-queries.tsv'):\n",
    "        url = 'https://msmarco.z22.web.core.windows.net/msmarcoranking/msmarco-test2019-queries.tsv.gz'\n",
    "        gdown.download(url, './collection/msmarco/msmarco-test2019-queries.tsv.gz', quiet=False)\n",
    "        with gzip.open('./collection/msmarco/msmarco-test2019-queries.tsv.gz', 'rt') as f_in:\n",
    "            with open('./collection/msmarco/msmarco-queries.tsv', 'w') as f_out:\n",
    "                f_out.write(f_in.read())\n",
    "        os.remove('./collection/msmarco/msmarco-test2019-queries.tsv.gz') # delete the compressed file\n",
    "    queries = pd.read_csv('./collection/msmarco/msmarco-queries.tsv', sep='\\t', header=None)\n",
    "    queries.columns = ['qid', 'query']\n",
    "    print(\"Number of queries: \",len(queries))\n",
    "\n",
    "    if not os.path.exists('./collection/msmarco/msmarco-qrels.txt'):\n",
    "        url = 'https://trec.nist.gov/data/deep/2019qrels-pass.txt'\n",
    "        gdown.download(url, './collection/msmarco/msmarco-qrels.txt', quiet=False)\n",
    "    qrels = pd.read_csv('./collection/msmarco/msmarco-qrels.txt', sep=' ', header=None)\n",
    "    qrels.columns = ['qid', 'Q0', 'docid', 'rating']\n",
    "    print(\"Number of relevance judgments: \",len(qrels))\n",
    "\n",
    "\n",
    "elif chosen_collection == \"vaswani\":\n",
    "    queries = pd.DataFrame(vaswani_dataset.queries_iter())\n",
    "    queries.columns = ['qid', 'query']\n",
    "    print(\"Number of queries: \",len(list(vaswani_dataset.queries_iter()))) \n",
    "    if not os.path.exists('./collection/vaswani/vaswani-queries.tsv'):\n",
    "        queries.to_csv('./collection/vaswani/vaswani-queries.tsv', sep='\\t', header=False, index=False)\n",
    "    qrels = pd.DataFrame(vaswani_dataset.qrels_iter()) \n",
    "    qrels.columns = ['qid', 'docid', 'relevance', 'iteration']\n",
    "    qrels['docid'] = (qrels['docid'].astype(int) - 1).astype(str) # convert to 0-based indexing\n",
    "\n",
    "    if not os.path.exists('./collection/vaswani/vaswani-qrels.txt'):\n",
    "        qrels.to_csv('./collection/vaswani/vaswani-qrels.txt', sep='\\t', header=False, index=False)\n",
    "    print(\"Number of relevance judgments: \",len(list(vaswani_dataset.qrels_iter())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T17:27:41.859156Z",
     "start_time": "2024-11-19T17:27:41.848111Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of queries is:  200\n"
     ]
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "\n",
    "class QueriesDataset:\n",
    "    def __init__(self, df):\n",
    "        self.queries = [Query(row.query_id, row.text) for row in df.itertuples()]\n",
    "\n",
    "    def queries_iter(self):\n",
    "        return iter(self.queries)\n",
    "\n",
    "    def queries_count(self):\n",
    "        return len(self.queries)\n",
    "    \n",
    "    def get_query(self, query_id):\n",
    "        return self.queries[query_id]\n",
    "\n",
    "\n",
    "Query = namedtuple('Query', ['query_id', 'text'])\n",
    "queries.columns = ['query_id', 'text']\n",
    "queries_dataset = QueriesDataset(queries)\n",
    "print(\"The number of queries is: \", queries_dataset.queries_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's prepare the functions necessary to perform TAAT and DAAT query processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need a TopQueue class, which stores the top  K  (score, docid) tuples, using an heap "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T17:27:41.954190Z",
     "start_time": "2024-11-19T17:27:41.931158Z"
    }
   },
   "outputs": [],
   "source": [
    "import heapq\n",
    "\n",
    "class TopQueue:\n",
    "    def __init__(self, k=10, threshold=0.0):\n",
    "        self.queue = []\n",
    "        self.k = k\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.queue)\n",
    "\n",
    "    def would_enter(self, score):\n",
    "        return score > self.threshold\n",
    "\n",
    "    def clear(self, new_threshold=None):\n",
    "        self.queue = []\n",
    "        if new_threshold:\n",
    "            self.threshold = new_threshold\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'<{self.size()} items, th={self.threshold} {self.queue}'\n",
    "\n",
    "    def insert(self, docid, score):\n",
    "        if score > self.threshold:\n",
    "            if self.size() >= self.k:\n",
    "                heapq.heapreplace(self.queue, (score, docid))\n",
    "            else:\n",
    "                heapq.heappush(self.queue, (score, docid))\n",
    "            if self.size() >= self.k:\n",
    "                self.threshold = max(self.threshold, self.queue[0][0])\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "#print(sorted(topq.queue, reverse=True)) # print the queue sorted by score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TAAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T17:27:42.016394Z",
     "start_time": "2024-11-19T17:27:42.004032Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def taat(postings, k=10, method='bm25'):\n",
    "    A = defaultdict(float)\n",
    "    for posting in postings:\n",
    "        current_docid = posting.docid()\n",
    "        while current_docid != math.inf:\n",
    "            A[current_docid] += posting.score(method)\n",
    "            posting.next()\n",
    "            current_docid = posting.docid()\n",
    "    top = TopQueue(k)\n",
    "    for docid, score in A.items():\n",
    "        top.insert(docid, score)\n",
    "    return sorted(top.queue, reverse=True)\n",
    "\n",
    "\n",
    "def query_process(query, index):\n",
    "    qtokens = set(preprocess(query))\n",
    "    qtermids = index.get_termids(qtokens)\n",
    "    postings = index.get_postings(qtermids)\n",
    "    return taat(postings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DAAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T17:27:42.080146Z",
     "start_time": "2024-11-19T17:27:42.068274Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def min_docid(postings):\n",
    "    min_docid = math.inf\n",
    "    for p in postings:\n",
    "        if not p.is_end_list():\n",
    "            min_docid = min(p.docid(), min_docid)\n",
    "    return min_docid\n",
    "\n",
    "def daat(postings, k=10, method='bm25'):\n",
    "    top = TopQueue(k)\n",
    "    current_docid = min_docid(postings)\n",
    "    while current_docid != math.inf:\n",
    "        score = 0\n",
    "        next_docid = math.inf\n",
    "        for posting in postings:\n",
    "            if posting.docid() == current_docid:\n",
    "                score += posting.score(method)\n",
    "                posting.next()\n",
    "            if not posting.is_end_list():\n",
    "                next_docid = posting.docid()\n",
    "        top.insert(current_docid, score)\n",
    "        current_docid = next_docid\n",
    "    return sorted(top.queue, reverse=True)\n",
    "\n",
    "def query_process(query, index):\n",
    "    qtokens = set(preprocess(query))\n",
    "    qtermids = index.get_termids(qtokens)\n",
    "    postings = index.get_postings(qtermids)\n",
    "    return daat(postings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T17:27:42.158183Z",
     "start_time": "2024-11-19T17:27:42.146647Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import cProfile\n",
    "import pstats\n",
    "\n",
    "@profile\n",
    "def query_processing(queries_iter, fn):\n",
    "    for q in tqdm(queries_iter, desc=\"Processing queries\", total=queries_dataset.queries_count(), unit=\"query\"):\n",
    "        query = preprocess(q.text)\n",
    "        termids = inv_index.get_termids(query)\n",
    "        postings = inv_index.get_postings(termids)\n",
    "        res = fn(postings)\n",
    "\n",
    "\n",
    "cProfile.run(\"query_processing(queries_dataset.queries_iter(), taat)\", \"./perfm/result.prof\")\n",
    "p = pstats.Stats(\"./perfm/result.prof\")\n",
    "p.sort_stats(\"cumtime\").print_stats(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A relevance assessment (called ***qrel*** in `ir_datasets`) is composed by:\n",
    "* a **topic id** (called *query_id* in `ir_datasets`) as in a topic,\n",
    "* a **docno** (called *doc_id* in `ir_datasets`) as in a document,\n",
    "* a **judgement** (called *relevance* in `ir_datasets`) as a binary or graded relevance judgment/label, and\n",
    "* an **iteration**, **UNUSED** and always equal to the string `'0'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T17:27:48.428545Z",
     "start_time": "2024-11-19T17:27:48.400834Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of relevance judgments:  9260\n"
     ]
    }
   ],
   "source": [
    "# get the qrels for the chosen collection\n",
    "if chosen_collection == \"vaswani\":\n",
    "    sep = '\\t'\n",
    "else:\n",
    "    sep = ' '\n",
    "\n",
    "qrels = pd.read_csv('./collection/'+chosen_collection+'/'+chosen_collection+'-qrels.txt', sep=sep, header=None)\n",
    "\n",
    "if chosen_collection == \"vaswani\":\n",
    "    qrels.columns = ['query_id', 'doc_id', 'relevance', 'iteration']\n",
    "else:\n",
    "    qrels.columns = ['query_id', 'Q0', 'doc_id', 'relevance']\n",
    "    qrels['query_id'] = qrels['query_id'].apply(str)\n",
    "    qrels['doc_id'] = qrels['doc_id'].apply(str)\n",
    "    qrels['relevance'] = qrels['relevance'].apply(int)\n",
    "\n",
    "print(\"Number of relevance judgments: \",len(qrels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T17:27:48.522665Z",
     "start_time": "2024-11-19T17:27:48.509131Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def create_run_file(queries_iter, fn, k, method, run_id, output_file):\n",
    "    \"\"\"\n",
    "    Preprocess the queries and write the results to a run file.\n",
    "    :param queries_iter: Query iterator\n",
    "    :param fn: Function to process the postings and return the results in the format (score, docid)\n",
    "    :param run_id: Name identifier for the run\n",
    "    :param output_file: Output run file\n",
    "    \"\"\"\n",
    "    if not os.path.exists('./results'):\n",
    "        os.makedirs('./results')\n",
    "    with open(f\"./results/{output_file}\", \"w\") as f:\n",
    "        for q in queries_iter:\n",
    "            topic_id = q.query_id \n",
    "            query = preprocess(q.text)\n",
    "            termids = inv_index.get_termids(query)\n",
    "            postings = inv_index.get_postings(termids)\n",
    "            results = fn(postings, k=k, method=method)\n",
    "            \n",
    "            if results:\n",
    "                # Write results to the run file\n",
    "                for rank, (score, docno) in enumerate(results, start=1):\n",
    "                    line = f\"{topic_id}\\tQ0\\t{docno}\\t{rank}\\t{score:.6f}\\t{run_id}\\n\"\n",
    "                    f.write(line)\n",
    "            else:\n",
    "                # Annotate that no results were found for this query\n",
    "                line = f\"{topic_id}\\tQ0\\tNO_RESULTS\\t0\\t0.0\\t{run_id}\\n\"\n",
    "                f.write(line)\n",
    "\n",
    "    print(f\"Run file {output_file} produced successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T17:27:49.927949Z",
     "start_time": "2024-11-19T17:27:48.573038Z"
    }
   },
   "outputs": [],
   "source": [
    "create_run_file(queries_dataset.queries_iter(), taat, 1000, 'tfidf', \"run_tfidf\", f\"{chosen_collection}_tfidf_200_queries.run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_run_file(queries_dataset.queries_iter(), taat, 1000, 'bm25', \"run_bm25\", f\"{chosen_collection}_bm25_200_queries.run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8841823\n"
     ]
    }
   ],
   "source": [
    "print(inv_index.num_docs())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IR Measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ir_measures\n",
    "from ir_measures import *\n",
    "\n",
    "# Load the run\n",
    "run_file_tfidf = list(ir_measures.read_trec_run(f'./results/{chosen_collection}_tfidf_200_queries.run'))\n",
    "\n",
    "run_file_bm25 = list(ir_measures.read_trec_run(f'./results/{chosen_collection}_bm25_200_queries.run'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF:  {AP: 0.26743796793877844}\n",
      "BM25:  {AP: 0.36512401777983017}\n",
      "TFIDF:  {AP(judged_only=True): 0.4323169834401592}\n",
      "BM25:  {AP(judged_only=True): 0.4826520578883197}\n",
      "TFIDF:  {AP(rel=2,judged_only=True): 0.3196569021180839}\n",
      "BM25:  {AP(rel=2,judged_only=True): 0.34134864158104516}\n",
      "TFIDF:  {AP(rel=3,judged_only=True): 0.1657749231625388}\n",
      "BM25:  {AP(rel=3,judged_only=True): 0.18336371701465518}\n"
     ]
    }
   ],
   "source": [
    "# The [Mean] Average Precision ([M]AP). \n",
    "# The average precision of a single query is the mean of the precision scores at each relevant item returned in a search results list.\n",
    "\n",
    "print(f\"TFIDF: \", ir_measures.calc_aggregate([AP], qrels, run_file_tfidf))\n",
    "print(f\"BM25: \", ir_measures.calc_aggregate([AP], qrels, run_file_bm25))\n",
    "\n",
    "print(f\"TFIDF: \", ir_measures.calc_aggregate([AP(judged_only=True)], qrels, run_file_tfidf))\n",
    "print(f\"BM25: \", ir_measures.calc_aggregate([AP(judged_only=True)], qrels, run_file_bm25))\n",
    "\n",
    "print(f\"TFIDF: \", ir_measures.calc_aggregate([AP(rel=2, judged_only=True)], qrels, run_file_tfidf))\n",
    "print(f\"BM25: \", ir_measures.calc_aggregate([AP(rel=2, judged_only=True)], qrels, run_file_bm25))\n",
    "\n",
    "print(f\"TFIDF: \", ir_measures.calc_aggregate([AP(rel=3, judged_only=True)], qrels, run_file_tfidf))\n",
    "print(f\"BM25: \", ir_measures.calc_aggregate([AP(rel=3, judged_only=True)], qrels, run_file_bm25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF:  {nDCG: 0.5164447977209103}\n",
      "BM25:  {nDCG: 0.5939273906659651}\n",
      "TFIDF:  {nDCG@10: 0.41352178810945667}\n",
      "BM25:  {nDCG@10: 0.4727669870653633}\n",
      "TFIDF:  {nDCG(judged_only=True): 0.5896083919582058}\n",
      "BM25:  {nDCG(judged_only=True): 0.6386108191944135}\n",
      "TFIDF:  {nDCG(judged_only=True)@10: 0.48249339364752636}\n",
      "BM25:  {nDCG(judged_only=True)@10: 0.47913114994269124}\n"
     ]
    }
   ],
   "source": [
    "# The normalized Discounted Cumulative Gain (nDCG). \n",
    "# Uses graded labels - systems that put the highest graded documents at the top of the ranking. \n",
    "# It is normalized wrt. the Ideal NDCG, i.e. documents ranked in descending order of graded label.\n",
    "\n",
    "print(f\"TFIDF: \", ir_measures.calc_aggregate([nDCG], qrels, run_file_tfidf))\n",
    "print(f\"BM25: \", ir_measures.calc_aggregate([nDCG], qrels, run_file_bm25))\n",
    "\n",
    "print(f\"TFIDF: \", ir_measures.calc_aggregate([nDCG@10], qrels, run_file_tfidf))\n",
    "print(f\"BM25: \", ir_measures.calc_aggregate([nDCG@10], qrels, run_file_bm25))\n",
    "\n",
    "print(f\"TFIDF: \", ir_measures.calc_aggregate([nDCG(judged_only=True)], qrels, run_file_tfidf))\n",
    "print(f\"BM25: \", ir_measures.calc_aggregate([nDCG(judged_only=True)], qrels, run_file_bm25))\n",
    "\n",
    "print(f\"TFIDF: \", ir_measures.calc_aggregate([nDCG(judged_only=True)@10], qrels, run_file_tfidf))\n",
    "print(f\"BM25: \", ir_measures.calc_aggregate([nDCG(judged_only=True)@10], qrels, run_file_bm25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF:  {Bpref: 0.4571673693044115}\n",
      "BM25:  {Bpref: 0.4909895312262259}\n",
      "TFIDF:  {Bpref(rel=2): 0.30611709329769105}\n",
      "BM25:  {Bpref(rel=2): 0.31312768729137774}\n",
      "TFIDF:  {Bpref(rel=3): 0.12365750849455547}\n",
      "BM25:  {Bpref(rel=3): 0.1311730279512321}\n"
     ]
    }
   ],
   "source": [
    "# Binary Preference (Bpref). \n",
    "# This measure examines the relative ranks of judged relevant and non-relevant documents. \n",
    "# Non-judged documents are not considered.\n",
    "\n",
    "print(f\"TFIDF: \", ir_measures.calc_aggregate([Bpref], qrels, run_file_tfidf))\n",
    "print(f\"BM25: \", ir_measures.calc_aggregate([Bpref], qrels, run_file_bm25))\n",
    "\n",
    "print(f\"TFIDF: \", ir_measures.calc_aggregate([Bpref(rel=2)], qrels, run_file_tfidf))\n",
    "print(f\"BM25: \", ir_measures.calc_aggregate([Bpref(rel=2)], qrels, run_file_bm25))\n",
    "\n",
    "print(f\"TFIDF: \", ir_measures.calc_aggregate([Bpref(rel=3)], qrels, run_file_tfidf))\n",
    "print(f\"BM25: \", ir_measures.calc_aggregate([Bpref(rel=3)], qrels, run_file_bm25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF:  {Judged: 0.10595348837209306}\n",
      "BM25:  {Judged: 0.12069767441860466}\n",
      "TFIDF:  {Judged@5: 0.8372093023255816}\n",
      "BM25:  {Judged@5: 0.9813953488372092}\n",
      "TFIDF:  {Judged@10: 0.7581395348837208}\n",
      "BM25:  {Judged@10: 0.9488372093023255}\n",
      "TFIDF:  {Judged@100: 0.41418604651162794}\n",
      "BM25:  {Judged@100: 0.5362790697674419}\n"
     ]
    }
   ],
   "source": [
    "# BPercentage of results in the top k (cutoff) results that have relevance judgments. \n",
    "# Equivalent to P@k with a rel lower than any judgment.\n",
    "\n",
    "print(f\"TFIDF: \", ir_measures.calc_aggregate([Judged], qrels, run_file_tfidf))\n",
    "print(f\"BM25: \", ir_measures.calc_aggregate([Judged], qrels, run_file_bm25))\n",
    "\n",
    "print(f\"TFIDF: \", ir_measures.calc_aggregate([Judged@5], qrels, run_file_tfidf))\n",
    "print(f\"BM25: \", ir_measures.calc_aggregate([Judged@5], qrels, run_file_bm25))\n",
    "\n",
    "print(f\"TFIDF: \", ir_measures.calc_aggregate([Judged@10], qrels, run_file_tfidf))\n",
    "print(f\"BM25: \", ir_measures.calc_aggregate([Judged@10], qrels, run_file_bm25))\n",
    "\n",
    "print(f\"TFIDF: \", ir_measures.calc_aggregate([Judged@100], qrels, run_file_tfidf))\n",
    "print(f\"BM25: \", ir_measures.calc_aggregate([Judged@100], qrels, run_file_bm25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T17:27:51.114391Z",
     "start_time": "2024-11-19T17:27:51.039101Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO : da rivedere e capire perchè è qui\n",
    "\n",
    "# count = 0\n",
    "# for metric in ir_measures.iter_calc([P@5, P(rel=2)@5, nDCG@10, AP, AP(rel=2), Bpref, Bpref(rel=2), Judged@10], qrels, run_file):\n",
    "#   print(metric)\n",
    "#   count += 1\n",
    "#   if count >= 10: break # only show top 10 items\n",
    "\n",
    "# run_file_p_rel2_5 = {m.query_id: m.value for m in ir_measures.iter_calc([Bpref(rel=2)], qrels, run_file)}\n",
    "\n",
    "# from scipy.stats import ttest_rel\n",
    "# qids = list(run_file_p_rel2_5.keys())\n",
    "# #ttest_rel([run_file_p_rel2_5[v] for v in qids])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTerrier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T18:00:25.080832Z",
     "start_time": "2024-11-19T18:00:16.846453Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Java started and loaded: pyterrier.java, pyterrier.terrier.java [version=5.10 (build: craigm 2024-08-22 17:33), helper_version=0.0.8]\n"
     ]
    }
   ],
   "source": [
    "import pyterrier as pt\n",
    "import pandas as pd\n",
    "\n",
    "# note that pt.started() and pt.init() are deprecated\n",
    "\n",
    "if not pt.java.started():\n",
    "    pt.java.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T18:31:51.495278Z",
     "start_time": "2024-11-19T18:31:51.383429Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  docno                                               text\n",
      "0     0  The presence of communication amid scientific ...\n",
      "1     1  The Manhattan Project and its atomic bomb help...\n",
      "2     2  Essay on The Manhattan Project - The Manhattan...\n",
      "3     3  The Manhattan Project was the name for a proje...\n",
      "4     4  versions of each volume as well as complementa...\n"
     ]
    }
   ],
   "source": [
    "# Load the pickled inverted index\n",
    "df = pd.read_csv(\"collection/msmarco/msmarco.tsv\", sep=\"\\t\", header=None)\n",
    "\n",
    "# assign columns\n",
    "df.columns = [\"docno\", \"text\"]\n",
    "\n",
    "# Convert columns to strings\n",
    "df[\"docno\"] = df[\"docno\"].astype(str)\n",
    "df[\"text\"] = df[\"text\"].astype(str)\n",
    "\n",
    "# Display the DataFrame to verify\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T16:12:33.562549Z",
     "start_time": "2024-11-20T16:12:30.199027Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:20:12.612 [main] WARN org.terrier.structures.indexing.Indexer -- Adding an empty document to the index (500080) - further warnings are suppressed\n",
      "19:53:38.369 [main] WARN org.terrier.structures.indexing.Indexer -- Indexed 5 empty documents\n"
     ]
    }
   ],
   "source": [
    "# Convert DataFrame to a list of dictionaries\n",
    "docs = [{\"docno\": docno, \"text\": text} for docno, text in zip(df[\"docno\"], df[\"text\"])]\n",
    "\n",
    "# get the root path of the current directory\n",
    "cwd = os.getcwd()\n",
    "# from this get the path to index_3docs\n",
    "index_path = os.path.join(cwd, \"index_3docs\")\n",
    "\n",
    "# if the folder does not exists or is empty\n",
    "if not os.path.exists(index_path) or len(os.listdir(index_path)) == 0:\n",
    "    # indexer = pt.DFIndexer(\"./index_3docs\", overwrite=True) # deprecated\n",
    "    indexer = pt.IterDictIndexer(index_path, overwrite=True)\n",
    "    indexref = indexer.index(docs)\n",
    "    indexref.toString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 8841823\n",
      "Number of terms: 1170682\n",
      "Number of postings: 215238456\n",
      "Number of fields: 1\n",
      "Number of tokens: 288759529\n",
      "Field names: [text]\n",
      "Positions:   false\n",
      "\n"
     ]
    }
   ],
   "source": [
    "indexref = pt.IndexRef.of(os.path.abspath(\"index_3docs/data.properties\"))\n",
    "index = pt.IndexFactory.of(indexref)\n",
    "print(index.getCollectionStatistics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "\n",
    "\n",
    "# File path for the input file\n",
    "file_path = \"collection/msmarco/msmarco-queries.tsv\"  # Replace with your file path\n",
    "# Read the input file\n",
    "queries = pd.read_csv(file_path, sep=\"\\t\", header=None, names=[\"qid\", \"query\"])\n",
    "\n",
    "# Apply preprocessing to each query\n",
    "queries[\"processed_query\"] = queries[\"query\"].apply(preprocess)\n",
    "\n",
    "# Save the processed queries to a new file\n",
    "output_path = \"processed_queries.txt\"\n",
    "queries[[\"qid\", \"processed_query\"]].to_csv(output_path, sep=\"\\t\", index=False)\n",
    "\n",
    "# Read the processed file with the correct column names\n",
    "processed_queries = pd.read_csv(output_path, sep=\"\\t\", names=[\"qid\", \"query\"], skiprows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File processato salvato in: final_processed_queries.txt\n",
      "       qid                  query\n",
      "0  1108939        slow flow blood\n",
      "1  1112389  counti grand rapid mn\n",
      "2   792752                 ruclip\n",
      "3  1119729           noseble nose\n",
      "4  1105095  sugar lake lodg locat\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "def concatenate_list(query):\n",
    "    \"\"\"\n",
    "    Funzione per concatenare gli elementi di una lista di termini in una stringa.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        query_list = ast.literal_eval(query)  # Converte la stringa in lista\n",
    "        return \" \".join(query_list)  # Concatena gli elementi della lista con uno spazio\n",
    "    except (ValueError, SyntaxError):\n",
    "        return query  # Restituisci il valore originale se non è una lista valida\n",
    "\n",
    "\n",
    "# Step 1: Leggi il file delle query originali\n",
    "file_path = \"collection/msmarco/msmarco-queries.tsv\"  # Sostituisci con il percorso del file\n",
    "queries = pd.read_csv(file_path, sep=\"\\t\", header=None, names=[\"qid\", \"query\"])\n",
    "\n",
    "# Step 2: Applica la funzione preprocess a tutte le query\n",
    "queries[\"processed_query\"] = queries[\"query\"].apply(preprocess)\n",
    "\n",
    "# Step 3: Salva le query preprocessate in un file temporaneo\n",
    "temp_output_path = \"processed_queries_temp.txt\"\n",
    "queries[[\"qid\", \"processed_query\"]].to_csv(temp_output_path, sep=\"\\t\", index=False)\n",
    "\n",
    "# Step 4: Rileggi il file temporaneo per applicare concatenate_list\n",
    "processed_queries = pd.read_csv(temp_output_path, sep=\"\\t\")\n",
    "\n",
    "# Applica concatenate_list alla colonna 'processed_query'\n",
    "processed_queries[\"qid\"] = processed_queries[\"qid\"].astype(str)\n",
    "processed_queries[\"query\"] = processed_queries[\"processed_query\"].apply(concatenate_list)\n",
    "processed_queries = processed_queries[[\"qid\", \"query\"]]\n",
    "# Step 5: Salva il file finale con le colonne 'qid' e 'query'\n",
    "final_output_path = \"final_processed_queries.txt\"\n",
    "processed_queries[[\"qid\", \"query\"]].to_csv(final_output_path, sep=\"\\t\", index=False)\n",
    "\n",
    "# Output per conferma\n",
    "print(f\"File processato salvato in: {final_output_path}\")\n",
    "print(processed_queries.head())\n",
    "\n",
    "processed_queries[\"qid\"] = processed_queries[\"qid\"].astype(str)\n",
    "\n",
    "\n",
    "qrels = pd.read_csv('./collection/'+chosen_collection+'/'+chosen_collection+'-qrels.txt', sep=' ', header=None)\n",
    "qrels.columns = ['qid', 'Q0', 'docno', 'relevance']\n",
    "qrels['qid'] = qrels['qid'].apply(str)\n",
    "qrels['docno'] = qrels['docno'].apply(str)\n",
    "#qrels['relevance'] = qrels['relevance'].apply(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero totale di qid in queries: 200\n",
      "Numero totale di qid in qrels: 43\n",
      "Numero di qid comuni: 43\n",
      "Qid mancanti nei qrels: 157\n",
      "Qid mancanti nelle queries: 0\n"
     ]
    }
   ],
   "source": [
    "queries_qids = set(processed_queries[\"qid\"])\n",
    "qrels_qids = set(qrels[\"qid\"])\n",
    "\n",
    "common_qids = queries_qids & qrels_qids\n",
    "missing_in_qrels = queries_qids - qrels_qids\n",
    "missing_in_queries = qrels_qids - queries_qids\n",
    "\n",
    "# Mostra i risultati\n",
    "print(f\"Numero totale di qid in queries: {len(queries_qids)}\")\n",
    "print(f\"Numero totale di qid in qrels: {len(qrels_qids)}\")\n",
    "print(f\"Numero di qid comuni: {len(common_qids)}\")\n",
    "print(f\"Qid mancanti nei qrels: {len(missing_in_qrels)}\")\n",
    "print(f\"Qid mancanti nelle queries: {len(missing_in_queries)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File salvato in: filtered_msmarco-qrels.txt\n",
      "Numero di valori univoci nella prima colonna ('qid'): 43\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'queries_qids = set(processed_queries[\"qid\"])\\nqrels_qids = set(qrels[\"qid\"])\\n\\ncommon_qids = queries_qids & qrels_qids\\nmissing_in_qrels = queries_qids - qrels_qids\\nmissing_in_queries = qrels_qids - queries_qids\\n\\n# Mostra i risultati\\nprint(f\"Numero totale di qid in queries: {len(queries_qids)}\")\\nprint(f\"Numero totale di qid in qrels: {len(qrels_qids)}\")\\nprint(f\"Numero di qid comuni: {len(common_qids)}\")\\nprint(f\"Qid mancanti nei qrels: {len(missing_in_qrels)}\")\\nprint(f\"Qid mancanti nelle queries: {len(missing_in_queries)}\")'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Percorso del file da leggere\n",
    "input_file = \"collection/msmarco/msmarco-qrels.txt\"  # Sostituisci con il percorso del tuo file\n",
    "output_file = \"filtered_msmarco-qrels.txt\"  # Nome del file di output\n",
    "\n",
    "# Leggi il file con pandas\n",
    "df = pd.read_csv(input_file, sep=\" \", header=None, names=[\"qid\", \"Q0\", \"docno\", \"relevance\"])\n",
    "\n",
    "# Filtra le righe in cui la colonna 'relevance' è diversa da zero\n",
    "filtered_df = df[df[\"relevance\"] != 0]\n",
    "\n",
    "# Salva il risultato in un nuovo file\n",
    "filtered_df.to_csv(output_file, sep=\" \", index=False, header=False)\n",
    "\n",
    "print(f\"File salvato in: {output_file}\")\n",
    "\n",
    "file_path = \"filtered_msmarco-qrels.txt\"\n",
    "\n",
    "# Leggi il file con pandas\n",
    "df = pd.read_csv(file_path, sep=\" \", header=None, names=[\"qid\", \"Q0\", \"docno\", \"relevance\"])\n",
    "\n",
    "# Conta il numero di valori univoci nella colonna 'qid'\n",
    "unique_values_count = df[\"qid\"].nunique()\n",
    "\n",
    "# Stampa il numero di valori univoci\n",
    "print(f\"Numero di valori univoci nella prima colonna ('qid'): {unique_values_count}\")\n",
    "\n",
    "\"\"\"queries_qids = set(processed_queries[\"qid\"])\n",
    "qrels_qids = set(qrels[\"qid\"])\n",
    "\n",
    "common_qids = queries_qids & qrels_qids\n",
    "missing_in_qrels = queries_qids - qrels_qids\n",
    "missing_in_queries = qrels_qids - queries_qids\n",
    "\n",
    "# Mostra i risultati\n",
    "print(f\"Numero totale di qid in queries: {len(queries_qids)}\")\n",
    "print(f\"Numero totale di qid in qrels: {len(qrels_qids)}\")\n",
    "print(f\"Numero di qid comuni: {len(common_qids)}\")\n",
    "print(f\"Qid mancanti nei qrels: {len(missing_in_qrels)}\")\n",
    "print(f\"Qid mancanti nelle queries: {len(missing_in_queries)}\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero di valori univoci nella prima colonna ('qid'): 43\n"
     ]
    }
   ],
   "source": [
    "file_path = \"collection/msmarco/msmarco-qrels.txt\"\n",
    "\n",
    "df = pd.read_csv(file_path, sep=\" \", header=None, names=[\"qid\", \"Q0\", \"docno\", \"relevance\"])\n",
    "\n",
    "# Conta il numero di valori univoci nella colonna 'qid'\n",
    "unique_values_count = df[\"qid\"].nunique()\n",
    "\n",
    "# Stampa il numero di valori univoci\n",
    "print(f\"Numero di valori univoci nella prima colonna ('qid'): {unique_values_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyterrier.measures import *\n",
    "\n",
    "# Definizione dei retriever\n",
    "#TF_IDF = pt.BatchRetrieve(index, wmodel=\"TF_IDF\")\n",
    "TF_IDF = pt.terrier.Retriever(index, wmodel=\"TF_IDF\")\n",
    "BM25 = pt.terrier.Retriever(index, wmodel=\"BM25\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>AP</th>\n",
       "      <th>AP(judged_only=True)</th>\n",
       "      <th>AP(rel=2,judged_only=True)</th>\n",
       "      <th>AP(rel=3,judged_only=True)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TerrierRetr(TF_IDF)</td>\n",
       "      <td>0.370491</td>\n",
       "      <td>0.486072</td>\n",
       "      <td>0.345561</td>\n",
       "      <td>0.195860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TerrierRetr(BM25)</td>\n",
       "      <td>0.370611</td>\n",
       "      <td>0.486339</td>\n",
       "      <td>0.345719</td>\n",
       "      <td>0.196049</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  name        AP  AP(judged_only=True)  \\\n",
       "0  TerrierRetr(TF_IDF)  0.370491              0.486072   \n",
       "1    TerrierRetr(BM25)  0.370611              0.486339   \n",
       "\n",
       "   AP(rel=2,judged_only=True)  AP(rel=3,judged_only=True)  \n",
       "0                    0.345561                    0.195860  \n",
       "1                    0.345719                    0.196049  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt.Experiment(\n",
    "    [TF_IDF, BM25],\n",
    "    processed_queries,\n",
    "    qrels,\n",
    "    eval_metrics=[AP, AP(judged_only=True), AP(rel=2, judged_only=True), AP(rel=3, judged_only=True)]\n",
    ")\n",
    "\n",
    "# The [Mean] Average Precision ([M]AP). \n",
    "# The average precision of a single query is the mean of the precision scores at each relevant item returned in a search results list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>nDCG@10</th>\n",
       "      <th>nDCG(judged_only=True)@10</th>\n",
       "      <th>nDCG</th>\n",
       "      <th>nDCG(judged_only=True)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TerrierRetr(TF_IDF)</td>\n",
       "      <td>0.467867</td>\n",
       "      <td>0.480725</td>\n",
       "      <td>0.588751</td>\n",
       "      <td>0.631008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TerrierRetr(BM25)</td>\n",
       "      <td>0.468932</td>\n",
       "      <td>0.481790</td>\n",
       "      <td>0.589085</td>\n",
       "      <td>0.631384</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  name   nDCG@10  nDCG(judged_only=True)@10      nDCG  \\\n",
       "0  TerrierRetr(TF_IDF)  0.467867                   0.480725  0.588751   \n",
       "1    TerrierRetr(BM25)  0.468932                   0.481790  0.589085   \n",
       "\n",
       "   nDCG(judged_only=True)  \n",
       "0                0.631008  \n",
       "1                0.631384  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt.Experiment(\n",
    "    [TF_IDF, BM25],\n",
    "    processed_queries,\n",
    "    qrels,\n",
    "    eval_metrics=[nDCG@10, nDCG(judged_only=True)@10, nDCG, nDCG(judged_only=True)]\n",
    ")\n",
    "\n",
    "# The normalized Discounted Cumulative Gain (nDCG). \n",
    "# Uses graded labels - systems that put the highest graded documents at the top of the ranking. \n",
    "# It is normalized wrt. the Ideal NDCG, i.e. documents ranked in descending order of graded label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>Bpref</th>\n",
       "      <th>Bpref(rel=2)</th>\n",
       "      <th>Bpref(rel=3)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TerrierRetr(TF_IDF)</td>\n",
       "      <td>0.492971</td>\n",
       "      <td>0.315293</td>\n",
       "      <td>0.135511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TerrierRetr(BM25)</td>\n",
       "      <td>0.493091</td>\n",
       "      <td>0.315556</td>\n",
       "      <td>0.135500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  name     Bpref  Bpref(rel=2)  Bpref(rel=3)\n",
       "0  TerrierRetr(TF_IDF)  0.492971      0.315293      0.135511\n",
       "1    TerrierRetr(BM25)  0.493091      0.315556      0.135500"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt.Experiment(\n",
    "    [TF_IDF, BM25],\n",
    "    processed_queries,\n",
    "    qrels,\n",
    "    eval_metrics=[Bpref, Bpref(rel=2), Bpref(rel=3)],\n",
    ")\n",
    "\n",
    "# Binary Preference (Bpref). \n",
    "# This measure examines the relative ranks of judged relevant and non-relevant documents. \n",
    "# Non-judged documents are not considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>Judged</th>\n",
       "      <th>Judged@5</th>\n",
       "      <th>Judged@10</th>\n",
       "      <th>Judged@100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TerrierRetr(TF_IDF)</td>\n",
       "      <td>0.143070</td>\n",
       "      <td>0.95814</td>\n",
       "      <td>0.937209</td>\n",
       "      <td>0.551860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TerrierRetr(BM25)</td>\n",
       "      <td>0.143233</td>\n",
       "      <td>0.95814</td>\n",
       "      <td>0.937209</td>\n",
       "      <td>0.552558</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  name    Judged  Judged@5  Judged@10  Judged@100\n",
       "0  TerrierRetr(TF_IDF)  0.143070   0.95814   0.937209    0.551860\n",
       "1    TerrierRetr(BM25)  0.143233   0.95814   0.937209    0.552558"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt.Experiment(\n",
    "    [TF_IDF, BM25],\n",
    "    processed_queries,\n",
    "    qrels,\n",
    "    eval_metrics=[Judged, Judged@5, Judged@10, Judged@100],\n",
    ")\n",
    "\n",
    "# BPercentage of results in the top k (cutoff) results that have relevance judgments. \n",
    "# Equivalent to P@k with a rel lower than any judgment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pt.Experiment(\\n    [TF_IDF, BM25, PL2],\\n    processed_queries,\\n    qrels,\\n    eval_metrics=[\"map\"],\\n    round={\"map\" : 4},\\n    names=[\\'TF-IDF\\', \\'BM25\\', \\'PL2\\'],\\n    baseline=0,\\n    correction=\\'b\\'\\n)'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''pt.Experiment(\n",
    "    [TF_IDF, BM25, PL2],\n",
    "    processed_queries,\n",
    "    qrels,\n",
    "    eval_metrics=[\"map\"],\n",
    "    round={\"map\" : 4},\n",
    "    names=['TF-IDF', 'BM25', 'PL2'],\n",
    "    baseline=0,\n",
    "    correction='b'\n",
    ")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pt.Experiment(\\n    [TF_IDF, BM25, PL2],\\n    processed_queries,\\n    qrels,\\n    eval_metrics=[\"map\"],\\n    perquery=True\\n)'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''pt.Experiment(\n",
    "    [TF_IDF, BM25, PL2],\n",
    "    processed_queries,\n",
    "    qrels,\n",
    "    eval_metrics=[\"map\"],\n",
    "    perquery=True\n",
    ")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pt.Experiment(\\n    [TF_IDF, BM25, PL2],\\n    processed_queries,\\n    qrels,\\n    eval_metrics=[\"map\"],\\n    round={\"map\" : 4},\\n    names=[\\'TF-IDF\\', \\'BM25\\', \\'PL2\\'],\\n    baseline=0,\\n    correction=\\'b\\',\\n    save_dir=\\'./\\'\\n)'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''pt.Experiment(\n",
    "    [TF_IDF, BM25, PL2],\n",
    "    processed_queries,\n",
    "    qrels,\n",
    "    eval_metrics=[\"map\"],\n",
    "    round={\"map\" : 4},\n",
    "    names=['TF-IDF', 'BM25', 'PL2'],\n",
    "    baseline=0,\n",
    "    correction='b',\n",
    "    save_dir='./'\n",
    ")'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84c6edd66f2b441fb0e404889e80c85a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(RadioButtons(description='Scoring function:', options=('TF-IDF', 'BM25'), value=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0de235f35e08465b814863c46a2b0c81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import time\n",
    "from cachetools import LFUCache\n",
    "\n",
    "\n",
    "# UI elements\n",
    "search_bar = widgets.Text(\n",
    "    placeholder='Type in a query...',\n",
    "    description='Search:',\n",
    "    layout=widgets.Layout(width='80%')\n",
    ")\n",
    "search_button = widgets.Button(\n",
    "    description='Search',\n",
    "    button_style='success',\n",
    "    tooltip='Execute the query',\n",
    "    icon='search'\n",
    ")\n",
    "\n",
    "score_function_rbtn = widgets.RadioButtons(options=['TF-IDF', 'BM25'], description='Scoring function:', disabled=False)\n",
    "algo_rbtn = widgets.RadioButtons(options=['TAAT', 'DAAT'], description='Algorithm:', disabled=False)\n",
    "_style = widgets.HTML(\n",
    "    \"<style>.widget-radio-box {flex-direction: row !important;}.widget-radio-box\"\n",
    "    \" label{margin:2px !important;width: 100px !important;}</style>\",\n",
    "    layout=widgets.Layout(display=\"none\"),\n",
    ")\n",
    "\n",
    "output_area = widgets.Output()\n",
    "\n",
    "query_cache = LFUCache(maxsize=120)\n",
    "\n",
    "def on_search_click(b):\n",
    "    with output_area:\n",
    "        clear_output()  # clean previous output\n",
    "        query = search_bar.value\n",
    "        if not query.strip():\n",
    "            print(\"Please, type in a query.\")\n",
    "            return\n",
    "        \n",
    "        selected_scoring_function = score_function_rbtn.value\n",
    "        print(f\"Selected scoring function: {selected_scoring_function}\")\n",
    "        if selected_scoring_function == 'TF-IDF':\n",
    "            method = 'tfidf'\n",
    "        else:   \n",
    "            method = 'bm25'\n",
    "\n",
    "        selected_algorithm = algo_rbtn.value\n",
    "        print(f\"Selected Algorithm: {selected_algorithm}\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        # --- CON CACHE ---\n",
    "        processed_query = preprocess(query)\n",
    "        \n",
    "        # Usa una chiave basata sulla query preprocessata\n",
    "        cache_key = tuple(processed_query)  # Usa una rappresentazione immutabile (es. tuple)\n",
    "\n",
    "        # Verifica se il risultato è già in cache\n",
    "        if cache_key in query_cache:\n",
    "            #print(\"Cache hit!\")\n",
    "            results = query_cache[cache_key]\n",
    "        else:\n",
    "            #print(\"Cache miss!\")\n",
    "            # Elaborazione della query\n",
    "            termids = inv_index.get_termids(query)\n",
    "            postings = inv_index.get_postings(termids)\n",
    "            if selected_algorithm == 'TAAT':\n",
    "                results = taat(postings, method=method)\n",
    "            else:\n",
    "                results = daat(postings, method=method)\n",
    "\n",
    "            # Salva il risultato nella cache\n",
    "            query_cache[cache_key] = results\n",
    "        # --- QUERY EXECUTION ---\n",
    "        # processed_query = preprocess(query)\n",
    "        # termids = inv_index.get_termids(processed_query)\n",
    "        # postings = inv_index.get_postings(termids)\n",
    "        \n",
    "        # if selected_algorithm == 'TAAT':\n",
    "        #     results = taat(postings, method=method)\n",
    "        # else:\n",
    "        #     results = daat(postings, method=method)\n",
    "        # ------------------------\n",
    "        elapsed_time = (time.time() - start_time) * 1000 # convert in ms\n",
    "\n",
    "        # finally show the results\n",
    "        print(f\"Found: {len(results)} documents\\n\")\n",
    "        for res in results:\n",
    "            res = (round(res[0], 4), res[1]) # TODO : si potrebbe spostare direttamente nella score function\n",
    "            print(f\" - {res}\")\n",
    "        print(f\"\\nExecution time: {elapsed_time:.2f} ms\")\n",
    "\n",
    "search_button.on_click(on_search_click)\n",
    "# link search button to the enter key\n",
    "search_bar.continuous_update = False\n",
    "search_bar.observe(on_search_click, names='value')\n",
    "\n",
    "top_row = widgets.HBox([score_function_rbtn,_style])\n",
    "middle_row = widgets.HBox([algo_rbtn,_style])\n",
    "bottom_row = widgets.HBox([search_bar, search_button])\n",
    "\n",
    "# finally display the UI\n",
    "display(widgets.VBox([top_row,middle_row, bottom_row]))\n",
    "display(output_area)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mircv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
