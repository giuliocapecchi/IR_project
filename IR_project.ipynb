{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/giuliocapecchi/IR_project/blob/main/IR_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T17:27:39.294725Z",
     "start_time": "2024-11-19T17:27:39.283186Z"
    }
   },
   "outputs": [],
   "source": [
    "#%pip install torch matplotlib nltk tqdm gdown ir_datasets humanize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Download and prepare the collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T17:27:39.388287Z",
     "start_time": "2024-11-19T17:27:39.359476Z"
    }
   },
   "outputs": [],
   "source": [
    "# chosen_collection can be one of [\"vaswani\", \"msmarco\"]\n",
    "\n",
    "chosen_collection = \"msmarco\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T17:27:39.561923Z",
     "start_time": "2024-11-19T17:27:39.454360Z"
    }
   },
   "outputs": [],
   "source": [
    "import gdown\n",
    "import ir_datasets\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "if chosen_collection not in [\"vaswani\", \"msmarco\"]:\n",
    "    raise ValueError(\"chosen_collection must be one of ['vaswani', 'msmarco']\")\n",
    "\n",
    "if chosen_collection == \"msmarco\":\n",
    "\n",
    "    os.makedirs('./collection/msmarco', exist_ok=True)\n",
    "\n",
    "    url_collection = 'https://drive.google.com/uc?id=1_wXJjiwdgc9Kpt7o7atP8oWe-U4Z56hn'\n",
    "    \n",
    "    if not os.path.exists('./collection/msmarco/msmarco.tsv'):\n",
    "        gdown.download(url_collection, './collection/msmarco/msmarco.tsv', quiet=False)\n",
    "    \n",
    "    \"\"\"os.makedirs('./pickles', exist_ok=True)\n",
    "    if not os.path.exists('./pickles/stats.pkl'):\n",
    "        gdown.download(url_stats, './pickles/stats.pkl', quiet=False)\n",
    "    if not os.path.exists('./pickles/lex.pkl'):\n",
    "        gdown.download(url_lex, './pickles/lex.pkl', quiet=False)\n",
    "    if not os.path.exists('./pickles/inv.pkl'):\n",
    "        gdown.download(url_inv, './pickles/inv.pkl', quiet=False)\n",
    "    if not os.path.exists('./pickles/doc.pkl'):\n",
    "        gdown.download(url_doc, './pickles/doc.pkl', quiet=False)\"\"\"\n",
    "\n",
    "elif chosen_collection == \"vaswani\":\n",
    "    os.makedirs('./collection/vaswani', exist_ok=True)\n",
    "\n",
    "    vaswani_dataset = ir_datasets.load(chosen_collection)\n",
    "    docs = list(vaswani_dataset.docs_iter())\n",
    "    df = pd.DataFrame(docs)\n",
    "    df['doc_id'] = (df['doc_id'].astype(int) - 1).astype(str)\n",
    "    # rimuovi i \\n da ogni documento\n",
    "    df['text'] = df['text'].str.replace('\\n', ' ')\n",
    "    if not os.path.exists('./collection/vaswani/vaswani.tsv'):\n",
    "        df.to_csv('./collection/vaswani/vaswani.tsv', sep='\\t', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard preprocessing but with the usage of the *PyStemmer* library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T17:27:39.671041Z",
     "start_time": "2024-11-19T17:27:39.649947Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import Stemmer # PyStemmer\n",
    "\n",
    "\n",
    "nltk.download(\"stopwords\", quiet=True)\n",
    "STOPWORDS = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "STEMMER = Stemmer.Stemmer('english')\n",
    "# stemmer = nltk.stem.PorterStemmer().stem # much slower than PyStemmer\n",
    "\n",
    "\n",
    "def preprocess(s):\n",
    "    # lowercasing\n",
    "    s = s.lower()\n",
    "    # ampersand and special chars\n",
    "    s = re.sub(r\"[‘’´“”–-]\", \"'\", s.replace(\"&\", \" and \")) # this replaces & with 'and' and normalises quotes\n",
    "    # acronyms\n",
    "    s = re.sub(r\"\\.(?!(\\S[^. ])|\\d)\", \"\", s) # this removes dots that are not part of an acronym\n",
    "    # remove punctuation\n",
    "    s = s.translate(str.maketrans(string.punctuation, \" \" * len(string.punctuation)))\n",
    "    # strip whitespaces\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    # tokenisation\n",
    "    tokens = [t for t in s.split() if t not in STOPWORDS]\n",
    "    # stemming\n",
    "    return STEMMER.stemWords(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T17:27:39.717258Z",
     "start_time": "2024-11-19T17:27:39.705731Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def profile(f):\n",
    "    def f_timer(*args, **kwargs):\n",
    "        start = time.time()\n",
    "        result = f(*args, **kwargs)\n",
    "        end = time.time()\n",
    "        ms = (end - start) * 1000\n",
    "        print(f\"{f.__name__} ({ms:.3f} ms)\")\n",
    "        return result\n",
    "    return f_timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T17:27:39.810568Z",
     "start_time": "2024-11-19T17:27:39.782650Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO rivedere\n",
    "\n",
    "import pickle\n",
    "import humanize\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def print_pickled_size(var_name, var):\n",
    "    # If the 'tmp' directory does not exist, we first create it\n",
    "    os.makedirs('./tmp', exist_ok=True)\n",
    "    with open(f\"./tmp/{var_name}.pickle\", 'wb') as f:\n",
    "        pickle.dump(var, f)\n",
    "    print(f'{var_name} requires {humanize.naturalsize(os.path.getsize(f\"./tmp/{var_name}.pickle\"))}')\n",
    "    os.remove(f\"./tmp/{var_name}.pickle\")\n",
    "    os.removedirs('./tmp')\n",
    "\n",
    "\n",
    "def vbyte_encode(number):\n",
    "    bytes_list = bytearray()\n",
    "    while True:\n",
    "        byte = number & 0x7F # Prendi i 7 bit meno significativi -> 0111 1111 = 0x7F\n",
    "        number >>= 7 # Shifta a destra di 7 bit\n",
    "        if number:\n",
    "            bytes_list.append(byte) # Aggiungo i 7 bit al risultato\n",
    "        else:\n",
    "            bytes_list.append(0x80 | byte) # Aggiungo i 7 bit con il bit di continuazione, 0x80 = 1000 0000\n",
    "            break\n",
    "    return bytes(bytes_list)\n",
    "\n",
    "def vbyte_decode(bytes_seq):\n",
    "    number = 0\n",
    "    for i, byte in enumerate(bytes_seq):\n",
    "        number |= (byte & 0x7F) << (7 * i)\n",
    "        if byte & 0x80:\n",
    "            break\n",
    "    return number\n",
    "\n",
    "def decode_concatenated_vbyte(encoded_bytes):\n",
    "    decoded_numbers = []\n",
    "    current_number = 0\n",
    "    shift_amount = 0\n",
    "    \n",
    "    for byte in encoded_bytes:\n",
    "        if byte & 0x80:  # Bit di continuazione trovato, fine del numero\n",
    "            current_number |= (byte & 0x7F) << shift_amount\n",
    "            decoded_numbers.append(current_number)\n",
    "            current_number = 0\n",
    "            shift_amount = 0\n",
    "        else:  # Continuo a comporre il numero\n",
    "            current_number |= (byte & 0x7F) << shift_amount\n",
    "            shift_amount += 7\n",
    "    \n",
    "    return decoded_numbers\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "def compress_index(lexicon, inv_d, inv_f):    \n",
    "    compressed_inv_d = {}\n",
    "    compressed_inv_f = {}\n",
    "\n",
    "    for term, (termid, df, _) in tqdm(lexicon.items(), desc=\"Compressing lists\", unit=\"term\"):\n",
    "        encoded_d = bytearray()\n",
    "        for x in inv_d[termid]:\n",
    "            encoded_d.extend(vbyte_encode(x)) \n",
    "        assert decode_concatenated_vbyte(encoded_d) == inv_d[termid]\n",
    "        compressed_inv_d[termid] = encoded_d\n",
    "\n",
    "        encoded_f = bytearray()\n",
    "        for x in inv_f[termid]:\n",
    "            encoded_f.extend(vbyte_encode(x))\n",
    "        assert decode_concatenated_vbyte(encoded_f) == inv_f[termid]\n",
    "        compressed_inv_f[termid] = encoded_f\n",
    "\n",
    "    return compressed_inv_d, compressed_inv_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to build the inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T17:27:39.889433Z",
     "start_time": "2024-11-19T17:27:39.848182Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def build_index(filepath, batch_size=10000):\n",
    "    total_documents = sum(1 for _ in open(filepath)) # get total number of documents\n",
    "\n",
    "    lexicon = {}\n",
    "    inv_d = {}\n",
    "    inv_f = {}\n",
    "    doc_index = []\n",
    "    total_dl = 0\n",
    "    num_docs = 0\n",
    "    termid = 0\n",
    "\n",
    "    with open(filepath, 'r') as file:        \n",
    "        batch = []\n",
    "        \n",
    "        with tqdm(total=total_documents, desc=\"Processing documents\", unit=\"doc\") as pbar:\n",
    "            for line in file:\n",
    "                batch.append(line.strip())\n",
    "                \n",
    "                # when the batch is full, we process it\n",
    "                if len(batch) >= batch_size:\n",
    "                    for line in batch:\n",
    "                        doc_id, text = line.split('\\t', 1) # '1' specifies the number of splits\n",
    "                        doc_id = int(doc_id)\n",
    "                        tokens = preprocess(text)\n",
    "                        token_tf = Counter(tokens)\n",
    "\n",
    "                        for token, tf in token_tf.items():\n",
    "                            if token not in lexicon:\n",
    "                                lexicon[token] = [termid, 0, 0] # termid, df, tf\n",
    "                                inv_d[termid], inv_f[termid] = [], [] # docids, freqs\n",
    "                                termid += 1\n",
    "                            token_id = lexicon[token][0]  # get termid\n",
    "                            inv_d[token_id].append(doc_id)  # add doc_id to the list of documents containing the term\n",
    "                            inv_f[token_id].append(tf)  # add term frequency for this doc\n",
    "                            lexicon[token][1] += 1  # increment document frequency (df)\n",
    "                            lexicon[token][2] += tf  # increment total term frequency (tf)\n",
    "\n",
    "                        doclen = len(tokens)\n",
    "                        doc_index.append((str(doc_id), doclen))\n",
    "                        total_dl += doclen\n",
    "                        num_docs += 1                    \n",
    "                    # update progress bar for each processed document\n",
    "                    pbar.update(len(batch))\n",
    "                    batch = []\n",
    "\n",
    "            # process the remaining documents in the last batch\n",
    "            if batch:\n",
    "                for line in batch:\n",
    "                    doc_id, text = line.split('\\t', 1)\n",
    "                    doc_id = int(doc_id)\n",
    "                    tokens = preprocess(text)\n",
    "                    token_tf = Counter(tokens)\n",
    "\n",
    "                    for token, tf in token_tf.items():\n",
    "                        if token not in lexicon:\n",
    "                            lexicon[token] = [termid, 0, 0]\n",
    "                            inv_d[termid], inv_f[termid] = [], []\n",
    "                            termid += 1\n",
    "                        token_id = lexicon[token][0]  # get termid\n",
    "                        inv_d[token_id].append(doc_id)  # get doc_id to the list of documents containing the term\n",
    "                        inv_f[token_id].append(tf)  # get term frequency for this doc\n",
    "                        lexicon[token][1] += 1  # increment document frequency (df)\n",
    "                        lexicon[token][2] += tf  # increment total term frequency (tf)\n",
    "\n",
    "                    doclen = len(tokens)\n",
    "                    doc_index.append((str(doc_id), doclen))\n",
    "                    total_dl += doclen\n",
    "                    num_docs += 1                    \n",
    "                    pbar.update(1)\n",
    "                    \n",
    "     # Calculate average document length (avdl)\n",
    "    avdl = total_dl / num_docs if num_docs > 0 else 0\n",
    "                    \n",
    "    stats = {\n",
    "        'num_docs': num_docs,\n",
    "        'num_terms': len(lexicon),\n",
    "        'num_tokens': total_dl,\n",
    "        'avdl': avdl  # Add avdl to stats\n",
    "    }\n",
    "    return lexicon, {'docids': inv_d, 'freqs': inv_f}, doc_index, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T17:27:39.982267Z",
     "start_time": "2024-11-19T17:27:39.948216Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import bisect\n",
    "\n",
    "\n",
    "class InvertedIndex:\n",
    "\n",
    "    class PostingListIterator:\n",
    "        def __init__(self, docids, freqs, doc, avdl):\n",
    "            self.docids = docids\n",
    "            self.freqs = freqs\n",
    "            self.pos = 0\n",
    "            self.doc = doc\n",
    "            self.total_docs_number = len(doc)\n",
    "            self.avdl = avdl\n",
    "\n",
    "        def docid(self):\n",
    "            if self.is_end_list():\n",
    "                return math.inf\n",
    "            return self.docids[self.pos]\n",
    "        \n",
    "        def score(self, method='tfidf'):\n",
    "            if method == 'tfidf':\n",
    "                return self.score_tfidf()\n",
    "            elif method == 'bm25':\n",
    "                return self.score_bm25()\n",
    "            else:\n",
    "                raise ValueError(\"Invalid scoring method\")\n",
    "        \n",
    "        ###################################################################################        \n",
    "        def score_tfidf(self): # TODO : check if correct, this is TF-IDF score\n",
    "            \"\"\"\n",
    "            Calculate TF-IDF score of the current document in the posting list.\n",
    "            \"\"\"\n",
    "            if self.is_end_list():\n",
    "                return math.inf \n",
    "            \n",
    "            tf = self.freqs[self.pos]\n",
    "                        \n",
    "            if tf > 0:\n",
    "                wtd = 1 + math.log(tf)\n",
    "            else:\n",
    "                wtd = 0 # avoid log(0)\n",
    "            \n",
    "            df = self.len()  # document frequency\n",
    "            if df > 0:\n",
    "                idf = math.log(self.total_docs_number / df)\n",
    "            else:\n",
    "                idf = 0  # avoid log(0)\n",
    "            \n",
    "            # finally calculate tf-idf score\n",
    "            tfidf = wtd * idf\n",
    "\n",
    "            return tfidf\n",
    "\n",
    "        ###################################################################################\n",
    "        # new score_bm25 function\n",
    "        # TODO: check if correct -> OK rivista\n",
    "        def score_bm25(self): # Modified to match the BM25 formula from the slides\n",
    "            if self.is_end_list():\n",
    "                return math.inf\n",
    "            else:\n",
    "                # Standard BM25 parameters\n",
    "                b = 0.75\n",
    "                k_1 = 1.5\n",
    "                \n",
    "                # Length of the current document\n",
    "                dl = self.doc[self.docid()][1]\n",
    "                \n",
    "                # Term frequency in the current document\n",
    "                tf = self.freqs[self.pos]\n",
    "                \n",
    "                # Total number of documents in the collection\n",
    "                N = self.total_docs_number\n",
    "                \n",
    "                # Number of documents containing the term\n",
    "                n = self.len()  # document frequency\n",
    "                \n",
    "                # Calculate document length normalization component (B_j)\n",
    "                B_j = (1 - b) + b * (dl / self.avdl)\n",
    "                \n",
    "                # Calculate the IDF component\n",
    "                idf = math.log( N / n)\n",
    "                \n",
    "                # Calculate the BM25 score\n",
    "                rsv_bm25 = ((tf) / (tf + k_1 * B_j)) * idf\n",
    "                \n",
    "                return rsv_bm25\n",
    "            \n",
    "            ###################################################################################\n",
    "\n",
    "        def next(self, target=None):\n",
    "            if not target:\n",
    "                if not self.is_end_list():\n",
    "                    self.pos += 1\n",
    "            else:\n",
    "                if target > self.docid():\n",
    "                    self.pos = bisect.bisect_left(self.docids, target, self.pos)\n",
    "\n",
    "        def is_end_list(self):\n",
    "            return self.pos == len(self.docids)\n",
    "\n",
    "\n",
    "        def len(self):\n",
    "            return len(self.docids)\n",
    "        \n",
    "\n",
    "    def __init__(self, lex, inv, doc, stats):\n",
    "        self.lexicon = lex\n",
    "        self.inv = inv\n",
    "        self.doc = doc\n",
    "        self.stats = stats\n",
    "\n",
    "    def num_docs(self):\n",
    "        return self.stats['num_docs']\n",
    "    \n",
    "    def avdl(self):\n",
    "        return self.stats['avdl']\n",
    "\n",
    "    def get_posting(self, termid):\n",
    "        return InvertedIndex.PostingListIterator(self.inv['docids'][termid], self.inv['freqs'][termid], self.doc, self.stats['avdl'])\n",
    "    \n",
    "    def get_termids(self, tokens):\n",
    "        return [self.lexicon[token][0] for token in tokens if token in self.lexicon]\n",
    "\n",
    "    def get_postings(self, termids):\n",
    "        return [self.get_posting(termid) for termid in termids]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T17:27:40.060261Z",
     "start_time": "2024-11-19T17:27:40.033105Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO remove?\n",
    "# import cProfile\n",
    "# import pstats\n",
    "\n",
    "# cProfile.run(\"build_index('./vaswani.tsv')\", \"output.prof\")\n",
    "# p = pstats.Stats(\"output.prof\")\n",
    "# p.sort_stats(\"cumtime\").print_stats(10)\n",
    "# os.remove(\"output.prof\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the index on the chosen collection \n",
    "\n",
    "Now build up the index for the chosen collection. It is built only if a pickled version of its components doesn't exist already :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T17:27:41.514455Z",
     "start_time": "2024-11-19T17:27:40.142154Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero di documenti: 8841823\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# If the 'pickles' directory does not exist, we first create it\n",
    "os.makedirs('./pickles', exist_ok=True)\n",
    "\n",
    "if chosen_collection == \"msmarco\":\n",
    "    try: # try to open the pickled files, else build the index\n",
    "        with open('./pickles/inv_index.pkl', 'rb') as f:\n",
    "            inv_index = pickle.load(f)\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        lex, inv, doc, stats = build_index('./collection/'+chosen_collection + '/'+chosen_collection+'.tsv')\n",
    "\n",
    "        # Save the lexicon, inverted lists, and document index to disk\n",
    "        with open('./pickles/lex.pkl', 'wb') as f:\n",
    "            pickle.dump(lex, f)\n",
    "        with open('./pickles/inv.pkl', 'wb') as f:\n",
    "            pickle.dump(inv, f)\n",
    "        with open('./pickles/doc.pkl', 'wb') as f:\n",
    "            pickle.dump(doc, f)\n",
    "        with open('./pickles/stats.pkl', 'wb') as f:\n",
    "            pickle.dump(stats, f)\n",
    "                    \n",
    "        # Compress the inverted lists\n",
    "        #inv['docids'], inv['freqs'] = compress_index(lex, inv['docids'], inv['freqs'])\n",
    "        \n",
    "        inv_index = InvertedIndex(lex, inv, doc, stats)\n",
    "        with open('./pickles/inv_index.pkl', 'wb') as f:\n",
    "            pickle.dump(inv_index, f)\n",
    "else:\n",
    "    lex, inv, doc, stats = build_index('./collection/'+chosen_collection + '/'+chosen_collection+'.tsv')\n",
    "    inv_index = InvertedIndex(lex, inv, doc, stats)\n",
    "\n",
    "\n",
    "print(f\"Numero di documenti: {inv_index.num_docs()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T17:27:41.577071Z",
     "start_time": "2024-11-19T17:27:41.566029Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avdl: 34.687022122021666\n"
     ]
    }
   ],
   "source": [
    "# print avdl\n",
    "print(f\"Avdl: {inv_index.avdl()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T17:27:41.718244Z",
     "start_time": "2024-11-19T17:27:41.709662Z"
    }
   },
   "outputs": [],
   "source": [
    "#print_pickled_size('inv_index', inv_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Download and prepare queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T17:27:41.796711Z",
     "start_time": "2024-11-19T17:27:41.768982Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of queries:  200\n",
      "Number of relevance judgments:  9260\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import os\n",
    "\n",
    "if chosen_collection not in [\"vaswani\", \"msmarco\"]:\n",
    "    raise ValueError(\"chosen_collection must be one of ['vaswani', 'msmarco']\")\n",
    "\n",
    "if chosen_collection == \"msmarco\":\n",
    "    if not os.path.exists('./collection/msmarco/msmarco-queries.tsv'):\n",
    "        url = 'https://msmarco.z22.web.core.windows.net/msmarcoranking/msmarco-test2019-queries.tsv.gz'\n",
    "        gdown.download(url, './collection/msmarco/msmarco-test2019-queries.tsv.gz', quiet=False)\n",
    "        with gzip.open('./collection/msmarco/msmarco-test2019-queries.tsv.gz', 'rt') as f_in:\n",
    "            with open('./collection/msmarco/msmarco-queries.tsv', 'w') as f_out:\n",
    "                f_out.write(f_in.read())\n",
    "        os.remove('./collection/msmarco/msmarco-test2019-queries.tsv.gz') # delete the compressed file\n",
    "    queries = pd.read_csv('./collection/msmarco/msmarco-queries.tsv', sep='\\t', header=None)\n",
    "    queries.columns = ['qid', 'query']\n",
    "    print(\"Number of queries: \",len(queries))\n",
    "\n",
    "    if not os.path.exists('./collection/msmarco/msmarco-qrels.txt'):\n",
    "        url = 'https://trec.nist.gov/data/deep/2019qrels-pass.txt'\n",
    "        gdown.download(url, './collection/msmarco/msmarco-qrels.txt', quiet=False)\n",
    "    qrels = pd.read_csv('./collection/msmarco/msmarco-qrels.txt', sep=' ', header=None)\n",
    "    qrels.columns = ['qid', 'Q0', 'docid', 'rating']\n",
    "    print(\"Number of relevance judgments: \",len(qrels))\n",
    "\n",
    "\n",
    "elif chosen_collection == \"vaswani\":\n",
    "    queries = pd.DataFrame(vaswani_dataset.queries_iter())\n",
    "    queries.columns = ['qid', 'query']\n",
    "    print(\"Number of queries: \",len(list(vaswani_dataset.queries_iter()))) \n",
    "    if not os.path.exists('./collection/vaswani/vaswani-queries.tsv'):\n",
    "        queries.to_csv('./collection/vaswani/vaswani-queries.tsv', sep='\\t', header=False, index=False)\n",
    "    qrels = pd.DataFrame(vaswani_dataset.qrels_iter()) \n",
    "    qrels.columns = ['qid', 'docid', 'relevance', 'iteration']\n",
    "    qrels['docid'] = (qrels['docid'].astype(int) - 1).astype(str) # convert to 0-based indexing\n",
    "\n",
    "    if not os.path.exists('./collection/vaswani/vaswani-qrels.txt'):\n",
    "        qrels.to_csv('./collection/vaswani/vaswani-qrels.txt', sep='\\t', header=False, index=False)\n",
    "    print(\"Number of relevance judgments: \",len(list(vaswani_dataset.qrels_iter())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T17:27:41.859156Z",
     "start_time": "2024-11-19T17:27:41.848111Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of queries is:  200\n"
     ]
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "\n",
    "class QueriesDataset:\n",
    "    def __init__(self, df):\n",
    "        self.queries = [Query(row.query_id, row.text) for row in df.itertuples()]\n",
    "\n",
    "    def queries_iter(self):\n",
    "        return iter(self.queries)\n",
    "\n",
    "    def queries_count(self):\n",
    "        return len(self.queries)\n",
    "    \n",
    "    def get_query(self, query_id):\n",
    "        return self.queries[query_id]\n",
    "\n",
    "\n",
    "Query = namedtuple('Query', ['query_id', 'text'])\n",
    "queries.columns = ['query_id', 'text']\n",
    "queries_dataset = QueriesDataset(queries)\n",
    "print(\"The number of queries is: \", queries_dataset.queries_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's prepare the functions necessary to perform TAAT and DAAT query processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need a TopQueue class, which stores the top  K  (score, docid) tuples, using an heap "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T17:27:41.954190Z",
     "start_time": "2024-11-19T17:27:41.931158Z"
    }
   },
   "outputs": [],
   "source": [
    "import heapq\n",
    "\n",
    "class TopQueue:\n",
    "    def __init__(self, k=10, threshold=0.0):\n",
    "        self.queue = []\n",
    "        self.k = k\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.queue)\n",
    "\n",
    "    def would_enter(self, score):\n",
    "        return score > self.threshold\n",
    "\n",
    "    def clear(self, new_threshold=None):\n",
    "        self.queue = []\n",
    "        if new_threshold:\n",
    "            self.threshold = new_threshold\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'<{self.size()} items, th={self.threshold} {self.queue}'\n",
    "\n",
    "    def insert(self, docid, score):\n",
    "        if score > self.threshold:\n",
    "            if self.size() >= self.k:\n",
    "                heapq.heapreplace(self.queue, (score, docid))\n",
    "            else:\n",
    "                heapq.heappush(self.queue, (score, docid))\n",
    "            if self.size() >= self.k:\n",
    "                self.threshold = max(self.threshold, self.queue[0][0])\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "#print(sorted(topq.queue, reverse=True)) # print the queue sorted by score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TAAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T17:27:42.016394Z",
     "start_time": "2024-11-19T17:27:42.004032Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def taat(postings, k=10, method='bm25'):\n",
    "    A = defaultdict(float)\n",
    "    for posting in postings:\n",
    "        current_docid = posting.docid()\n",
    "        while current_docid != math.inf:\n",
    "            A[current_docid] += posting.score(method)\n",
    "            posting.next()\n",
    "            current_docid = posting.docid()\n",
    "    top = TopQueue(k)\n",
    "    for docid, score in A.items():\n",
    "        top.insert(docid, score)\n",
    "    return sorted(top.queue, reverse=True)\n",
    "\n",
    "\n",
    "def query_process(query, index):\n",
    "    qtokens = set(preprocess(query))\n",
    "    qtermids = index.get_termids(qtokens)\n",
    "    postings = index.get_postings(qtermids)\n",
    "    return taat(postings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DAAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T17:27:42.080146Z",
     "start_time": "2024-11-19T17:27:42.068274Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def min_docid(postings):\n",
    "    min_docid = math.inf\n",
    "    for p in postings:\n",
    "        if not p.is_end_list():\n",
    "            min_docid = min(p.docid(), min_docid)\n",
    "    return min_docid\n",
    "\n",
    "def daat(postings, k=10, method='bm25'):\n",
    "    top = TopQueue(k)\n",
    "    current_docid = min_docid(postings)\n",
    "    while current_docid != math.inf:\n",
    "        score = 0\n",
    "        next_docid = math.inf\n",
    "        for posting in postings:\n",
    "            if posting.docid() == current_docid:\n",
    "                score += posting.score(method)\n",
    "                posting.next()\n",
    "            if not posting.is_end_list():\n",
    "                next_docid = posting.docid()\n",
    "        top.insert(current_docid, score)\n",
    "        current_docid = next_docid\n",
    "    return sorted(top.queue, reverse=True)\n",
    "\n",
    "def query_process(query, index):\n",
    "    qtokens = set(preprocess(query))\n",
    "    qtermids = index.get_termids(qtokens)\n",
    "    postings = index.get_postings(qtermids)\n",
    "    return daat(postings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T17:27:42.158183Z",
     "start_time": "2024-11-19T17:27:42.146647Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import cProfile\n",
    "import pstats\n",
    "\n",
    "@profile\n",
    "def query_processing(queries_iter, fn):\n",
    "    for q in tqdm(queries_iter, desc=\"Processing queries\", total=queries_dataset.queries_count(), unit=\"query\"):\n",
    "        query = preprocess(q.text)\n",
    "        termids = inv_index.get_termids(query)\n",
    "        postings = inv_index.get_postings(termids)\n",
    "        res = fn(postings)\n",
    "\n",
    "\n",
    "cProfile.run(\"query_processing(queries_dataset.queries_iter(), taat)\", \"./perfm/result.prof\")\n",
    "p = pstats.Stats(\"./perfm/result.prof\")\n",
    "p.sort_stats(\"cumtime\").print_stats(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A relevance assessment (called ***qrel*** in `ir_datasets`) is composed by:\n",
    "* a **topic id** (called *query_id* in `ir_datasets`) as in a topic,\n",
    "* a **docno** (called *doc_id* in `ir_datasets`) as in a document,\n",
    "* a **judgement** (called *relevance* in `ir_datasets`) as a binary or graded relevance judgment/label, and\n",
    "* an **iteration**, **UNUSED** and always equal to the string `'0'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T17:27:48.428545Z",
     "start_time": "2024-11-19T17:27:48.400834Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of relevance judgments:  9260\n"
     ]
    }
   ],
   "source": [
    "# get the qrels for the chosen collection\n",
    "if chosen_collection == \"vaswani\":\n",
    "    sep = '\\t'\n",
    "else:\n",
    "    sep = ' '\n",
    "\n",
    "qrels = pd.read_csv('./collection/'+chosen_collection+'/'+chosen_collection+'-qrels.txt', sep=sep, header=None)\n",
    "\n",
    "if chosen_collection == \"vaswani\":\n",
    "    qrels.columns = ['query_id', 'doc_id', 'relevance', 'iteration']\n",
    "else:\n",
    "    qrels.columns = ['query_id', 'Q0', 'doc_id', 'relevance']\n",
    "    qrels['query_id'] = qrels['query_id'].apply(str)\n",
    "    qrels['doc_id'] = qrels['doc_id'].apply(str)\n",
    "    qrels['relevance'] = qrels['relevance'].apply(int)\n",
    "\n",
    "print(\"Number of relevance judgments: \",len(qrels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T17:27:48.522665Z",
     "start_time": "2024-11-19T17:27:48.509131Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def create_run_file(queries_iter, fn, k, method, run_id, output_file):\n",
    "    \"\"\"\n",
    "    Preprocess the queries and write the results to a run file.\n",
    "    :param queries_iter: Query iterator\n",
    "    :param fn: Function to process the postings and return the results in the format (score, docid)\n",
    "    :param run_id: Name identifier for the run\n",
    "    :param output_file: Output run file\n",
    "    \"\"\"\n",
    "    if not os.path.exists('./results'):\n",
    "        os.makedirs('./results')\n",
    "    with open(f\"./results/{output_file}\", \"w\") as f:\n",
    "        for q in queries_iter:\n",
    "            topic_id = q.query_id \n",
    "            query = preprocess(q.text)\n",
    "            termids = inv_index.get_termids(query)\n",
    "            postings = inv_index.get_postings(termids)\n",
    "            results = fn(postings, k=k, method=method)\n",
    "            \n",
    "            if results:\n",
    "                # Write results to the run file\n",
    "                for rank, (score, docno) in enumerate(results, start=1):\n",
    "                    line = f\"{topic_id}\\tQ0\\t{docno}\\t{rank}\\t{score:.6f}\\t{run_id}\\n\"\n",
    "                    f.write(line)\n",
    "            else:\n",
    "                # Annotate that no results were found for this query\n",
    "                line = f\"{topic_id}\\tQ0\\tNO_RESULTS\\t0\\t0.0\\t{run_id}\\n\"\n",
    "                f.write(line)\n",
    "\n",
    "    print(f\"Run file {output_file} produced successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T17:27:49.927949Z",
     "start_time": "2024-11-19T17:27:48.573038Z"
    }
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('./results'):\n",
    "    os.makedirs('./results')\n",
    "    create_run_file(queries_dataset.queries_iter(), taat, 1000, 'tfidf', \"run_tfidf\", f\"{chosen_collection}_tfidf_200_queries.run\")\n",
    "    create_run_file(queries_dataset.queries_iter(), taat, 1000, 'bm25', \"run_bm25\", f\"{chosen_collection}_bm25_200_queries.run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8841823\n"
     ]
    }
   ],
   "source": [
    "print(inv_index.num_docs())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IR Measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ir_measures\n",
    "from ir_measures import *\n",
    "\n",
    "# Load the run\n",
    "run_file_tfidf = list(ir_measures.read_trec_run(f'./results/{chosen_collection}_tfidf_200_queries.run'))\n",
    "\n",
    "run_file_bm25 = list(ir_measures.read_trec_run(f'./results/{chosen_collection}_bm25_200_queries.run'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF:  {AP: 0.26743796793877844}\n",
      "BM25:  {AP: 0.36512401777983017}\n",
      "TFIDF:  {AP(judged_only=True): 0.4323169834401592}\n",
      "BM25:  {AP(judged_only=True): 0.4826520578883197}\n",
      "TFIDF:  {AP(rel=2,judged_only=True): 0.3196569021180839}\n",
      "BM25:  {AP(rel=2,judged_only=True): 0.34134864158104516}\n",
      "TFIDF:  {AP(rel=3,judged_only=True): 0.1657749231625388}\n",
      "BM25:  {AP(rel=3,judged_only=True): 0.18336371701465518}\n"
     ]
    }
   ],
   "source": [
    "# The [Mean] Average Precision ([M]AP). \n",
    "# The average precision of a single query is the mean of the precision scores at each relevant item returned in a search results list.\n",
    "\n",
    "print(f\"TFIDF: \", ir_measures.calc_aggregate([AP], qrels, run_file_tfidf))\n",
    "print(f\"BM25: \", ir_measures.calc_aggregate([AP], qrels, run_file_bm25))\n",
    "\n",
    "print(f\"TFIDF: \", ir_measures.calc_aggregate([AP(judged_only=True)], qrels, run_file_tfidf))\n",
    "print(f\"BM25: \", ir_measures.calc_aggregate([AP(judged_only=True)], qrels, run_file_bm25))\n",
    "\n",
    "print(f\"TFIDF: \", ir_measures.calc_aggregate([AP(rel=2, judged_only=True)], qrels, run_file_tfidf))\n",
    "print(f\"BM25: \", ir_measures.calc_aggregate([AP(rel=2, judged_only=True)], qrels, run_file_bm25))\n",
    "\n",
    "print(f\"TFIDF: \", ir_measures.calc_aggregate([AP(rel=3, judged_only=True)], qrels, run_file_tfidf))\n",
    "print(f\"BM25: \", ir_measures.calc_aggregate([AP(rel=3, judged_only=True)], qrels, run_file_bm25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF:  {nDCG: 0.5164447977209103}\n",
      "BM25:  {nDCG: 0.5939273906659651}\n",
      "TFIDF:  {nDCG@10: 0.41352178810945667}\n",
      "BM25:  {nDCG@10: 0.4727669870653633}\n",
      "TFIDF:  {nDCG(judged_only=True): 0.5896083919582058}\n",
      "BM25:  {nDCG(judged_only=True): 0.6386108191944135}\n",
      "TFIDF:  {nDCG(judged_only=True)@10: 0.48249339364752636}\n",
      "BM25:  {nDCG(judged_only=True)@10: 0.47913114994269124}\n"
     ]
    }
   ],
   "source": [
    "# The normalized Discounted Cumulative Gain (nDCG). \n",
    "# Uses graded labels - systems that put the highest graded documents at the top of the ranking. \n",
    "# It is normalized wrt. the Ideal NDCG, i.e. documents ranked in descending order of graded label.\n",
    "\n",
    "print(f\"TFIDF: \", ir_measures.calc_aggregate([nDCG], qrels, run_file_tfidf))\n",
    "print(f\"BM25: \", ir_measures.calc_aggregate([nDCG], qrels, run_file_bm25))\n",
    "\n",
    "print(f\"TFIDF: \", ir_measures.calc_aggregate([nDCG@10], qrels, run_file_tfidf))\n",
    "print(f\"BM25: \", ir_measures.calc_aggregate([nDCG@10], qrels, run_file_bm25))\n",
    "\n",
    "print(f\"TFIDF: \", ir_measures.calc_aggregate([nDCG(judged_only=True)], qrels, run_file_tfidf))\n",
    "print(f\"BM25: \", ir_measures.calc_aggregate([nDCG(judged_only=True)], qrels, run_file_bm25))\n",
    "\n",
    "print(f\"TFIDF: \", ir_measures.calc_aggregate([nDCG(judged_only=True)@10], qrels, run_file_tfidf))\n",
    "print(f\"BM25: \", ir_measures.calc_aggregate([nDCG(judged_only=True)@10], qrels, run_file_bm25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF:  {Bpref: 0.4571673693044115}\n",
      "BM25:  {Bpref: 0.4909895312262259}\n",
      "TFIDF:  {Bpref(rel=2): 0.30611709329769105}\n",
      "BM25:  {Bpref(rel=2): 0.31312768729137774}\n",
      "TFIDF:  {Bpref(rel=3): 0.12365750849455547}\n",
      "BM25:  {Bpref(rel=3): 0.1311730279512321}\n"
     ]
    }
   ],
   "source": [
    "# Binary Preference (Bpref). \n",
    "# This measure examines the relative ranks of judged relevant and non-relevant documents. \n",
    "# Non-judged documents are not considered.\n",
    "\n",
    "print(f\"TFIDF: \", ir_measures.calc_aggregate([Bpref], qrels, run_file_tfidf))\n",
    "print(f\"BM25: \", ir_measures.calc_aggregate([Bpref], qrels, run_file_bm25))\n",
    "\n",
    "print(f\"TFIDF: \", ir_measures.calc_aggregate([Bpref(rel=2)], qrels, run_file_tfidf))\n",
    "print(f\"BM25: \", ir_measures.calc_aggregate([Bpref(rel=2)], qrels, run_file_bm25))\n",
    "\n",
    "print(f\"TFIDF: \", ir_measures.calc_aggregate([Bpref(rel=3)], qrels, run_file_tfidf))\n",
    "print(f\"BM25: \", ir_measures.calc_aggregate([Bpref(rel=3)], qrels, run_file_bm25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF:  {Judged: 0.10595348837209306}\n",
      "BM25:  {Judged: 0.12069767441860466}\n",
      "TFIDF:  {Judged@5: 0.8372093023255816}\n",
      "BM25:  {Judged@5: 0.9813953488372092}\n",
      "TFIDF:  {Judged@10: 0.7581395348837208}\n",
      "BM25:  {Judged@10: 0.9488372093023255}\n",
      "TFIDF:  {Judged@100: 0.41418604651162794}\n",
      "BM25:  {Judged@100: 0.5362790697674419}\n"
     ]
    }
   ],
   "source": [
    "# BPercentage of results in the top k (cutoff) results that have relevance judgments. \n",
    "# Equivalent to P@k with a rel lower than any judgment.\n",
    "\n",
    "print(f\"TFIDF: \", ir_measures.calc_aggregate([Judged], qrels, run_file_tfidf))\n",
    "print(f\"BM25: \", ir_measures.calc_aggregate([Judged], qrels, run_file_bm25))\n",
    "\n",
    "print(f\"TFIDF: \", ir_measures.calc_aggregate([Judged@5], qrels, run_file_tfidf))\n",
    "print(f\"BM25: \", ir_measures.calc_aggregate([Judged@5], qrels, run_file_bm25))\n",
    "\n",
    "print(f\"TFIDF: \", ir_measures.calc_aggregate([Judged@10], qrels, run_file_tfidf))\n",
    "print(f\"BM25: \", ir_measures.calc_aggregate([Judged@10], qrels, run_file_bm25))\n",
    "\n",
    "print(f\"TFIDF: \", ir_measures.calc_aggregate([Judged@100], qrels, run_file_tfidf))\n",
    "print(f\"BM25: \", ir_measures.calc_aggregate([Judged@100], qrels, run_file_bm25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T17:27:51.114391Z",
     "start_time": "2024-11-19T17:27:51.039101Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'count = 0\\nfor metric in ir_measures.iter_calc([P@5, P(rel=2)@5, nDCG@10, AP, AP(rel=2), Bpref, Bpref(rel=2), Judged@10], qrels, run_file):\\n  print(metric)\\n  count += 1\\n  if count >= 10: break # only show top 10 items\\n\\nrun_file_p_rel2_5 = {m.query_id: m.value for m in ir_measures.iter_calc([Bpref(rel=2)], qrels, run_file)}\\n\\nfrom scipy.stats import ttest_rel\\nqids = list(run_file_p_rel2_5.keys())\\n#ttest_rel([run_file_p_rel2_5[v] for v in qids])'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO : da rivedere e capire perchè è qui\n",
    "\n",
    "'''count = 0\n",
    "for metric in ir_measures.iter_calc([P@5, P(rel=2)@5, nDCG@10, AP, AP(rel=2), Bpref, Bpref(rel=2), Judged@10], qrels, run_file):\n",
    "  print(metric)\n",
    "  count += 1\n",
    "  if count >= 10: break # only show top 10 items\n",
    "\n",
    "run_file_p_rel2_5 = {m.query_id: m.value for m in ir_measures.iter_calc([Bpref(rel=2)], qrels, run_file)}\n",
    "\n",
    "from scipy.stats import ttest_rel\n",
    "qids = list(run_file_p_rel2_5.keys())\n",
    "#ttest_rel([run_file_p_rel2_5[v] for v in qids])'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTerrier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T18:00:25.080832Z",
     "start_time": "2024-11-19T18:00:16.846453Z"
    }
   },
   "outputs": [],
   "source": [
    "import pyterrier as pt\n",
    "#import pandas as pd\n",
    "#import os\n",
    "\n",
    "# note that pt.started() and pt.init() are deprecated\n",
    "\n",
    "if not pt.java.started():\n",
    "    pt.java.add_option('-Dtrec.encoding=UTF-8')\n",
    "    pt.java.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       qid                                              query\n",
      "0  1108939                  what slows down the flow of blood\n",
      "1  1112389             what is the county for grand rapids mn\n",
      "2   792752                                     what is ruclip\n",
      "3  1119729  what do you do when you have a nosebleed from ...\n",
      "4  1105095                  where is sugar lake lodge located\n",
      "     qid    docno  label\n",
      "0  19335  1017759      0\n",
      "1  19335  1082489      0\n",
      "2  19335   109063      0\n",
      "3  19335  1160863      0\n",
      "4  19335  1160871      0\n"
     ]
    }
   ],
   "source": [
    "dataset = pt.get_dataset(\"msmarco_passage\")\n",
    "\n",
    "print(dataset.get_topics(\"test-2019\").head())\n",
    "print(dataset.get_qrels(\"test-2019\").head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 8841823\n",
      "Number of terms: 1170682\n",
      "Number of postings: 215238456\n",
      "Number of fields: 1\n",
      "Number of tokens: 288759529\n",
      "Field names: [text]\n",
      "Positions:   false\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print getCollectionStatistics\n",
    "index = pt.IndexFactory.of(dataset.get_index(variant='terrier_stemmed'))\n",
    "print(index.getCollectionStatistics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "9260\n",
      "43\n"
     ]
    }
   ],
   "source": [
    "# print the number of queries\n",
    "print(len(dataset.get_topics(\"test-2019\")))\n",
    "\n",
    "# print the number of relevance judgments\n",
    "print(len(dataset.get_qrels(\"test-2019\")))\n",
    "\n",
    "# print the number of unique queries ids in the qrels\n",
    "print(len(dataset.get_qrels(\"test-2019\")[\"qid\"].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T18:31:51.495278Z",
     "start_time": "2024-11-19T18:31:51.383429Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Load the pickled inverted index\\ndf = pd.read_csv(\"collection/msmarco/msmarco.tsv\", sep=\"\\t\", header=None)\\n\\n# assign columns\\ndf.columns = [\"docno\", \"text\"]\\n\\n# Convert columns to strings\\ndf[\"docno\"] = df[\"docno\"].astype(str)\\ndf[\"text\"] = df[\"text\"].astype(str)\\n\\n\\n# Display the DataFrame to verify\\nprint(df.head())\\n'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Old approach\n",
    "\n",
    "'''\n",
    "# Load the pickled inverted index\n",
    "df = pd.read_csv(\"collection/msmarco/msmarco.tsv\", sep=\"\\t\", header=None)\n",
    "\n",
    "# assign columns\n",
    "df.columns = [\"docno\", \"text\"]\n",
    "\n",
    "# Convert columns to strings\n",
    "df[\"docno\"] = df[\"docno\"].astype(str)\n",
    "df[\"text\"] = df[\"text\"].astype(str)\n",
    "\n",
    "\n",
    "# Display the DataFrame to verify\n",
    "print(df.head())\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T16:12:33.562549Z",
     "start_time": "2024-11-20T16:12:30.199027Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n# Convert DataFrame to a list of dictionaries\\ndocs = [{\"docno\": docno, \"text\": text} for docno, text in zip(df[\"docno\"], df[\"text\"])]\\n\\n# get the root path of the current directory\\ncwd = os.getcwd()\\n# from this get the path to index_3docs\\nindex_path = os.path.join(cwd, \"index_3docs\")\\n\\n# if the folder does not exists or is empty\\nif not os.path.exists(index_path) or len(os.listdir(index_path)) == 0:\\n    # indexer = pt.DFIndexer(\"./index_3docs\", overwrite=True) # deprecated\\n    indexer = pt.IterDictIndexer(index_path, overwrite=True)\\n    indexref = indexer.index(docs)\\n    indexref.toString()\\n    \\n'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Old approach\n",
    "\n",
    "'''\n",
    "\n",
    "# Convert DataFrame to a list of dictionaries\n",
    "docs = [{\"docno\": docno, \"text\": text} for docno, text in zip(df[\"docno\"], df[\"text\"])]\n",
    "\n",
    "# get the root path of the current directory\n",
    "cwd = os.getcwd()\n",
    "# from this get the path to index_3docs\n",
    "index_path = os.path.join(cwd, \"index_3docs\")\n",
    "\n",
    "# if the folder does not exists or is empty\n",
    "if not os.path.exists(index_path) or len(os.listdir(index_path)) == 0:\n",
    "    # indexer = pt.DFIndexer(\"./index_3docs\", overwrite=True) # deprecated\n",
    "    indexer = pt.IterDictIndexer(index_path, overwrite=True)\n",
    "    indexref = indexer.index(docs)\n",
    "    indexref.toString()\n",
    "    \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nindexref = pt.IndexRef.of(os.path.abspath(\"index_3docs/data.properties\"))\\nindex = pt.IndexFactory.of(indexref)\\nprint(index.getCollectionStatistics())\\n'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "indexref = pt.IndexRef.of(os.path.abspath(\"index_3docs/data.properties\"))\n",
    "index = pt.IndexFactory.of(indexref)\n",
    "print(index.getCollectionStatistics())\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport pandas as pd\\nimport re\\nimport string\\n\\n\\n# File path for the input file\\nfile_path = \"collection/msmarco/msmarco-queries.tsv\"  # Replace with your file path\\n# Read the input file\\nqueries = pd.read_csv(file_path, sep=\"\\t\", header=None, names=[\"qid\", \"query\"])\\n\\n# Apply preprocessing to each query\\nqueries[\"processed_query\"] = queries[\"query\"].apply(preprocess)\\n\\n# Save the processed queries to a new file\\noutput_path = \"processed_queries.txt\"\\nqueries[[\"qid\", \"processed_query\"]].to_csv(output_path, sep=\"\\t\", index=False)\\n\\n# Read the processed file with the correct column names\\nprocessed_queries = pd.read_csv(output_path, sep=\"\\t\", names=[\"qid\", \"query\"], skiprows=1)\\n'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Old approach\n",
    "\n",
    "'''\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "\n",
    "\n",
    "# File path for the input file\n",
    "file_path = \"collection/msmarco/msmarco-queries.tsv\"  # Replace with your file path\n",
    "# Read the input file\n",
    "queries = pd.read_csv(file_path, sep=\"\\t\", header=None, names=[\"qid\", \"query\"])\n",
    "\n",
    "# Apply preprocessing to each query\n",
    "queries[\"processed_query\"] = queries[\"query\"].apply(preprocess)\n",
    "\n",
    "# Save the processed queries to a new file\n",
    "output_path = \"processed_queries.txt\"\n",
    "queries[[\"qid\", \"processed_query\"]].to_csv(output_path, sep=\"\\t\", index=False)\n",
    "\n",
    "# Read the processed file with the correct column names\n",
    "processed_queries = pd.read_csv(output_path, sep=\"\\t\", names=[\"qid\", \"query\"], skiprows=1)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport ast\\n\\ndef concatenate_list(query):\\n    \"\"\"\\n    Funzione per concatenare gli elementi di una lista di termini in una stringa.\\n    \"\"\"\\n    try:\\n        query_list = ast.literal_eval(query)  # Converte la stringa in lista\\n        return \" \".join(query_list)  # Concatena gli elementi della lista con uno spazio\\n    except (ValueError, SyntaxError):\\n        return query  # Restituisci il valore originale se non è una lista valida\\n\\n\\n# Step 1: Leggi il file delle query originali\\nfile_path = \"collection/msmarco/msmarco-queries.tsv\"  # Sostituisci con il percorso del file\\nqueries = pd.read_csv(file_path, sep=\"\\t\", header=None, names=[\"qid\", \"query\"])\\n\\n# Step 2: Applica la funzione preprocess a tutte le query\\nqueries[\"processed_query\"] = queries[\"query\"].apply(preprocess)\\n\\n# Step 3: Salva le query preprocessate in un file temporaneo\\ntemp_output_path = \"processed_queries_temp.txt\"\\nqueries[[\"qid\", \"processed_query\"]].to_csv(temp_output_path, sep=\"\\t\", index=False)\\n\\n# Step 4: Rileggi il file temporaneo per applicare concatenate_list\\nprocessed_queries = pd.read_csv(temp_output_path, sep=\"\\t\")\\n\\n# Applica concatenate_list alla colonna \\'processed_query\\'\\nprocessed_queries[\"qid\"] = processed_queries[\"qid\"].astype(str)\\nprocessed_queries[\"query\"] = processed_queries[\"processed_query\"].apply(concatenate_list)\\nprocessed_queries = processed_queries[[\"qid\", \"query\"]]\\n# Step 5: Salva il file finale con le colonne \\'qid\\' e \\'query\\'\\nfinal_output_path = \"final_processed_queries.txt\"\\nprocessed_queries[[\"qid\", \"query\"]].to_csv(final_output_path, sep=\"\\t\", index=False)\\n\\n# Output per conferma\\nprint(f\"File processato salvato in: {final_output_path}\")\\nprint(processed_queries.head())\\n\\nprocessed_queries[\"qid\"] = processed_queries[\"qid\"].astype(str)\\n\\n\\nqrels = pd.read_csv(\\'./collection/\\'+chosen_collection+\\'/\\'+chosen_collection+\\'-qrels.txt\\', sep=\\' \\', header=None)\\nqrels.columns = [\\'qid\\', \\'Q0\\', \\'docno\\', \\'relevance\\']\\nqrels[\\'qid\\'] = qrels[\\'qid\\'].apply(str)\\nqrels[\\'docno\\'] = qrels[\\'docno\\'].apply(str)\\n#qrels[\\'relevance\\'] = qrels[\\'relevance\\'].apply(int)\\n'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Old approach\n",
    "'''\n",
    "import ast\n",
    "\n",
    "def concatenate_list(query):\n",
    "    \"\"\"\n",
    "    Funzione per concatenare gli elementi di una lista di termini in una stringa.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        query_list = ast.literal_eval(query)  # Converte la stringa in lista\n",
    "        return \" \".join(query_list)  # Concatena gli elementi della lista con uno spazio\n",
    "    except (ValueError, SyntaxError):\n",
    "        return query  # Restituisci il valore originale se non è una lista valida\n",
    "\n",
    "\n",
    "# Step 1: Leggi il file delle query originali\n",
    "file_path = \"collection/msmarco/msmarco-queries.tsv\"  # Sostituisci con il percorso del file\n",
    "queries = pd.read_csv(file_path, sep=\"\\t\", header=None, names=[\"qid\", \"query\"])\n",
    "\n",
    "# Step 2: Applica la funzione preprocess a tutte le query\n",
    "queries[\"processed_query\"] = queries[\"query\"].apply(preprocess)\n",
    "\n",
    "# Step 3: Salva le query preprocessate in un file temporaneo\n",
    "temp_output_path = \"processed_queries_temp.txt\"\n",
    "queries[[\"qid\", \"processed_query\"]].to_csv(temp_output_path, sep=\"\\t\", index=False)\n",
    "\n",
    "# Step 4: Rileggi il file temporaneo per applicare concatenate_list\n",
    "processed_queries = pd.read_csv(temp_output_path, sep=\"\\t\")\n",
    "\n",
    "# Applica concatenate_list alla colonna 'processed_query'\n",
    "processed_queries[\"qid\"] = processed_queries[\"qid\"].astype(str)\n",
    "processed_queries[\"query\"] = processed_queries[\"processed_query\"].apply(concatenate_list)\n",
    "processed_queries = processed_queries[[\"qid\", \"query\"]]\n",
    "# Step 5: Salva il file finale con le colonne 'qid' e 'query'\n",
    "final_output_path = \"final_processed_queries.txt\"\n",
    "processed_queries[[\"qid\", \"query\"]].to_csv(final_output_path, sep=\"\\t\", index=False)\n",
    "\n",
    "# Output per conferma\n",
    "print(f\"File processato salvato in: {final_output_path}\")\n",
    "print(processed_queries.head())\n",
    "\n",
    "processed_queries[\"qid\"] = processed_queries[\"qid\"].astype(str)\n",
    "\n",
    "\n",
    "qrels = pd.read_csv('./collection/'+chosen_collection+'/'+chosen_collection+'-qrels.txt', sep=' ', header=None)\n",
    "qrels.columns = ['qid', 'Q0', 'docno', 'relevance']\n",
    "qrels['qid'] = qrels['qid'].apply(str)\n",
    "qrels['docno'] = qrels['docno'].apply(str)\n",
    "#qrels['relevance'] = qrels['relevance'].apply(int)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nqueries_qids = set(processed_queries[\"qid\"])\\nqrels_qids = set(qrels[\"qid\"])\\n\\ncommon_qids = queries_qids & qrels_qids\\nmissing_in_qrels = queries_qids - qrels_qids\\nmissing_in_queries = qrels_qids - queries_qids\\n\\n# Mostra i risultati\\nprint(f\"Numero totale di qid in queries: {len(queries_qids)}\")\\nprint(f\"Numero totale di qid in qrels: {len(qrels_qids)}\")\\nprint(f\"Numero di qid comuni: {len(common_qids)}\")\\nprint(f\"Qid mancanti nei qrels: {len(missing_in_qrels)}\")\\nprint(f\"Qid mancanti nelle queries: {len(missing_in_queries)}\")\\n'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Old approach TODO rewrite with new things\n",
    "'''\n",
    "queries_qids = set(processed_queries[\"qid\"])\n",
    "qrels_qids = set(qrels[\"qid\"])\n",
    "\n",
    "common_qids = queries_qids & qrels_qids\n",
    "missing_in_qrels = queries_qids - qrels_qids\n",
    "missing_in_queries = qrels_qids - queries_qids\n",
    "\n",
    "# Mostra i risultati\n",
    "print(f\"Numero totale di qid in queries: {len(queries_qids)}\")\n",
    "print(f\"Numero totale di qid in qrels: {len(qrels_qids)}\")\n",
    "print(f\"Numero di qid comuni: {len(common_qids)}\")\n",
    "print(f\"Qid mancanti nei qrels: {len(missing_in_qrels)}\")\n",
    "print(f\"Qid mancanti nelle queries: {len(missing_in_queries)}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'queries_qids = set(processed_queries[\"qid\"])\\nqrels_qids = set(qrels[\"qid\"])\\n\\ncommon_qids = queries_qids & qrels_qids\\nmissing_in_qrels = queries_qids - qrels_qids\\nmissing_in_queries = qrels_qids - queries_qids\\n\\n# Mostra i risultati\\nprint(f\"Numero totale di qid in queries: {len(queries_qids)}\")\\nprint(f\"Numero totale di qid in qrels: {len(qrels_qids)}\")\\nprint(f\"Numero di qid comuni: {len(common_qids)}\")\\nprint(f\"Qid mancanti nei qrels: {len(missing_in_qrels)}\")\\nprint(f\"Qid mancanti nelle queries: {len(missing_in_queries)}\")'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import pandas as pd\n",
    "\n",
    "# Percorso del file da leggere\n",
    "input_file = \"collection/msmarco/msmarco-qrels.txt\"  # Sostituisci con il percorso del tuo file\n",
    "output_file = \"filtered_msmarco-qrels.txt\"  # Nome del file di output\n",
    "\n",
    "# Leggi il file con pandas\n",
    "df = pd.read_csv(input_file, sep=\" \", header=None, names=[\"qid\", \"Q0\", \"docno\", \"relevance\"])\n",
    "\n",
    "# Filtra le righe in cui la colonna 'relevance' è diversa da zero\n",
    "filtered_df = df[df[\"relevance\"] != 0]\n",
    "\n",
    "# Salva il risultato in un nuovo file\n",
    "filtered_df.to_csv(output_file, sep=\" \", index=False, header=False)\n",
    "\n",
    "print(f\"File salvato in: {output_file}\")\n",
    "\n",
    "file_path = \"filtered_msmarco-qrels.txt\"\n",
    "\n",
    "# Leggi il file con pandas\n",
    "df = pd.read_csv(file_path, sep=\" \", header=None, names=[\"qid\", \"Q0\", \"docno\", \"relevance\"])\n",
    "\n",
    "# Conta il numero di valori univoci nella colonna 'qid'\n",
    "unique_values_count = df[\"qid\"].nunique()\n",
    "\n",
    "# Stampa il numero di valori univoci\n",
    "print(f\"Numero di valori univoci nella prima colonna ('qid'): {unique_values_count}\")\n",
    "'''\n",
    "\n",
    "\"\"\"queries_qids = set(processed_queries[\"qid\"])\n",
    "qrels_qids = set(qrels[\"qid\"])\n",
    "\n",
    "common_qids = queries_qids & qrels_qids\n",
    "missing_in_qrels = queries_qids - qrels_qids\n",
    "missing_in_queries = qrels_qids - queries_qids\n",
    "\n",
    "# Mostra i risultati\n",
    "print(f\"Numero totale di qid in queries: {len(queries_qids)}\")\n",
    "print(f\"Numero totale di qid in qrels: {len(qrels_qids)}\")\n",
    "print(f\"Numero di qid comuni: {len(common_qids)}\")\n",
    "print(f\"Qid mancanti nei qrels: {len(missing_in_qrels)}\")\n",
    "print(f\"Qid mancanti nelle queries: {len(missing_in_queries)}\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'file_path = \"collection/msmarco/msmarco-qrels.txt\"\\n\\ndf = pd.read_csv(file_path, sep=\" \", header=None, names=[\"qid\", \"Q0\", \"docno\", \"relevance\"])\\n\\n# Conta il numero di valori univoci nella colonna \\'qid\\'\\nunique_values_count = df[\"qid\"].nunique()\\n\\n# Stampa il numero di valori univoci\\nprint(f\"Numero di valori univoci nella prima colonna (\\'qid\\'): {unique_values_count}\")'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''file_path = \"collection/msmarco/msmarco-qrels.txt\"\n",
    "\n",
    "df = pd.read_csv(file_path, sep=\" \", header=None, names=[\"qid\", \"Q0\", \"docno\", \"relevance\"])\n",
    "\n",
    "# Conta il numero di valori univoci nella colonna 'qid'\n",
    "unique_values_count = df[\"qid\"].nunique()\n",
    "\n",
    "# Stampa il numero di valori univoci\n",
    "print(f\"Numero di valori univoci nella prima colonna ('qid'): {unique_values_count}\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyterrier.measures import *\n",
    "\n",
    "queries = dataset.get_topics(\"test-2019\")\n",
    "qrels = dataset.get_qrels(\"test-2019\")\n",
    "\n",
    "# Definizione dei retriever\n",
    "#TF_IDF = pt.terrier.Retriever(index, wmodel=\"TF_IDF\")\n",
    "#BM25 =  pt.terrier.Retriever(index, wmodel=\"BM25\")\n",
    "TF_IDF = pt.terrier.Retriever.from_dataset('msmarco_passage', 'terrier_stemmed', wmodel='TF_IDF')\n",
    "BM25 = pt.terrier.Retriever.from_dataset('msmarco_passage', 'terrier_stemmed', wmodel='BM25')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>AP</th>\n",
       "      <th>AP(judged_only=True)</th>\n",
       "      <th>AP(rel=2,judged_only=True)</th>\n",
       "      <th>AP(rel=3,judged_only=True)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TerrierRetr(TF_IDF)</td>\n",
       "      <td>0.369486</td>\n",
       "      <td>0.486215</td>\n",
       "      <td>0.345577</td>\n",
       "      <td>0.196327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TerrierRetr(BM25)</td>\n",
       "      <td>0.370004</td>\n",
       "      <td>0.486302</td>\n",
       "      <td>0.345722</td>\n",
       "      <td>0.196460</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  name        AP  AP(judged_only=True)  \\\n",
       "0  TerrierRetr(TF_IDF)  0.369486              0.486215   \n",
       "1    TerrierRetr(BM25)  0.370004              0.486302   \n",
       "\n",
       "   AP(rel=2,judged_only=True)  AP(rel=3,judged_only=True)  \n",
       "0                    0.345577                    0.196327  \n",
       "1                    0.345722                    0.196460  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt.Experiment(\n",
    "    [TF_IDF, BM25],\n",
    "    queries,\n",
    "    qrels,\n",
    "    eval_metrics=[AP, AP(judged_only=True), AP(rel=2, judged_only=True), AP(rel=3, judged_only=True)]\n",
    ")\n",
    "\n",
    "# The [Mean] Average Precision ([M]AP). \n",
    "# The average precision of a single query is the mean of the precision scores at each relevant item returned in a search results list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>nDCG@10</th>\n",
       "      <th>nDCG(judged_only=True)@10</th>\n",
       "      <th>nDCG</th>\n",
       "      <th>nDCG(judged_only=True)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TerrierRetr(TF_IDF)</td>\n",
       "      <td>0.47831</td>\n",
       "      <td>0.48248</td>\n",
       "      <td>0.593198</td>\n",
       "      <td>0.635891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TerrierRetr(BM25)</td>\n",
       "      <td>0.47954</td>\n",
       "      <td>0.48371</td>\n",
       "      <td>0.593433</td>\n",
       "      <td>0.635943</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  name  nDCG@10  nDCG(judged_only=True)@10      nDCG  \\\n",
       "0  TerrierRetr(TF_IDF)  0.47831                    0.48248  0.593198   \n",
       "1    TerrierRetr(BM25)  0.47954                    0.48371  0.593433   \n",
       "\n",
       "   nDCG(judged_only=True)  \n",
       "0                0.635891  \n",
       "1                0.635943  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt.Experiment(\n",
    "    [TF_IDF, BM25],\n",
    "    queries,\n",
    "    qrels,\n",
    "    eval_metrics=[nDCG@10, nDCG(judged_only=True)@10, nDCG, nDCG(judged_only=True)]\n",
    ")\n",
    "\n",
    "# The normalized Discounted Cumulative Gain (nDCG). \n",
    "# Uses graded labels - systems that put the highest graded documents at the top of the ranking. \n",
    "# It is normalized wrt. the Ideal NDCG, i.e. documents ranked in descending order of graded label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>Bpref</th>\n",
       "      <th>Bpref(rel=2)</th>\n",
       "      <th>Bpref(rel=3)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TerrierRetr(TF_IDF)</td>\n",
       "      <td>0.494397</td>\n",
       "      <td>0.316205</td>\n",
       "      <td>0.133529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TerrierRetr(BM25)</td>\n",
       "      <td>0.494534</td>\n",
       "      <td>0.316478</td>\n",
       "      <td>0.133637</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  name     Bpref  Bpref(rel=2)  Bpref(rel=3)\n",
       "0  TerrierRetr(TF_IDF)  0.494397      0.316205      0.133529\n",
       "1    TerrierRetr(BM25)  0.494534      0.316478      0.133637"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt.Experiment(\n",
    "    [TF_IDF, BM25],\n",
    "    queries,\n",
    "    qrels,\n",
    "    eval_metrics=[Bpref, Bpref(rel=2), Bpref(rel=3)],\n",
    ")\n",
    "\n",
    "# Binary Preference (Bpref). \n",
    "# This measure examines the relative ranks of judged relevant and non-relevant documents. \n",
    "# Non-judged documents are not considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>Judged</th>\n",
       "      <th>Judged@5</th>\n",
       "      <th>Judged@10</th>\n",
       "      <th>Judged@100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TerrierRetr(TF_IDF)</td>\n",
       "      <td>0.142977</td>\n",
       "      <td>0.976744</td>\n",
       "      <td>0.95814</td>\n",
       "      <td>0.551395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TerrierRetr(BM25)</td>\n",
       "      <td>0.143116</td>\n",
       "      <td>0.976744</td>\n",
       "      <td>0.95814</td>\n",
       "      <td>0.551628</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  name    Judged  Judged@5  Judged@10  Judged@100\n",
       "0  TerrierRetr(TF_IDF)  0.142977  0.976744    0.95814    0.551395\n",
       "1    TerrierRetr(BM25)  0.143116  0.976744    0.95814    0.551628"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt.Experiment(\n",
    "    [TF_IDF, BM25],\n",
    "    queries,\n",
    "    qrels,\n",
    "    eval_metrics=[Judged, Judged@5, Judged@10, Judged@100],\n",
    ")\n",
    "\n",
    "# BPercentage of results in the top k (cutoff) results that have relevance judgments. \n",
    "# Equivalent to P@k with a rel lower than any judgment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pt.Experiment(\\n    [TF_IDF, BM25, PL2],\\n    processed_queries,\\n    qrels,\\n    eval_metrics=[\"map\"],\\n    round={\"map\" : 4},\\n    names=[\\'TF-IDF\\', \\'BM25\\', \\'PL2\\'],\\n    baseline=0,\\n    correction=\\'b\\'\\n)'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''pt.Experiment(\n",
    "    [TF_IDF, BM25, PL2],\n",
    "    processed_queries,\n",
    "    qrels,\n",
    "    eval_metrics=[\"map\"],\n",
    "    round={\"map\" : 4},\n",
    "    names=['TF-IDF', 'BM25', 'PL2'],\n",
    "    baseline=0,\n",
    "    correction='b'\n",
    ")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pt.Experiment(\\n    [TF_IDF, BM25, PL2],\\n    processed_queries,\\n    qrels,\\n    eval_metrics=[\"map\"],\\n    perquery=True\\n)'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''pt.Experiment(\n",
    "    [TF_IDF, BM25, PL2],\n",
    "    processed_queries,\n",
    "    qrels,\n",
    "    eval_metrics=[\"map\"],\n",
    "    perquery=True\n",
    ")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pt.Experiment(\\n    [TF_IDF, BM25, PL2],\\n    processed_queries,\\n    qrels,\\n    eval_metrics=[\"map\"],\\n    round={\"map\" : 4},\\n    names=[\\'TF-IDF\\', \\'BM25\\', \\'PL2\\'],\\n    baseline=0,\\n    correction=\\'b\\',\\n    save_dir=\\'./\\'\\n)'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''pt.Experiment(\n",
    "    [TF_IDF, BM25, PL2],\n",
    "    processed_queries,\n",
    "    qrels,\n",
    "    eval_metrics=[\"map\"],\n",
    "    round={\"map\" : 4},\n",
    "    names=['TF-IDF', 'BM25', 'PL2'],\n",
    "    baseline=0,\n",
    "    correction='b',\n",
    "    save_dir='./'\n",
    ")'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import time\n",
    "\n",
    "\n",
    "# UI elements\n",
    "search_bar = widgets.Text(\n",
    "    placeholder='Type in a query...',\n",
    "    description='Search:',\n",
    "    layout=widgets.Layout(width='80%')\n",
    ")\n",
    "search_button = widgets.Button(\n",
    "    description='Search',\n",
    "    button_style='success',\n",
    "    tooltip='Execute the query',\n",
    "    icon='search'\n",
    ")\n",
    "\n",
    "score_function_rbtn = widgets.RadioButtons(options=['TF-IDF', 'BM25'], description='Scoring function:', disabled=False)\n",
    "algo_rbtn = widgets.RadioButtons(options=['TAAT', 'DAAT'], description='Algorithm:', disabled=False)\n",
    "_style = widgets.HTML(\n",
    "    \"<style>.widget-radio-box {flex-direction: row !important;}.widget-radio-box\"\n",
    "    \" label{margin:2px !important;width: 100px !important;}</style>\",\n",
    "    layout=widgets.Layout(display=\"none\"),\n",
    ")\n",
    "\n",
    "output_area = widgets.Output()\n",
    "\n",
    "\n",
    "def on_search_click(b):\n",
    "    with output_area:\n",
    "        clear_output()  # clean previous output\n",
    "        query = search_bar.value\n",
    "        if not query.strip():\n",
    "            print(\"Please, type in a query.\")\n",
    "            return\n",
    "        \n",
    "        selected_scoring_function = score_function_rbtn.value\n",
    "        print(f\"Selected scoring function: {selected_scoring_function}\")\n",
    "        if selected_scoring_function == 'TF-IDF':\n",
    "            method = 'tfidf'\n",
    "        else:   \n",
    "            method = 'bm25'\n",
    "\n",
    "        selected_algorithm = algo_rbtn.value\n",
    "        print(f\"Selected Algorithm: {selected_algorithm}\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        # --- QUERY EXECUTION ---\n",
    "        processed_query = preprocess(query)\n",
    "        termids = inv_index.get_termids(processed_query)\n",
    "        postings = inv_index.get_postings(termids)\n",
    "        \n",
    "        if selected_algorithm == 'TAAT':\n",
    "            results = taat(postings, method=method)\n",
    "        else:\n",
    "            results = daat(postings, method=method)\n",
    "        # ------------------------\n",
    "        elapsed_time = (time.time() - start_time) * 1000 # convert in ms\n",
    "\n",
    "        # finally show the results\n",
    "        print(f\"Found: {len(results)} documents\\n\")\n",
    "        for res in results:\n",
    "            res = (round(res[0], 4), res[1]) # TODO : si potrebbe spostare direttamente nella score function\n",
    "            print(f\" - {res}\")\n",
    "        print(f\"\\nExecution time: {elapsed_time:.2f} ms\")\n",
    "\n",
    "search_button.on_click(on_search_click)\n",
    "# link search button to the enter key\n",
    "search_bar.continuous_update = False\n",
    "search_bar.observe(on_search_click, names='value')\n",
    "\n",
    "top_row = widgets.HBox([score_function_rbtn,_style])\n",
    "middle_row = widgets.HBox([algo_rbtn,_style])\n",
    "bottom_row = widgets.HBox([search_bar, search_button])\n",
    "\n",
    "# finally display the UI\n",
    "display(widgets.VBox([top_row,middle_row, bottom_row]))\n",
    "display(output_area)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mircv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
