{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/giuliocapecchi/IR_project/blob/main/IR_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iNk06jLCfXXM",
    "outputId": "07943952-15cf-4a6b-d8a5-a373a7715df4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\giuli\\miniconda3\\envs\\mircv\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\giuli\\miniconda3\\envs\\mircv\\lib\\site-packages (3.8.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\giuli\\miniconda3\\envs\\mircv\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\giuli\\miniconda3\\envs\\mircv\\lib\\site-packages (4.66.6)\n",
      "Requirement already satisfied: gdown in c:\\users\\giuli\\miniconda3\\envs\\mircv\\lib\\site-packages (5.2.0)\n",
      "Requirement already satisfied: ir_datasets in c:\\users\\giuli\\miniconda3\\envs\\mircv\\lib\\site-packages (0.5.8)\n",
      "Requirement already satisfied: humanize in c:\\users\\giuli\\miniconda3\\envs\\mircv\\lib\\site-packages (4.11.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\giuli\\miniconda3\\envs\\mircv\\lib\\site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\giuli\\miniconda3\\envs\\mircv\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\giuli\\miniconda3\\envs\\mircv\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\giuli\\miniconda3\\envs\\mircv\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\giuli\\miniconda3\\envs\\mircv\\lib\\site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\giuli\\miniconda3\\envs\\mircv\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\giuli\\miniconda3\\envs\\mircv\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\giuli\\miniconda3\\envs\\mircv\\lib\\site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\giuli\\miniconda3\\envs\\mircv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\giuli\\miniconda3\\envs\\mircv\\lib\\site-packages (from matplotlib) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\giuli\\miniconda3\\envs\\mircv\\lib\\site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: numpy<2,>=1.21 in c:\\users\\giuli\\miniconda3\\envs\\mircv\\lib\\site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\giuli\\miniconda3\\envs\\mircv\\lib\\site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\giuli\\miniconda3\\envs\\mircv\\lib\\site-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\giuli\\miniconda3\\envs\\mircv\\lib\\site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\giuli\\miniconda3\\envs\\mircv\\lib\\site-packages (from matplotlib) (2.9.0)\n",
      "Requirement already satisfied: click in c:\\users\\giuli\\miniconda3\\envs\\mircv\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\giuli\\miniconda3\\envs\\mircv\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\giuli\\miniconda3\\envs\\mircv\\lib\\site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: colorama in c:\\users\\giuli\\miniconda3\\envs\\mircv\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\giuli\\miniconda3\\envs\\mircv\\lib\\site-packages (from gdown) (4.12.3)\n",
      "Requirement already satisfied: requests[socks] in c:\\users\\giuli\\miniconda3\\envs\\mircv\\lib\\site-packages (from gdown) (2.32.3)\n",
      "Requirement already satisfied: inscriptis>=2.2.0 in c:\\users\\giuli\\miniconda3\\envs\\mircv\\lib\\site-packages (from ir_datasets) (2.5.0)\n",
      "Requirement already satisfied: lxml>=4.5.2 in c:\\users\\giuli\\miniconda3\\envs\\mircv\\lib\\site-packages (from ir_datasets) (5.3.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in c:\\users\\giuli\\miniconda3\\envs\\mircv\\lib\\site-packages (from ir_datasets) (6.0.2)\n",
      "Requirement already satisfied: trec-car-tools>=2.5.4 in c:\\users\\giuli\\miniconda3\\envs\\mircv\\lib\\site-packages (from ir_datasets) (2.6)\n",
      "Requirement already satisfied: lz4>=3.1.10 in c:\\users\\giuli\\miniconda3\\envs\\mircv\\lib\\site-packages (from ir_datasets) (4.3.3)\n",
      "Requirement already satisfied: warc3-wet>=0.2.3 in c:\\users\\giuli\\miniconda3\\envs\\mircv\\lib\\site-packages (from ir_datasets) (0.2.5)\n",
      "Requirement already satisfied: warc3-wet-clueweb09>=0.2.5 in c:\\users\\giuli\\miniconda3\\envs\\mircv\\lib\\site-packages (from ir_datasets) (0.2.5)\n",
      "Requirement already satisfied: zlib-state>=0.1.3 in c:\\users\\giuli\\miniconda3\\envs\\mircv\\lib\\site-packages (from ir_datasets) (0.1.9)\n",
      "Requirement already satisfied: ijson>=3.1.3 in c:\\users\\giuli\\miniconda3\\envs\\mircv\\lib\\site-packages (from ir_datasets) (3.3.0)\n",
      "Requirement already satisfied: unlzw3>=0.2.1 in c:\\users\\giuli\\miniconda3\\envs\\mircv\\lib\\site-packages (from ir_datasets) (0.2.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\giuli\\miniconda3\\envs\\mircv\\lib\\site-packages (from beautifulsoup4->gdown) (2.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\giuli\\miniconda3\\envs\\mircv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\giuli\\miniconda3\\envs\\mircv\\lib\\site-packages (from requests[socks]->gdown) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\giuli\\miniconda3\\envs\\mircv\\lib\\site-packages (from requests[socks]->gdown) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\giuli\\miniconda3\\envs\\mircv\\lib\\site-packages (from requests[socks]->gdown) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\giuli\\miniconda3\\envs\\mircv\\lib\\site-packages (from requests[socks]->gdown) (2024.8.30)\n",
      "Requirement already satisfied: cbor>=1.0.0 in c:\\users\\giuli\\miniconda3\\envs\\mircv\\lib\\site-packages (from trec-car-tools>=2.5.4->ir_datasets) (1.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\giuli\\miniconda3\\envs\\mircv\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\giuli\\miniconda3\\envs\\mircv\\lib\\site-packages (from requests[socks]->gdown) (1.7.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch matplotlib nltk tqdm gdown ir_datasets humanize "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q1f7eJ4tFSfq"
   },
   "source": [
    "# 1. Download and prepare the collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chosen_collection can be one of [\"vaswani\", \"MSMARCO\"]\n",
    "\n",
    "chosen_collection = \"MSMARCO\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "id": "8oKwHN5yEPTq",
    "outputId": "99d4c023-de14-406b-93a3-66fc6665972a"
   },
   "outputs": [],
   "source": [
    "import gdown\n",
    "import ir_datasets\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "if chosen_collection not in [\"vaswani\", \"MSMARCO\"]:\n",
    "    raise ValueError(\"chosen_collection must be one of ['vaswani', 'MSMARCO']\")\n",
    "\n",
    "if chosen_collection == \"MSMARCO\":\n",
    "    url_collection = 'https://drive.google.com/uc?id=1_wXJjiwdgc9Kpt7o7atP8oWe-U4Z56hn'\n",
    "    url_stats = 'https://drive.google.com/uc?id=1ruBLKGkKvfFlPMdjVUGkR09_f-528fEz'\n",
    "    url_lex = 'https://drive.google.com/uc?id=151vGXMMwslHM-Vv8vKRATNLazyAXDM1A'\n",
    "    url_inv = 'https://drive.google.com/uc?id=1UUL0SkjFq4V9tNGiabRo27GfH3XsUuyk'\n",
    "    url_doc = 'https://drive.google.com/uc?id=1p-AChVgbUN4nIzFO55-pm8CtCAghobWC'\n",
    "    if not os.path.exists('collection.tsv'):\n",
    "        gdown.download(url_collection, 'collection.tsv', quiet=False)\n",
    "    \n",
    "    \"\"\"os.makedirs('./pickles', exist_ok=True)\n",
    "    if not os.path.exists('./pickles/stats.pkl'):\n",
    "        gdown.download(url_stats, './pickles/stats.pkl', quiet=False)\n",
    "    if not os.path.exists('./pickles/lex.pkl'):\n",
    "        gdown.download(url_lex, './pickles/lex.pkl', quiet=False)\n",
    "    if not os.path.exists('./pickles/inv.pkl'):\n",
    "        gdown.download(url_inv, './pickles/inv.pkl', quiet=False)\n",
    "    if not os.path.exists('./pickles/doc.pkl'):\n",
    "        gdown.download(url_doc, './pickles/doc.pkl', quiet=False)\"\"\"\n",
    "\n",
    "    #df = pd.read_csv('collection.tsv', sep='\\t', header=None, names=['doc_id', 'text'])\n",
    "\n",
    "elif chosen_collection == \"vaswani\":\n",
    "    vaswani_dataset = ir_datasets.load(\"vaswani\")\n",
    "    docs = list(vaswani_dataset.docs_iter())\n",
    "    df = pd.DataFrame(docs)\n",
    "    df['doc_id'] = (df['doc_id'].astype(int) - 1).astype(str)\n",
    "    # rimuovi i \\n da ogni documento\n",
    "    df['text'] = df['text'].str.replace('\\n', ' ')\n",
    "    df.to_csv('vaswani.tsv', sep='\\t', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B-z1wJvFpXQ_",
    "outputId": "efebd7cb-3545-4c5e-dfee-5c6b92bab3de"
   },
   "outputs": [],
   "source": [
    "# #let's not truncate Pandas output too much\n",
    "# pd.set_option('display.max_colwidth', 50) # mettici 150\n",
    "\n",
    "\n",
    "# print(df.head(5)) # returns the first N rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "F3wDCYeRpYX5"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "\n",
    "nltk.download(\"stopwords\", quiet=True)\n",
    "STOPWORDS = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "STEMMER = nltk.stem.PorterStemmer()\n",
    "\n",
    "def preprocess(s):\n",
    "    # lowercasing\n",
    "    s = s.lower()\n",
    "    # ampersand and special chars\n",
    "    s = re.sub(r\"[‘’´“”–-]\", \"'\", s.replace(\"&\", \" and \")) # this replaces & with 'and' and normalises quotes\n",
    "    # acronyms\n",
    "    s = re.sub(r\"\\.(?!(\\S[^. ])|\\d)\", \"\", s) # this removes dots that are not part of an acronym\n",
    "    # remove punctuation\n",
    "    s = s.translate(str.maketrans(string.punctuation, \" \" * len(string.punctuation)))\n",
    "    # strip whitespaces\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    # tokenisation\n",
    "    tokens = [t for t in s.split() if t not in STOPWORDS]\n",
    "    # stemming\n",
    "    tokens = [STEMMER.stem(t) for t in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "vMXjjbwwpZg7"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def profile(f):\n",
    "    def f_timer(*args, **kwargs):\n",
    "        start = time.time()\n",
    "        result = f(*args, **kwargs)\n",
    "        end = time.time()\n",
    "        ms = (end - start) * 1000\n",
    "        print(f\"{f.__name__} ({ms:.3f} ms)\")\n",
    "        return result\n",
    "    return f_timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cGmR6-3EBUwA",
    "outputId": "0fe56195-635b-43db-c154-ee8c851ae46d"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import humanize\n",
    "import os\n",
    "\n",
    "def print_pickled_size(var_name, var):\n",
    "    # If the 'tmp' directory does not exist, we first create it\n",
    "    os.makedirs('./tmp', exist_ok=True)\n",
    "    with open(f\"./tmp/{var_name}.pickle\", 'wb') as f:\n",
    "        pickle.dump(var, f)\n",
    "    print(f'{var_name} requires {humanize.naturalsize(os.path.getsize(f\"./tmp/{var_name}.pickle\"))}')\n",
    "    os.remove(f\"./tmp/{var_name}.pickle\")\n",
    "    os.removedirs('./tmp')\n",
    "\n",
    "\n",
    "def vbyte_encode(number):\n",
    "    bytes_list = bytearray()\n",
    "    while True:\n",
    "        byte = number & 0x7F # Prendi i 7 bit meno significativi -> 0111 1111 = 0x7F\n",
    "        number >>= 7 # Shifta a destra di 7 bit\n",
    "        if number:\n",
    "            bytes_list.append(byte) # Aggiungo i 7 bit al risultato\n",
    "        else:\n",
    "            bytes_list.append(0x80 | byte) # Aggiungo i 7 bit con il bit di continuazione, 0x80 = 1000 0000\n",
    "            break\n",
    "    return bytes(bytes_list)\n",
    "\n",
    "def vbyte_decode(bytes_seq):\n",
    "    number = 0\n",
    "    for i, byte in enumerate(bytes_seq):\n",
    "        number |= (byte & 0x7F) << (7 * i)\n",
    "        if byte & 0x80:\n",
    "            break\n",
    "    return number\n",
    "\n",
    "def decode_concatenated_vbyte(encoded_bytes):\n",
    "    decoded_numbers = []\n",
    "    current_number = 0\n",
    "    shift_amount = 0\n",
    "    \n",
    "    for byte in encoded_bytes:\n",
    "        if byte & 0x80:  # Bit di continuazione trovato, fine del numero\n",
    "            current_number |= (byte & 0x7F) << shift_amount\n",
    "            decoded_numbers.append(current_number)\n",
    "            current_number = 0\n",
    "            shift_amount = 0\n",
    "        else:  # Continuo a comporre il numero\n",
    "            current_number |= (byte & 0x7F) << shift_amount\n",
    "            shift_amount += 7\n",
    "    \n",
    "    return decoded_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "GEMd0ALMpabE"
   },
   "outputs": [],
   "source": [
    "# from collections import namedtuple\n",
    "\n",
    "\n",
    "# class MSMarcoDataset:\n",
    "#     \"\"\"\n",
    "#     This class that takes the dataframe we created before with columns 'docno' and 'text', and creates a list of namedtuples\n",
    "#     \"\"\"\n",
    "#     def __init__(self, df):\n",
    "#         self.docs = [Document(row.doc_id, row.text) for row in df.itertuples()]\n",
    "\n",
    "#     def docs_iter(self):\n",
    "#         return iter(self.docs)\n",
    "\n",
    "#     def docs_count(self):\n",
    "#         return len(self.docs)\n",
    "\n",
    "# Document = namedtuple('Document', ['doc_id', 'text']) # must define what a document is\n",
    "\n",
    "\n",
    "# from collections import Counter\n",
    "# from tqdm.auto import tqdm\n",
    "\n",
    "# @profile\n",
    "# def old_build_index(dataset):\n",
    "#     lexicon = {}\n",
    "#     doc_index = []\n",
    "#     inv_d, inv_f = {}, {}\n",
    "#     termid = 0\n",
    "\n",
    "#     num_docs = 0\n",
    "#     total_dl = 0\n",
    "#     total_toks = 0\n",
    "#     for docid, doc in tqdm(enumerate(dataset.docs_iter()), desc='Indexing', total=dataset.docs_count()):\n",
    "#         tokens = preprocess(doc.text)\n",
    "#         #print(tokens)\n",
    "#         token_tf = Counter(tokens)\n",
    "#         for token, tf in token_tf.items():\n",
    "#             if token not in lexicon:\n",
    "#                 lexicon[token] = [termid, 0, 0]\n",
    "#                 inv_d[termid], inv_f[termid] =  [], []\n",
    "#                 termid += 1\n",
    "#             token_id = lexicon[token][0] # prendo il termid\n",
    "#             inv_d[token_id].append(docid) # aggiungo il docid alla lista dei docid in cui compare il termine\n",
    "#             inv_f[token_id].append(tf) # aggiungo il tf alla lista dei tf in cui compare il termine\n",
    "#             lexicon[token][1] += 1 # incremento il df\n",
    "#             lexicon[token][2] += tf # tf è quanto compare il termine nel documento\n",
    "#         doclen = len(tokens)\n",
    "#         doc_index.append((str(doc.doc_id), doclen))\n",
    "#         total_dl += doclen\n",
    "#         num_docs += 1\n",
    "\n",
    "#     # Compress the inv_d and inv_f lists\n",
    "#     for term, (termid, df, _) in lexicon.items():\n",
    "#         # Compress the docids\n",
    "#         encoded_list = [vbyte_encode(x) for x in inv_d[termid]]\n",
    "#         concatenated_encoded = b''.join(encoded_list)\n",
    "#         assert inv_d[termid] == decode_concatenated_vbyte(concatenated_encoded), \"Compression/Decompression mismatch!\"\n",
    "#         inv_d[termid] = concatenated_encoded\n",
    "#         # Compress the frequencies\n",
    "#         encoded_list = [vbyte_encode(x) for x in inv_f[termid]]\n",
    "#         concatenated_encoded = b''.join(encoded_list)\n",
    "#         assert inv_f[termid] == decode_concatenated_vbyte(concatenated_encoded), \"Compression/Decompression mismatch!\"\n",
    "#         inv_f[termid] = concatenated_encoded\n",
    "        \n",
    "\n",
    "    \n",
    "#     stats = {\n",
    "#         'num_docs': 1 + docid, # docid starts from 0\n",
    "#         'num_terms': len(lexicon),\n",
    "#         'num_tokens': total_dl,\n",
    "#     }\n",
    "#     return lexicon, {'docids': inv_d, 'freqs': inv_f}, doc_index, stats\n",
    "\n",
    "\n",
    "# dataset = MSMarcoDataset(df)\n",
    "\n",
    "# expected_lexicon, expected_inv, expected_doc_index, expected_stats = old_build_index(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First preprocess the documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing documents: 100%|██████████| 8841823/8841823 [10:01<00:00, 14705.30it/s]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from collections import Counter\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "\n",
    "nltk.download(\"stopwords\", quiet=True)\n",
    "STOPWORDS = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "STEMMER = nltk.stem.PorterStemmer()\n",
    "\n",
    "def process_document(docid, text):\n",
    "    \"\"\"\n",
    "    Preprocess a document, then returns the docid, the term frequencies, and the doclen. \n",
    "    \"\"\"\n",
    "    try:\n",
    "        # lowercasing, special chars, quotes, ampersand\n",
    "        text = text.lower()\n",
    "        text = re.sub(r\"[‘’´“”–-]\", \"'\", text.replace(\"&\", \" and \"))\n",
    "        text = re.sub(r\"\\.(?!(\\S[^. ])|\\d)\", \"\", text)\n",
    "        text = text.translate(str.maketrans(string.punctuation, \" \" * len(string.punctuation)))\n",
    "        text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "        # tokenisation, stopword removal, stemming\n",
    "        tokens = [STEMMER.stem(t) for t in text.split() if t not in STOPWORDS]\n",
    "        \n",
    "        # term frequencies and document length\n",
    "        token_tf = Counter(tokens) \n",
    "        token_tf = dict(token_tf) # just to have a serializable object\n",
    "        doclen = len(tokens) # document length\n",
    "\n",
    "        return (int(docid), token_tf, doclen)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing document {docid}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def process_in_chunks(file_path, batch_size=10000, num_cores=-1):\n",
    "    \"\"\"\n",
    "    Process a TSV file in chunks of batch_size, using num_cores CPU cores (default: all). \n",
    "    \"\"\"\n",
    "    results = []\n",
    "    docid_counter = 0  # counter for docids\n",
    "\n",
    "    # count the total number of lines in the file, just for see tqdm progress bar\n",
    "    with open(file_path, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        total_lines = sum(1 for _ in f) \n",
    "\n",
    "    with open(file_path, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\"\\t\")\n",
    "        batch = []\n",
    "        for line in tqdm(reader, desc=\"Processing documents\", total=total_lines):\n",
    "            text = line[1] \n",
    "            batch.append((docid_counter, text)) \n",
    "            docid_counter += 1\n",
    "            # when the batch is full, process it\n",
    "            if len(batch) >= batch_size:\n",
    "                batch_results = Parallel(n_jobs=num_cores)( \n",
    "                    delayed(process_document)(docid, text) for docid, text in batch\n",
    "                )\n",
    "                results.extend(batch_results) # append the results to the final list\n",
    "                batch = []  # reset the batch\n",
    "\n",
    "        # process the last batch\n",
    "        if batch:\n",
    "            batch_results = Parallel(n_jobs=num_cores)(\n",
    "                delayed(process_document)(docid, text) for docid, text in batch\n",
    "            )\n",
    "            results.extend(batch_results)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "if chosen_collection == \"vaswani\":\n",
    "    results = process_in_chunks(\"vaswani.tsv\")\n",
    "elif chosen_collection == \"MSMARCO\": # requires approx 11 minutes\n",
    "    results = process_in_chunks(\"collection.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created the dataframe and freed memory\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# time : approx 5 minutes\n",
    "import gc\n",
    "\n",
    "# leggi e metti dentro df 'processed_collection.csv'\n",
    "df = pd.DataFrame(results, columns=[\"doc_id\", \"token_tf\", \"doclen\"])\n",
    "\n",
    "del results  # Libera la memoria\n",
    "gc.collect()\n",
    "\n",
    "print(\"created the dataframe and freed memory\")\n",
    "df.to_csv(\"processed_collection.csv\", index=False)\n",
    "del df \n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving batches: 100%|██████████| 9/9 [00:08<00:00,  1.06it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "210"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# time : approx 35s\n",
    "from tqdm import trange\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "    \n",
    "if os.path.exists('./results_parquets'):\n",
    "    shutil.rmtree('./results_parquets')\n",
    "os.makedirs('./results_parquets', exist_ok=True)\n",
    "\n",
    "df = pd.read_csv(\"processed_collection.csv\")\n",
    "batch_size = 1_000_000 # rows per batch\n",
    "num_files = (len(df) + batch_size - 1) // batch_size  # number of files to save\n",
    "\n",
    "for i in trange(num_files, desc=\"Saving batches\"):\n",
    "    start_row = i * batch_size\n",
    "    end_row = start_row + batch_size\n",
    "    df_batch = df.iloc[start_row:end_row]\n",
    "    \n",
    "    df_batch.to_parquet(f\"./results_parquets/output_batch_{i+1}.parquet\", index=False)\n",
    "\n",
    "del df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection size: 3.1 GB\n",
      "Processed collection size: 1.2 GB\n"
     ]
    }
   ],
   "source": [
    "if chosen_collection == \"vaswani\":\n",
    "    print(f\"Collection size: {humanize.naturalsize(os.path.getsize('vaswani.tsv'))}\")\n",
    "elif chosen_collection == \"MSMARCO\":\n",
    "    print(f\"Collection size: {humanize.naturalsize(os.path.getsize('collection.tsv'))}\")\n",
    "\n",
    "\n",
    "# print os size of the parquet files inside results_parquets\n",
    "total_size = 0\n",
    "for file in os.listdir(\"./results_parquets\"):\n",
    "    total_size += os.path.getsize(os.path.join(\"./results_parquets\", file))\n",
    "print(f\"Processed collection size: {humanize.naturalsize(total_size)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we utilize the preprocessed results (which will be more manageable in memory) to build the final structures:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing file batches: 100%|██████████| 5/5 [09:00<00:00, 108.04s/it]\n",
      "Compressing: 100%|██████████| 1793595/1793595 [08:37<00:00, 3465.21it/s] \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "import ast\n",
    "\n",
    "# break the list into chunks of size chunk_size\n",
    "def chunk_list(data, chunk_size):\n",
    "    for i in range(0, len(data), chunk_size):\n",
    "        yield data[i:i + chunk_size]\n",
    "\n",
    "# elaborate a single parquet file\n",
    "def process_parquet(parquet_file):\n",
    "    lexicon = {}\n",
    "    doc_index = []\n",
    "    inv_d = defaultdict(list)\n",
    "    inv_f = defaultdict(list)\n",
    "    termid = 0\n",
    "    num_docs = 0\n",
    "    total_dl = 0\n",
    "\n",
    "    df = pd.read_parquet(parquet_file)\n",
    "\n",
    "    for result in df.itertuples(index=False, name=None):\n",
    "        docid, token_tf, doclen = result\n",
    "        if isinstance(token_tf, str):\n",
    "            try:\n",
    "                token_tf = ast.literal_eval(token_tf) # convert the string back to a dictionary\n",
    "                #print(\"Parsed token_tf\", token_tf)\n",
    "            except Exception as e:\n",
    "                print(f\"Error parsing token_tf for document {docid}: {e}\")\n",
    "                continue\n",
    "            \n",
    "\n",
    "        for token, tf in token_tf.items():\n",
    "            if token not in lexicon:\n",
    "                lexicon[token] = [termid, 0, 0]  # termid, df, tf\n",
    "                termid += 1\n",
    "\n",
    "            token_id = lexicon[token][0]\n",
    "            inv_d[token_id].append(docid)\n",
    "            inv_f[token_id].append(tf)\n",
    "            lexicon[token][1] += 1  # increase document frequency (df)\n",
    "            lexicon[token][2] += tf  # increase term frequency (tf)\n",
    "\n",
    "        # finally add the document to the doc_index\n",
    "        doc_index.append((str(docid), doclen))\n",
    "        total_dl += doclen\n",
    "        num_docs += 1\n",
    "\n",
    "    return lexicon, inv_d, inv_f, doc_index, num_docs, total_dl\n",
    "\n",
    "\n",
    "\n",
    "# combine the results from all the parquet files\n",
    "def combine_results(results):\n",
    "    lexicon = {}\n",
    "    inv_d = defaultdict(list)\n",
    "    inv_f = defaultdict(list)\n",
    "    doc_index = []\n",
    "    num_docs = 0\n",
    "    total_dl = 0\n",
    "\n",
    "    for lex, inv_d_part, inv_f_part, doc_index_part, part_num_docs, part_total_dl in results:\n",
    "        lexicon.update(lex) # with update we merge the dictionaries\n",
    "        for termid, docids in inv_d_part.items():\n",
    "            inv_d[termid].extend(docids)\n",
    "        for termid, freqs in inv_f_part.items():\n",
    "            inv_f[termid].extend(freqs)\n",
    "        doc_index.extend(doc_index_part)\n",
    "        num_docs += part_num_docs\n",
    "        total_dl += part_total_dl\n",
    "\n",
    "\n",
    "    # compress the inv_d and inv_f lists\n",
    "    for term, (termid, df, _) in tqdm(lexicon.items(), desc=\"Compressing\"):\n",
    "        # compress the docids\n",
    "        encoded_list = [vbyte_encode(x) for x in inv_d[termid]]\n",
    "        concatenated_encoded = b''.join(encoded_list)\n",
    "        # assert inv_d[termid] == decode_concatenated_vbyte(concatenated_encoded), \"Compression/Decompression mismatch!\"\n",
    "        inv_d[termid] = concatenated_encoded\n",
    "        # compress the frequencies\n",
    "        encoded_list = [vbyte_encode(x) for x in inv_f[termid]]\n",
    "        concatenated_encoded = b''.join(encoded_list)\n",
    "        #assert inv_f[termid] == decode_concatenated_vbyte(concatenated_encoded), \"Compression/Decompression mismatch!\"\n",
    "        inv_f[termid] = concatenated_encoded\n",
    "\n",
    "    return lexicon, {'docids': dict(inv_d), 'freqs': dict(inv_f)}, doc_index, num_docs, total_dl\n",
    "\n",
    "\n",
    "def build_index_from_parquet_files(parquet_dir, batch_size=2):\n",
    "    \"\"\"\n",
    "    lex,inv,doc_index,stats from a directory containing Parquet files.\n",
    "    \"\"\"\n",
    "    # first we get the list of Parquet files from the directory\n",
    "    parquet_files = [os.path.join(parquet_dir, f) for f in os.listdir(parquet_dir) if f.endswith('.parquet')]\n",
    "\n",
    "    # split the list of files into batches (by default 2 files per batch)\n",
    "    file_batches = list(chunk_list(parquet_files, batch_size))\n",
    "\n",
    "    # process each batch in parallel\n",
    "    all_results = []\n",
    "    for batch in tqdm(file_batches, desc=\"Processing file batches\"):\n",
    "        results = list(\n",
    "            Parallel(n_jobs=-1, backend='loky')(delayed(process_parquet)(parquet_file) for parquet_file in batch)\n",
    "        )\n",
    "        all_results.extend(results)\n",
    "\n",
    "    # finally combine all the results\n",
    "    lexicon, inv, doc_index, num_docs, total_dl = combine_results(all_results)\n",
    "\n",
    "    stats = {\n",
    "        'num_docs': num_docs,\n",
    "        'num_terms': len(lexicon),\n",
    "        'num_tokens': total_dl,\n",
    "    }\n",
    "\n",
    "    return lexicon, inv, doc_index, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "ceced8dd-3e3b-4b72-a0b7-2d6d5a2c2a19"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class InvertedIndex:\n",
    "\n",
    "    class PostingListIterator:\n",
    "        def __init__(self, docids, freqs, doc):\n",
    "            self.docids = docids\n",
    "            self.freqs = freqs\n",
    "            self.pos = 0\n",
    "            self.doc = doc\n",
    "\n",
    "        def docid(self):\n",
    "            if self.is_end_list():\n",
    "                return math.inf\n",
    "            return self.docids[self.pos]\n",
    "\n",
    "        def score(self):\n",
    "            if self.is_end_list():\n",
    "                return math.inf\n",
    "            return self.freqs[self.pos]/self.doc[self.docid()][1]\n",
    "\n",
    "        def next(self, target = None):\n",
    "            if not target:\n",
    "                if not self.is_end_list():\n",
    "                    self.pos += 1\n",
    "            else:\n",
    "                if target > self.docid():\n",
    "                    try:\n",
    "                        self.pos = self.docids.index(target, self.pos)\n",
    "                    except ValueError:\n",
    "                        self.pos = len(self.docids)\n",
    "\n",
    "        def is_end_list(self):\n",
    "            return self.pos == len(self.docids)\n",
    "\n",
    "\n",
    "        def len(self):\n",
    "            return len(self.docids)\n",
    "\n",
    "\n",
    "    def __init__(self, lex, inv, doc, stats):\n",
    "        self.lexicon = lex\n",
    "        self.inv = inv\n",
    "        self.doc = doc\n",
    "        self.stats = stats\n",
    "\n",
    "    def num_docs(self):\n",
    "        return self.stats['num_docs']\n",
    "\n",
    "    def get_posting(self, termid):\n",
    "        # Extract the encoded docids and freqs\n",
    "        docids_encoded = self.inv['docids'][termid]\n",
    "        freqs_encoded = self.inv['freqs'][termid]\n",
    "        # Decode the docids and freqs before returning the iterator\n",
    "        docids = decode_concatenated_vbyte(docids_encoded)\n",
    "        freqs = decode_concatenated_vbyte(freqs_encoded)\n",
    "    \n",
    "        return InvertedIndex.PostingListIterator(docids, freqs, self.doc)\n",
    "    \n",
    "\n",
    "    def get_termids(self, tokens):\n",
    "        return [self.lexicon[token][0] for token in tokens if token in self.lexicon]\n",
    "\n",
    "    def get_postings(self, termids):\n",
    "        return [self.get_posting(termid) for termid in termids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now build up the index for the chosen collection. It is built only if a pickled version of its components doesn't exist already :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index loaded from pickles\n",
      "Numero di documenti: 8841823\n",
      "Numero di termini nel lessico: 1793595\n",
      "Totale token: 306696510\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "# If the 'pickles' directory does not exist, we first create it\n",
    "os.makedirs('./pickles', exist_ok=True)\n",
    "\n",
    "try: # try to open the pickled files, else build the index\n",
    "    with open('./pickles/lex.pkl', 'rb') as f:\n",
    "        lex = pickle.load(f)\n",
    "    with open('./pickles/inv.pkl', 'rb') as f:\n",
    "        inv = pickle.load(f)\n",
    "    with open('./pickles/doc.pkl', 'rb') as f:\n",
    "        doc = pickle.load(f)\n",
    "    with open('./pickles/stats.pkl', 'rb') as f:\n",
    "        stats = pickle.load(f)\n",
    "    print(\"Index loaded from pickles\")\n",
    "\n",
    "except:\n",
    "    lex, inv, doc, stats = build_index_from_parquet_files(\"results_parquets\", batch_size=2)\n",
    "    print(\"lex, inv, doc, and stats built, now pickling them\")\n",
    "\n",
    "    # pickle lex, inv, doc, stats\n",
    "    with open('./pickles/lex.pkl', 'wb') as f:\n",
    "        pickle.dump(lex, f)\n",
    "\n",
    "    with open('./pickles/inv.pkl','wb') as f:\n",
    "        pickle.dump(inv, f)\n",
    "\n",
    "    with open('./pickles/doc.pkl', 'wb') as f:\n",
    "        pickle.dump(doc, f)\n",
    "\n",
    "    with open('./pickles/stats.pkl', 'wb') as f:\n",
    "        pickle.dump(stats, f)\n",
    "\n",
    "    print(\"Index built and pickled\")\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Numero di documenti: {stats['num_docs']}\")\n",
    "print(f\"Numero di termini nel lessico: {stats['num_terms']}\")\n",
    "print(f\"Totale token: {stats['num_tokens']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "if chosen_collection == \"vaswani\":\n",
    "    # delete the pickles folder\n",
    "    import shutil\n",
    "    shutil.rmtree('./pickles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try:\n",
    "#     assert lex == expected_lexicon, \"Lexicon does not match expected\"\n",
    "    \n",
    "#     # Ordinare le liste prima di confrontarle\n",
    "#     # assert sorted(inv['docids']) == sorted(expected_inv['docids']), \"Inverted document index does not match expected\"\n",
    "#     # assert sorted(inv['freqs']) == sorted(expected_inv['freqs']), \"Inverted frequencies do not match expected\"\n",
    "    \n",
    "#     assert inv['docids'] == expected_inv['docids'], \"Inverted document index does not match expected\"\n",
    "#     assert inv['freqs'] == expected_inv['freqs'], \"Inverted frequencies do not match expected\"\n",
    "    \n",
    "    \n",
    "    \n",
    "#     # Ordinare l'indice dei documenti se è una lista\n",
    "#     assert doc == expected_doc_index, \"Document index does not match expected\"\n",
    "    \n",
    "#     # Confronto per le statistiche, se sono dizionari o strutture simili\n",
    "#     assert stats == expected_stats, \"Stats do not match expected\"\n",
    "    \n",
    "# except AssertionError as e:\n",
    "#     print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********************************************************************************\n",
      "First 5 elements of lexicon:\n",
      "[('presenc', [3779, 2700, 2950]), ('commun', [2596, 14595, 18302]), ('amid', [36147, 254, 273]), ('scientif', [3013, 2508, 2893]), ('mind', [252, 5448, 6117])]\n",
      "\n",
      "First 5 elements of inv:\n",
      "\tFirst 5 elements of inverted index docids:\n",
      "\t {0: [0, 61, 203, 235, 323], 1: [0, 10, 11, 12, 13], 2: [128, 110, 129, 40, 169], 3: [0, 434, 461, 699, 1541], 4: [0, 41, 183, 184, 604]}\n",
      "\tFirst 5 elements of inverted index freqs:\n",
      "\t {0: [1, 1, 2, 1, 1], 1: [1, 1, 2, 2, 3], 2: [129, 129, 129, 129, 129], 3: [2, 1, 1, 1, 1], 4: [1, 1, 1, 1, 1]}\n",
      "\n",
      "First 5 elements of document index:\n",
      "[('0', 27), ('1', 19), ('2', 24), ('3', 30), ('4', 31)]\n",
      "\n",
      "Stats:\n",
      "{'num_docs': 8841823, 'num_terms': 1793595, 'num_tokens': 306696510} \n",
      "**********************************************************************************\n"
     ]
    }
   ],
   "source": [
    "def print_index(lex, inv, doc, stats): \n",
    "    print(\"**********************************************************************************\\nFirst 5 elements of lexicon:\")\n",
    "    print(list(lex.items())[:5])\n",
    "\n",
    "    print(\"\\nFirst 5 elements of inv:\\n\\tFirst 5 elements of inverted index docids:\")\n",
    "    print(\"\\t\",{k: decode_concatenated_vbyte(v)[:5] for k, v in sorted(list(inv['docids'].items()))[:5]})\n",
    "    print(\"\\tFirst 5 elements of inverted index freqs:\")\n",
    "    print(\"\\t\",{k: decode_concatenated_vbyte(v)[:5] for k, v in list(inv['freqs'].items())[:5]})\n",
    "\n",
    "    print(\"\\nFirst 5 elements of document index:\")\n",
    "    print(doc[:5])\n",
    "\n",
    "    print(\"\\nStats:\")\n",
    "    print(stats,\"\\n**********************************************************************************\")\n",
    "\n",
    "print_index(lex, inv, doc, stats)\n",
    "#print_index(expected_lexicon, expected_inv, expected_doc_index, expected_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have all the elements necessary to build the Inverted Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inv_index requires 1.6 GB\n"
     ]
    }
   ],
   "source": [
    "inv_index = InvertedIndex(lex, inv, doc, stats)\n",
    "\n",
    "# save the InvertedIndex object to a pickle file\n",
    "with open('./pickles/inv_index.pkl', 'wb') as f:\n",
    "    pickle.dump(inv_index, f)\n",
    "\n",
    "# print the size of the inv_index pickle file\n",
    "with open('./pickles/inv_index.pkl', 'rb') as f:\n",
    "    inv_index_size = humanize.naturalsize(os.path.getsize(f\"./pickles/inv_index.pkl\"))\n",
    "    print(f\"inv_index requires {inv_index_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# old_inv_index = InvertedIndex(expected_lexicon,expected_inv,expected_doc_index,expected_stats)\n",
    "# print_pickled_size('old_inv_index',old_inv_index)\n",
    "\n",
    "\n",
    "# assert inv_index.num_docs() == old_inv_index.num_docs(), \"Not equal\"\n",
    "# assert inv_index.get_termids([\"the\", \"cat\"]) == old_inv_index.get_termids([\"the\", \"cat\"]), \"Not equal\"\n",
    "\n",
    "# postings = zip(inv_index.get_postings([0, 1, 100]), old_inv_index.get_postings([0, 1,100]))\n",
    "# for posting, old_posting in postings:\n",
    "#     while not posting.is_end_list():\n",
    "#         assert posting.docid() == old_posting.docid(), \"Not equal\"\n",
    "#         assert posting.score() == old_posting.score(), \"Not equal\"\n",
    "#         posting.next() # TODO sEI QUI , adavance?\n",
    "#         old_posting.next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Download and prepare queries\n",
    "\n",
    "Aggiungiamo anche le query per il dataset scelto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YccWSsil0ZQF",
    "outputId": "937f5b1e-9a31-499e-8206-c00ab1f8568d"
   },
   "outputs": [],
   "source": [
    "if chosen_collection == \"MSMARCO\":\n",
    "    if not os.path.exists('queries.tar.gz'):\n",
    "        url = 'https://msmarco.z22.web.core.windows.net/msmarcoranking/queries.tar.gz'\n",
    "        gdown.download(url, 'queries.tar.gz', quiet=False)\n",
    "        !tar -xzf queries.tar.gz\n",
    "    queries = pd.read_csv('queries.eval.tsv', sep='\\t', header=None)\n",
    "    print(\"Number of queries: \",len(queries))\n",
    "elif chosen_collection == \"vaswani\":\n",
    "    # Converte le query in un DataFrame\n",
    "    queries = pd.DataFrame(vaswani_dataset.queries_iter())\n",
    "    queries.columns = ['qid', 'query']\n",
    "    print(queries.head(2))\n",
    "    print(\"Number of queries: \",len(list(vaswani_dataset.queries_iter()))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(queries.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "\n",
    "class MSMarcoQueries:\n",
    "    def __init__(self, df):\n",
    "        self.queries = [Query(row.query_id, row.text) for row in df.itertuples()]\n",
    "\n",
    "    def queries_iter(self):\n",
    "        return iter(self.queries)\n",
    "\n",
    "    def queries_count(self):\n",
    "        return len(self.queries)\n",
    "\n",
    "\n",
    "Query = namedtuple('Query', ['query_id', 'text'])\n",
    "\n",
    "\n",
    "queries.columns = ['query_id', 'text']\n",
    "queries_dataset = MSMarcoQueries(queries)\n",
    "print(\"The number of queries is: \", queries_dataset.queries_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's prepare the functions necessary to perform TAAT and DAAT query processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need a TopQueue class, which stores the top  K  (score, docid) tuples, using an heap "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "\n",
    "class TopQueue:\n",
    "    def __init__(self, k=10, threshold=0.0):\n",
    "        self.queue = []\n",
    "        self.k = k\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.queue)\n",
    "\n",
    "    def would_enter(self, score):\n",
    "        return score > self.threshold\n",
    "\n",
    "    def clear(self, new_threshold=None):\n",
    "        self.queue = []\n",
    "        if new_threshold:\n",
    "            self.threshold = new_threshold\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'<{self.size()} items, th={self.threshold} {self.queue}'\n",
    "\n",
    "    def insert(self, docid, score):\n",
    "        if score > self.threshold:\n",
    "            if self.size() >= self.k:\n",
    "                heapq.heapreplace(self.queue, (score, docid))\n",
    "            else:\n",
    "                heapq.heappush(self.queue, (score, docid))\n",
    "            if self.size() >= self.k:\n",
    "                self.threshold = max(self.threshold, self.queue[0][0])\n",
    "            return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TAAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def taat(postings, k=10):\n",
    "    A = defaultdict(float)\n",
    "    for posting in postings:\n",
    "        current_docid = posting.docid()\n",
    "        while current_docid != math.inf:\n",
    "            A[current_docid] += posting.score()\n",
    "            posting.next()\n",
    "            current_docid = posting.docid()\n",
    "    top = TopQueue(k)\n",
    "    for docid, score in A.items():\n",
    "        top.insert(docid, score)\n",
    "    return sorted(top.queue, reverse=True)\n",
    "\n",
    "def query_process(query, index):\n",
    "    qtokens = set(preprocess(query))\n",
    "    qtermids = index.get_termids(qtokens)\n",
    "    postings = index.get_postings(qtermids)\n",
    "    return taat(postings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DAAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def min_docid(postings):\n",
    "    min_docid = math.inf\n",
    "    for p in postings:\n",
    "        if not p.is_end_list():\n",
    "            min_docid = min(p.docid(), min_docid)\n",
    "    return min_docid\n",
    "\n",
    "def daat(postings, k=10):\n",
    "    top = TopQueue(k)\n",
    "    current_docid = min_docid(postings)\n",
    "    while current_docid != math.inf:\n",
    "        score = 0\n",
    "        next_docid = math.inf\n",
    "        for posting in postings:\n",
    "            if posting.docid() == current_docid:\n",
    "                score += posting.score()\n",
    "                posting.next()\n",
    "            if not posting.is_end_list():\n",
    "                next_docid = posting.docid()\n",
    "        top.insert(current_docid, score)\n",
    "        current_docid = next_docid\n",
    "    return sorted(top.queue, reverse=True)\n",
    "\n",
    "def query_process(query, index):\n",
    "    qtokens = set(preprocess(query))\n",
    "    qtermids = index.get_termids(qtokens)\n",
    "    postings = index.get_postings(qtermids)\n",
    "    return daat(postings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u-UQcCcTkNo-"
   },
   "outputs": [],
   "source": [
    "@profile\n",
    "def query_processing(queries_iter, fn):\n",
    "    for q in queries_iter:\n",
    "        query = preprocess(q.text)\n",
    "        termids = inv_index.get_termids(query)\n",
    "        postings = inv_index.get_postings(termids)\n",
    "        res = fn(postings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 294
    },
    "id": "1Nn02ZlSkiUP",
    "outputId": "76de8ac6-4d0e-4e28-ecc7-7ddbbd11b9de"
   },
   "outputs": [],
   "source": [
    "query_processing(queries_dataset.queries_iter(), taat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KuEay64mkrR1"
   },
   "outputs": [],
   "source": [
    "query_processing(queries_dataset.queries_iter(), daat)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "mircv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
