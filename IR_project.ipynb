{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/giuliocapecchi/IR_project/blob/main/IR_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iNk06jLCfXXM",
    "outputId": "07943952-15cf-4a6b-d8a5-a373a7715df4"
   },
   "outputs": [],
   "source": [
    "!pip install torch matplotlib nltk tqdm gdownl ir_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q1f7eJ4tFSfq"
   },
   "source": [
    "# 1. Download and prepare the collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chosen_collection can be one of [\"vaswani\", \"MSMARCO\"]\n",
    "\n",
    "chosen_collection = \"vaswani\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "collapsed": true,
    "id": "8oKwHN5yEPTq",
    "outputId": "99d4c023-de14-406b-93a3-66fc6665972a"
   },
   "outputs": [],
   "source": [
    "import gdown\n",
    "import ir_datasets\n",
    "import pandas as pd\n",
    "\n",
    "if chosen_collection != \"vaswani\" and chosen_collection != \"MSMARCO\":\n",
    "    raise ValueError(\"chosen_collection must be one of ['vaswani', 'MSMARCO']\")\n",
    "\n",
    "if chosen_collection == \"MSMARCO\":\n",
    "    url = 'https://drive.google.com/uc?id=1_wXJjiwdgc9Kpt7o7atP8oWe-U4Z56hn'\n",
    "    gdown.download(url, 'collection.tsv', quiet=False)\n",
    "\n",
    "    \"\"\"\n",
    "    read 'collection.tsv' file and prepare it for data manipulation\n",
    "    the file is organized in the following way:\n",
    "    <pid>\\t<text>\\n\n",
    "    where <pid> is the passage id and <text> is the passage text\n",
    "    \"\"\"\n",
    "    df = pd.read_csv('collection.tsv', sep='\\t', header=None)\n",
    "elif chosen_collection == \"vaswani\":\n",
    "    vaswani_dataset = ir_datasets.load(\"vaswani\")\n",
    "    docs = list(vaswani_dataset.docs_iter()) # Converte in una lista di dizionari\n",
    "    df = pd.DataFrame(docs) # Crea il DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B-z1wJvFpXQ_",
    "outputId": "efebd7cb-3545-4c5e-dfee-5c6b92bab3de"
   },
   "outputs": [],
   "source": [
    "# let's not truncate Pandas output too much\n",
    "pd.set_option('display.max_colwidth', 50) # mettici 150\n",
    "df.columns = ['doc_id', 'text']\n",
    "print(df.head(5)) # returns the first N rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F3wDCYeRpYX5"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"stopwords\", quiet=True)\n",
    "\n",
    "def preprocess(s):\n",
    "    # lowercasing\n",
    "    s = s.lower()\n",
    "    # ampersand\n",
    "    s = s.replace(\"&\", \" and \")\n",
    "    # special chars\n",
    "    s = s.translate(dict([(ord(x), ord(y)) for x, y in zip(\"‘’´“”–-\", \"'''\\\"\\\"--\")]))\n",
    "    # acronyms\n",
    "    s = re.sub(r\"\\.(?!(\\S[^. ])|\\d)\", \"\", s) # remove dots that are not part of an acronym\n",
    "    # remove punctuation\n",
    "    s = s.translate(str.maketrans(string.punctuation, \" \" * len(string.punctuation)))\n",
    "    # strip whitespaces\n",
    "    s = s.strip()\n",
    "    while \"  \" in s:\n",
    "        s = s.replace(\"  \", \" \")\n",
    "    # tokeniser\n",
    "    s = s.split()\n",
    "    # stopwords\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    s = [t for t in s if t not in stopwords]\n",
    "    # stemming\n",
    "    stemmer = nltk.stem.PorterStemmer().stem\n",
    "    s = [stemmer(t) for t in s]\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vMXjjbwwpZg7"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def profile(f):\n",
    "    def f_timer(*args, **kwargs):\n",
    "        start = time.time()\n",
    "        result = f(*args, **kwargs)\n",
    "        end = time.time()\n",
    "        ms = (end - start) * 1000\n",
    "        print(f\"{f.__name__} ({ms:.3f} ms)\")\n",
    "        return result\n",
    "    return f_timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cGmR6-3EBUwA",
    "outputId": "0fe56195-635b-43db-c154-ee8c851ae46d"
   },
   "outputs": [],
   "source": [
    "def VBEncode(n):\n",
    "    byte = [] # We will store the bytes in a list\n",
    "    while True:\n",
    "        # n % 128 gives us the 7 least significant bits\n",
    "        byte.append(n % 128) # Append 7 bits with most significant bit (control bit) set to 0\n",
    "        if n < 128:\n",
    "            break\n",
    "        n //= 128 # Shift the number to the right by 7 bits\n",
    "\n",
    "    byte[0] += 128 # Set the most significant bit (control bit) set to 1\n",
    "    return byte[::-1] # We provide the resulting list in reverse order\n",
    "\n",
    "\n",
    "def VBEncodeList(n_list):\n",
    "    b = []\n",
    "    for n in n_list:\n",
    "        b.extend(VBEncode(n)) # We extend the list with the bytes of the number\n",
    "    return b\n",
    "\n",
    "\n",
    "def VBDecode(byte_list):\n",
    "    n_list = []\n",
    "    n = 0\n",
    "    for b in byte_list:\n",
    "        if b < 128:\n",
    "            n = 128 * n + b\n",
    "        else:\n",
    "            n = 128 * n + b - 128\n",
    "            n_list.append(n)\n",
    "            n = 0\n",
    "    return n_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GEMd0ALMpabE"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "@profile\n",
    "def build_index(dataset):\n",
    "    lexicon = {}\n",
    "    doc_index = []\n",
    "    inv_d, inv_f = {}, {}\n",
    "    termid = 0\n",
    "\n",
    "    num_docs = 0\n",
    "    total_dl = 0\n",
    "    total_toks = 0\n",
    "    for docid, doc in tqdm(enumerate(dataset.docs_iter()), desc='Indexing', total=dataset.docs_count()):\n",
    "        tokens = preprocess(doc.text)\n",
    "        #print(tokens)\n",
    "        token_tf = Counter(tokens)\n",
    "        for token, tf in token_tf.items():\n",
    "            if token not in lexicon:\n",
    "                lexicon[token] = [termid, 0, 0]\n",
    "                inv_d[termid], inv_f[termid] =  [], []\n",
    "                termid += 1\n",
    "            token_id = lexicon[token][0] # prendo il termid\n",
    "            inv_d[token_id].append(docid) # aggiungo il docid alla lista dei docid in cui compare il termine\n",
    "            inv_f[token_id].append(tf) # aggiungo il tf alla lista dei tf in cui compare il termine\n",
    "            lexicon[token][1] += 1 # incremento il df\n",
    "            lexicon[token][2] += tf # tf è quanto compare il termine nel documento\n",
    "        doclen = len(tokens)\n",
    "        doc_index.append((str(doc.doc_id), doclen))\n",
    "        total_dl += doclen\n",
    "        num_docs += 1\n",
    "\n",
    "\n",
    "    stats = {\n",
    "        'num_docs': 1 + docid, # docid parte da 0\n",
    "        'num_terms': len(lexicon),\n",
    "        'num_tokens': total_dl,\n",
    "    }\n",
    "    return lexicon, {'docids': inv_d, 'freqs': inv_f}, doc_index, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bG1E5wTxpbPq"
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "\n",
    "class MSMarcoDataset:\n",
    "    \"\"\"\n",
    "    This class that takes the dataframe we created before with columns 'docno' and 'text', and creates a list of namedtuples\n",
    "    \"\"\"\n",
    "    def __init__(self, df):\n",
    "        self.docs = [Document(row.doc_id, row.text) for row in df.itertuples()]\n",
    "\n",
    "    def docs_iter(self):\n",
    "        return iter(self.docs)\n",
    "\n",
    "    def docs_count(self):\n",
    "        return len(self.docs)\n",
    "\n",
    "\n",
    "Document = namedtuple('Document', ['doc_id', 'text']) # must define what a document is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ceced8dd-3e3b-4b72-a0b7-2d6d5a2c2a19"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class InvertedIndex:\n",
    "\n",
    "    class PostingListIterator:\n",
    "        def __init__(self, docids, freqs, doc):\n",
    "            self.docids = docids\n",
    "            self.freqs = freqs\n",
    "            self.pos = 0\n",
    "            self.doc = doc\n",
    "\n",
    "        def docid(self):\n",
    "            if self.is_end_list():\n",
    "                return math.inf\n",
    "            return self.docids[self.pos]\n",
    "\n",
    "        def score(self):\n",
    "            if self.is_end_list():\n",
    "                return math.inf\n",
    "            return self.freqs[self.pos]/self.doc[self.docid()][1]\n",
    "\n",
    "        def next(self, target = None):\n",
    "            if not target:\n",
    "                if not self.is_end_list():\n",
    "                    self.pos += 1\n",
    "            else:\n",
    "                if target > self.docid():\n",
    "                    try:\n",
    "                        self.pos = self.docids.index(target, self.pos)\n",
    "                    except ValueError:\n",
    "                        self.pos = len(self.docids)\n",
    "\n",
    "        def is_end_list(self):\n",
    "            return self.pos == len(self.docids)\n",
    "\n",
    "\n",
    "        def len(self):\n",
    "            return len(self.docids)\n",
    "\n",
    "\n",
    "    def __init__(self, lex, inv, doc, stats):\n",
    "        self.lexicon = lex\n",
    "        self.inv = inv\n",
    "        self.doc = doc\n",
    "        self.stat = stats\n",
    "\n",
    "    def num_docs(self):\n",
    "        return self.stats['num_docs']\n",
    "\n",
    "    def get_posting(self, termid):\n",
    "        return InvertedIndex.PostingListIterator(self.inv['docids'][termid], self.inv['freqs'][termid], self.doc)\n",
    "\n",
    "    def get_termids(self, tokens):\n",
    "        return [self.lexicon[token][0] for token in tokens if token in self.lexicon]\n",
    "\n",
    "    def get_postings(self, termids):\n",
    "        return [self.get_posting(termid) for termid in termids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now build up the index for the chosen collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MSMarcoDataset(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The index is built only if a pickled version isn't already present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "try: # try to open the pickled files, else build the index\n",
    "    with open('./lex.pkl', 'rb') as f:\n",
    "        lex = pickle.load(f)\n",
    "    with open('./inv.pkl', 'rb') as f:\n",
    "        inv = pickle.load(f)\n",
    "    with open('./doc.pkl', 'rb') as f:\n",
    "        doc = pickle.load(f)\n",
    "    with open('./stats.pkl', 'rb') as f:\n",
    "        stats = pickle.load(f)\n",
    "    print(\"Index loaded from pickles\")\n",
    "\n",
    "except:\n",
    "    lex, inv, doc, stats = build_index(dataset)\n",
    "\n",
    "    # pickle lex, inv, doc, stats\n",
    "    with open('./lex.pkl', 'wb') as f:\n",
    "        pickle.dump(lex, f)\n",
    "\n",
    "    with open('./inv.pkl','wb') as f:\n",
    "        pickle.dump(inv, f)\n",
    "\n",
    "    with open('./doc.pkl', 'wb') as f:\n",
    "        pickle.dump(doc, f)\n",
    "\n",
    "    with open('./stats.pkl', 'wb') as f:\n",
    "        pickle.dump(stats, f)\n",
    "\n",
    "    print(\"Index built and pickled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lex)\n",
    "print(inv)\n",
    "print(doc)\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have all the elements necessary to build the Inverted Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_index = InvertedIndex(lex, inv, doc, stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Download and prepare queries\n",
    "\n",
    "Aggiungiamo anche le query per il dataset scelto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YccWSsil0ZQF",
    "outputId": "937f5b1e-9a31-499e-8206-c00ab1f8568d"
   },
   "outputs": [],
   "source": [
    "if chosen_collection == \"MSMARCO\":\n",
    "    url = 'https://msmarco.z22.web.core.windows.net/msmarcoranking/queries.tar.gz'\n",
    "    gdown.download(url, 'queries.tar.gz', quiet=False)\n",
    "    !tar -xzf queries.tar.gz\n",
    "    queries = pd.read_csv('queries.eval.tsv', sep='\\t', header=None)\n",
    "    print(\"Number of queries: \",len(queries))\n",
    "elif chosen_collection == \"vaswani\":\n",
    "    # Converte le query in un DataFrame\n",
    "    queries = pd.DataFrame(vaswani_dataset.queries_iter())\n",
    "    queries.columns = ['qid', 'query']\n",
    "    print(queries.head(2))\n",
    "    print(\"Number of queries: \",len(list(vaswani_dataset.queries_iter()))) \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eyRIr9uF9-6M"
   },
   "outputs": [],
   "source": [
    "class MSMarcoQueries:\n",
    "    def __init__(self, df):\n",
    "        self.queries = [Query(row.query_id, row.text) for row in df.itertuples()]\n",
    "\n",
    "    def queries_iter(self):\n",
    "        return iter(self.queries)\n",
    "\n",
    "    def queries_count(self):\n",
    "        return len(self.queries)\n",
    "\n",
    "\n",
    "Query = namedtuple('Query', ['query_id', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uBSQzl6j3W-E"
   },
   "outputs": [],
   "source": [
    "queries.columns = ['query_id', 'text']\n",
    "queries_dataset = MSMarcoQueries(queries)\n",
    "print(\"The number of queries is: \", queries_dataset.queries_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's prepare the functions necessary to perform TAAT and DAAT query processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need a TopQueue class, which stores the top  K  (score, docid) tuples, using an heap "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "\n",
    "class TopQueue:\n",
    "    def __init__(self, k=10, threshold=0.0):\n",
    "        self.queue = []\n",
    "        self.k = k\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.queue)\n",
    "\n",
    "    def would_enter(self, score):\n",
    "        return score > self.threshold\n",
    "\n",
    "    def clear(self, new_threshold=None):\n",
    "        self.queue = []\n",
    "        if new_threshold:\n",
    "            self.threshold = new_threshold\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'<{self.size()} items, th={self.threshold} {self.queue}'\n",
    "\n",
    "    def insert(self, docid, score):\n",
    "        if score > self.threshold:\n",
    "            if self.size() >= self.k:\n",
    "                heapq.heapreplace(self.queue, (score, docid))\n",
    "            else:\n",
    "                heapq.heappush(self.queue, (score, docid))\n",
    "            if self.size() >= self.k:\n",
    "                self.threshold = max(self.threshold, self.queue[0][0])\n",
    "            return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TAAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def taat(postings, k=10):\n",
    "    A = defaultdict(float)\n",
    "    for posting in postings:\n",
    "        current_docid = posting.docid()\n",
    "        while current_docid != math.inf:\n",
    "            A[current_docid] += posting.score()\n",
    "            posting.next()\n",
    "            current_docid = posting.docid()\n",
    "    top = TopQueue(k)\n",
    "    for docid, score in A.items():\n",
    "        top.insert(docid, score)\n",
    "    return sorted(top.queue, reverse=True)\n",
    "\n",
    "def query_process(query, index):\n",
    "    qtokens = set(preprocess(query))\n",
    "    qtermids = index.get_termids(qtokens)\n",
    "    postings = index.get_postings(qtermids)\n",
    "    return taat(postings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DAAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def min_docid(postings):\n",
    "    min_docid = math.inf\n",
    "    for p in postings:\n",
    "        if not p.is_end_list():\n",
    "            min_docid = min(p.docid(), min_docid)\n",
    "    return min_docid\n",
    "\n",
    "def daat(postings, k=10):\n",
    "    top = TopQueue(k)\n",
    "    current_docid = min_docid(postings)\n",
    "    while current_docid != math.inf:\n",
    "        score = 0\n",
    "        next_docid = math.inf\n",
    "        for posting in postings:\n",
    "            if posting.docid() == current_docid:\n",
    "                score += posting.score()\n",
    "                posting.next()\n",
    "            if not posting.is_end_list():\n",
    "                next_docid = posting.docid()\n",
    "        top.insert(current_docid, score)\n",
    "        current_docid = next_docid\n",
    "    return sorted(top.queue, reverse=True)\n",
    "\n",
    "def query_process(query, index):\n",
    "    qtokens = set(preprocess(query))\n",
    "    qtermids = index.get_termids(qtokens)\n",
    "    postings = index.get_postings(qtermids)\n",
    "    return daat(postings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u-UQcCcTkNo-"
   },
   "outputs": [],
   "source": [
    "@profile\n",
    "def query_processing(queries_iter, fn):\n",
    "    for q in queries_iter:\n",
    "        query = preprocess(q.text)\n",
    "        termids = inv_index.get_termids(query)\n",
    "        postings = inv_index.get_postings(termids)\n",
    "        res = fn(postings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 294
    },
    "id": "1Nn02ZlSkiUP",
    "outputId": "76de8ac6-4d0e-4e28-ecc7-7ddbbd11b9de"
   },
   "outputs": [],
   "source": [
    "query_processing(queries_dataset.queries_iter(), taat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KuEay64mkrR1"
   },
   "outputs": [],
   "source": [
    "query_processing(queries_dataset.queries_iter(), daat)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "mircv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "17ec15452b0a435bb1d4040d6b8c8b85": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2159ab6a19334335ad145938f874bbbb",
      "placeholder": "​",
      "style": "IPY_MODEL_29ea010f911e48a3b0550a2b21d63345",
      "value": " 3/3 [00:00&lt;00:00, 85.49it/s]"
     }
    },
    "2159ab6a19334335ad145938f874bbbb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "292fd00a1b524da2a79f5273768e06d6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "29ea010f911e48a3b0550a2b21d63345": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3ec2e40f9cc34efab5544a7727b0686a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b810509205ea45f699bd3848f998a187",
       "IPY_MODEL_78a09b740453455a99e14d6109919c52",
       "IPY_MODEL_17ec15452b0a435bb1d4040d6b8c8b85"
      ],
      "layout": "IPY_MODEL_292fd00a1b524da2a79f5273768e06d6"
     }
    },
    "78a09b740453455a99e14d6109919c52": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8f264f8a61924e879806974dcd2103c8",
      "max": 3,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_aba07292495142c4be08ede74d9b7637",
      "value": 3
     }
    },
    "831e8fd443684346ad096bdd300657d7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8f264f8a61924e879806974dcd2103c8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aba07292495142c4be08ede74d9b7637": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b810509205ea45f699bd3848f998a187": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e030f29b2cf348d3a2f29aef2a0e5fcb",
      "placeholder": "​",
      "style": "IPY_MODEL_831e8fd443684346ad096bdd300657d7",
      "value": "Indexing: 100%"
     }
    },
    "e030f29b2cf348d3a2f29aef2a0e5fcb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
