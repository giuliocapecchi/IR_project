{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/giuliocapecchi/IR_project/blob/main/IR_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iNk06jLCfXXM"
      },
      "outputs": [],
      "source": [
        "!pip install torch matplotlib nltk tqdm gdown"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckU92ZwXFBSe"
      },
      "source": [
        "C'è da discutere di come caricare la collection sul notebook. Ho trovato due approcci:\n",
        "\n",
        "## 1. Utilizzare drive.mount\n",
        "\n",
        "Il problema di questo approccio è che la location del file può cambiare da utente ad utente e il file andrebbe condiviso con Tonellotto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7F6euyNneS0t"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "%cd /content/drive/MyDrive/MIRCV/IR_project/'MSMARCO Passages collection'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1f7eJ4tFSfq"
      },
      "source": [
        "## 2. utilizzare gdown\n",
        "\n",
        "Con questo modulo si possono scaricare files, quindi ho scaricato la collection e l'ho butttata sul mio drive (ci vogliono circa 30s/1 minuto di tempo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "8oKwHN5yEPTq",
        "outputId": "0417dae5-b346-4f2e-ee0c-f1dc2f31b287"
      },
      "outputs": [],
      "source": [
        "import gdown\n",
        "\n",
        "url = 'https://drive.google.com/uc?id=1_wXJjiwdgc9Kpt7o7atP8oWe-U4Z56hn'\n",
        "gdown.download(url, 'test.tsv', quiet=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "blWrsr8QEzrC"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('test.tsv', sep='\\t', header=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZUxKK6rFKxT",
        "outputId": "828a0bbd-2a19-4e11-b340-0bb23be15276"
      },
      "outputs": [],
      "source": [
        "print(df.head(1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUlmccoZF4v5"
      },
      "source": [
        "In ogni caso dobbiamo:\n",
        "\n",
        "- trovare una collection più piccola per cominciare (ok anche vaswani secondo me)\n",
        "- realizzare una copia in locale di questo notebook, non si può tutte le volte aspettare di riscaricare 3GB di collection (in realtà anche si perchè ci mette 30s/1 minuto al download).\n",
        "\n",
        "\n",
        "La copia locale forse la farei in un .py normale come suggerito da tonellotto (vedi repo mio github)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DL3XqoGEg8Qs"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZpA2jo08hW7V"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "read 'collection.tsv' file and prepare it for data manipulation\n",
        "the file is organized in the following way:\n",
        "<pid>\\t<text>\\n \n",
        "where <pid> is the passage id and <text> is the passage text\n",
        "\"\"\"\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('collection.tsv', sep='\\t', header=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# let's not truncate Pandas output too much\n",
        "pd.set_option('display.max_colwidth', 50) # mettici 150\n",
        "df.columns = ['doc_id', 'text']\n",
        "print(df.head(2)) # returns the first N rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "x-SIdQx-g93h"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import string\n",
        "import nltk\n",
        "\n",
        "nltk.download(\"stopwords\", quiet=True)\n",
        "\n",
        "def preprocess(s):\n",
        "    # lowercasing\n",
        "    s = s.lower()\n",
        "    # ampersand\n",
        "    s = s.replace(\"&\", \" and \")\n",
        "    # special chars\n",
        "    s = s.translate(dict([(ord(x), ord(y)) for x, y in zip(\"‘’´“”–-\", \"'''\\\"\\\"--\")]))\n",
        "    # acronyms\n",
        "    s = re.sub(r\"\\.(?!(\\S[^. ])|\\d)\", \"\", s) # remove dots that are not part of an acronym\n",
        "    # remove punctuation\n",
        "    s = s.translate(str.maketrans(string.punctuation, \" \" * len(string.punctuation)))\n",
        "    # strip whitespaces\n",
        "    s = s.strip()\n",
        "    while \"  \" in s:\n",
        "        s = s.replace(\"  \", \" \")\n",
        "    # tokeniser\n",
        "    s = s.split()\n",
        "    # stopwords\n",
        "    stopwords = nltk.corpus.stopwords.words('english')\n",
        "    s = [t for t in s if t not in stopwords]\n",
        "    # stemming\n",
        "    stemmer = nltk.stem.PorterStemmer().stem\n",
        "    s = [stemmer(t) for t in s]\n",
        "    return s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "ix3btYvdhVH9"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "def profile(f):\n",
        "    def f_timer(*args, **kwargs):\n",
        "        start = time.time()\n",
        "        result = f(*args, **kwargs)\n",
        "        end = time.time()\n",
        "        ms = (end - start) * 1000\n",
        "        print(f\"{f.__name__} ({ms:.3f} ms)\")\n",
        "        return result\n",
        "    return f_timer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "@profile\n",
        "def build_index(dataset):\n",
        "    lexicon = {}\n",
        "    doc_index = [] \n",
        "    inv_d, inv_f = {}, {}\n",
        "    termid = 0\n",
        "\n",
        "    num_docs = 0\n",
        "    total_dl = 0\n",
        "    total_toks = 0\n",
        "    for docid, doc in tqdm(enumerate(dataset.docs_iter()), desc='Indexing', total=dataset.docs_count()):\n",
        "        tokens = preprocess(doc.text)\n",
        "        #print(tokens)\n",
        "        token_tf = Counter(tokens)\n",
        "        for token, tf in token_tf.items():\n",
        "            if token not in lexicon:\n",
        "                lexicon[token] = [termid, 0, 0]\n",
        "                inv_d[termid], inv_f[termid] =  [], []\n",
        "                termid += 1\n",
        "            token_id = lexicon[token][0] # prendo il termid\n",
        "            inv_d[token_id].append(docid) # aggiungo il docid alla lista dei docid in cui compare il termine\n",
        "            inv_f[token_id].append(tf) # aggiungo il tf alla lista dei tf in cui compare il termine\n",
        "            lexicon[token][1] += 1 # incremento il df\n",
        "            lexicon[token][2] += tf # tf è quanto compare il termine nel documento\n",
        "        doclen = len(tokens)\n",
        "        doc_index.append((str(doc.doc_id), doclen))\n",
        "        total_dl += doclen                         \n",
        "        num_docs += 1\n",
        "        \n",
        "\n",
        "    stats = {\n",
        "        'num_docs': 1 + docid, # docid parte da 0\n",
        "        'num_terms': len(lexicon),\n",
        "        'num_tokens': total_dl,\n",
        "    }\n",
        "    return lexicon, {'docids': inv_d, 'freqs': inv_f}, doc_index, stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "This class that takes the dataframe we created before with columns 'docno' and 'text', and creates a list of namedtuples\n",
        "\"\"\"\n",
        "from collections import namedtuple\n",
        "\n",
        "\n",
        "class MSMarcoDataset:\n",
        "    def __init__(self, df):\n",
        "        self.docs = [Document(row.doc_id, row.text) for row in df.itertuples()]\n",
        "\n",
        "    def docs_iter(self):\n",
        "        return iter(self.docs)\n",
        "\n",
        "    def docs_count(self):\n",
        "        return len(self.docs)\n",
        "    \n",
        "\n",
        "Document = namedtuple('Document', ['doc_id', 'text']) # must define what a document is"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Document(doc_id=1, text='A school')\n",
            "Document(doc_id=2, text='Another example.')\n",
            "Document(doc_id=3, text='This is a house.')\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ca5ebd39aa46457abe6f9c42f0ab1333",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Indexing:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "build_index (12.007 ms)\n"
          ]
        }
      ],
      "source": [
        "# Test the MSMarcoDataset class by passing Document(1, \"school\"), Document(2, \"example.\"), Document(3, \"house.\")\n",
        "\n",
        "test_docs = [Document(1, \"A school\"), Document(2, \"Another example.\"), Document(3, \"This is a house.\")]\n",
        "test_dataset = MSMarcoDataset(pd.DataFrame(test_docs, columns=['doc_id', 'text']))\n",
        "\n",
        "for doc in test_dataset.docs_iter():\n",
        "    print(doc)\n",
        "\n",
        "lex, inv, doc, stats = build_index(test_dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'school': [0, 1, 1], 'anoth': [1, 1, 1], 'exampl': [2, 1, 1], 'hous': [3, 1, 1]}\n",
            "{'docids': {0: [0], 1: [1], 2: [1], 3: [2]}, 'freqs': {0: [1], 1: [1], 2: [1], 3: [1]}}\n",
            "[('1', 1), ('2', 2), ('3', 1)]\n",
            "{'num_docs': 3, 'num_terms': 4, 'num_tokens': 4}\n"
          ]
        }
      ],
      "source": [
        "print(lex)\n",
        "print(inv)\n",
        "print(doc)\n",
        "print(stats)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3e6697be00c54f0e81546b9b0d6cd8f5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Indexing:   0%|          | 0/10 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "build_index (18.219 ms)\n"
          ]
        }
      ],
      "source": [
        "# create a df with the first 10 rows\n",
        "df = df.head(10) # TODO : REMOVE THIS\n",
        "\n",
        "dataset = MSMarcoDataset(df)\n",
        "lex, inv, doc, stats = build_index(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'presenc': [0, 1, 1], 'commun': [1, 1, 1], 'amid': [2, 1, 1], 'scientif': [3, 1, 2], 'mind': [4, 1, 1], 'equal': [5, 1, 1], 'import': [6, 1, 1], 'success': [7, 2, 3], 'manhattan': [8, 10, 13], 'project': [9, 10, 18], 'intellect': [10, 1, 1], 'cloud': [11, 1, 1], 'hang': [12, 1, 1], 'impress': [13, 1, 1], 'achiev': [14, 1, 1], 'atom': [15, 7, 9], 'research': [16, 2, 2], 'engin': [17, 3, 3], 'truli': [18, 1, 1], 'meant': [19, 1, 1], 'hundr': [20, 1, 1], 'thousand': [21, 1, 1], 'innoc': [22, 1, 1], 'live': [23, 1, 1], 'obliter': [24, 1, 1], 'bomb': [25, 7, 7], 'help': [26, 1, 1], 'bring': [27, 1, 1], 'end': [28, 2, 2], 'world': [29, 5, 5], 'war': [30, 4, 4], 'ii': [31, 4, 4], 'legaci': [32, 1, 1], 'peac': [33, 1, 1], 'use': [34, 1, 1], 'energi': [35, 1, 1], 'continu': [36, 1, 1], 'impact': [37, 1, 1], 'histori': [38, 2, 4], 'scienc': [39, 1, 1], 'essay': [40, 1, 1], 'see': [41, 1, 1], 'make': [42, 1, 2], 'possibl': [43, 1, 1], 'would': [44, 1, 1], 'forev': [45, 1, 2], 'chang': [46, 1, 1], 'known': [47, 1, 1], 'someth': [48, 1, 1], 'power': [49, 1, 1], 'manmad': [50, 1, 1], 'name': [51, 2, 2], 'conduct': [52, 1, 1], 'develop': [53, 3, 3], 'first': [54, 4, 4], 'refer': [55, 1, 1], 'specif': [56, 1, 1], 'period': [57, 1, 1], '194': [58, 1, 1], 'â\\x80¦': [59, 1, 1], '2': [60, 2, 2], '1946': [61, 2, 2], 'control': [62, 1, 1], 'us': [63, 2, 2], 'armi': [64, 3, 4], 'corp': [65, 3, 3], 'administr': [66, 1, 1], 'gener': [67, 2, 2], 'lesli': [68, 2, 2], 'r': [69, 1, 1], 'grove': [70, 2, 2], 'version': [71, 1, 1], 'volum': [72, 1, 1], 'well': [73, 1, 1], 'complementari': [74, 1, 1], 'websit': [75, 1, 2], 'websiteâ\\x80\\x93th': [76, 1, 1], 'interact': [77, 1, 1], 'historyâ\\x80\\x93i': [78, 1, 1], 'avail': [79, 1, 1], 'offic': [80, 1, 2], 'heritag': [81, 1, 2], 'resourc': [82, 1, 2], 'http': [83, 1, 1], 'www': [84, 1, 1], 'cfo': [85, 1, 1], 'doe': [86, 1, 1], 'gov': [87, 1, 1], 'me70': [88, 1, 1], 'nation': [89, 1, 1], 'nuclear': [90, 3, 4], 'secur': [91, 1, 1], 'classifi': [92, 1, 1], 'photograph': [93, 1, 1], 'featur': [94, 1, 1], 'â\\x80\\x94': [95, 1, 1], 'weapon': [96, 2, 2], 'scientist': [97, 1, 1], 'nicknam': [98, 1, 1], 'gadget': [99, 1, 1], 'age': [100, 1, 1], 'began': [101, 1, 1], 'juli': [102, 1, 1], '16': [103, 1, 1], '1945': [104, 1, 1], 'deton': [105, 1, 1], 'new': [106, 1, 1], 'mexico': [107, 1, 1], 'desert': [108, 1, 1], 'attempt': [109, 1, 2], 'substitut': [110, 1, 1], 'extraordinarili': [111, 1, 1], 'rich': [112, 1, 1], 'literatur': [113, 1, 1], 'collect': [114, 1, 1], 'document': [115, 1, 1], 'origin': [116, 1, 1], 'undertak': [117, 1, 1], 'produc': [118, 1, 1], 'led': [119, 1, 1], 'unit': [120, 2, 3], 'state': [121, 2, 2], 'support': [122, 1, 1], 'kingdom': [123, 1, 1], 'canada': [124, 1, 1], '1942': [125, 2, 2], 'direct': [126, 1, 1], 'major': [127, 1, 1], 'physicist': [128, 1, 1], 'robert': [129, 1, 1], 'oppenheim': [130, 1, 1], 'director': [131, 1, 1], 'lo': [132, 1, 1], 'alamo': [133, 1, 1], 'laboratori': [134, 1, 1], 'design': [135, 1, 2], 'actual': [136, 1, 1], 'compon': [137, 1, 1], 'june': [138, 1, 1], 'engineersbegan': [139, 1, 1], 'secret': [140, 1, 1], 'one': [141, 1, 1], 'main': [142, 1, 1], 'reason': [143, 1, 1], 'hanford': [144, 1, 1], 'select': [145, 1, 1], 'site': [146, 1, 1], 'b': [147, 1, 1], 'reactor': [148, 1, 1], 'proxim': [149, 1, 1], 'columbia': [150, 1, 1], 'river': [151, 1, 2], 'largest': [152, 1, 1], 'flow': [153, 1, 1], 'pacif': [154, 1, 1], 'ocean': [155, 1, 1], 'north': [156, 1, 1], 'american': [157, 1, 1], 'coast': [158, 1, 1]}\n",
            "{'docids': {0: [0], 1: [0], 2: [0], 3: [0], 4: [0], 5: [0], 6: [0], 7: [0, 2], 8: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], 9: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], 10: [0], 11: [0], 12: [0], 13: [0], 14: [0], 15: [0, 1, 2, 3, 5, 6, 8], 16: [0, 7], 17: [0, 3, 7], 18: [0], 19: [0], 20: [0], 21: [0], 22: [0], 23: [0], 24: [0], 25: [1, 2, 3, 5, 6, 7, 8], 26: [1], 27: [1], 28: [1, 6], 29: [1, 2, 3, 6, 7], 30: [1, 3, 6, 7], 31: [1, 3, 6, 7], 32: [1], 33: [1], 34: [1], 35: [1], 36: [1], 37: [1], 38: [1, 4], 39: [1], 40: [2], 41: [2], 42: [2], 43: [2], 44: [2], 45: [2], 46: [2], 47: [2], 48: [2], 49: [2], 50: [2], 51: [3, 8], 52: [3], 53: [3, 6, 7], 54: [3, 4, 5, 7], 55: [3], 56: [3], 57: [3], 58: [3], 59: [3], 60: [3, 8], 61: [3, 7], 62: [3], 63: [3, 7], 64: [3, 7, 8], 65: [3, 7, 8], 66: [3], 67: [3, 7], 68: [3, 7], 69: [3], 70: [3, 7], 71: [4], 72: [4], 73: [4], 74: [4], 75: [4], 76: [4], 77: [4], 78: [4], 79: [4], 80: [4], 81: [4], 82: [4], 83: [4], 84: [4], 85: [4], 86: [4], 87: [4], 88: [4], 89: [4], 90: [4, 5, 7], 91: [4], 92: [5], 93: [5], 94: [5], 95: [5], 96: [5, 7], 97: [5], 98: [5], 99: [5], 100: [5], 101: [5], 102: [5], 103: [5], 104: [5], 105: [5], 106: [5], 107: [5], 108: [5], 109: [6], 110: [6], 111: [6], 112: [6], 113: [6], 114: [6], 115: [6], 116: [6], 117: [7], 118: [7], 119: [7], 120: [7, 8], 121: [7, 8], 122: [7], 123: [7], 124: [7], 125: [7, 8], 126: [7], 127: [7], 128: [7], 129: [7], 130: [7], 131: [7], 132: [7], 133: [7], 134: [7], 135: [7], 136: [7], 137: [7], 138: [8], 139: [8], 140: [8], 141: [9], 142: [9], 143: [9], 144: [9], 145: [9], 146: [9], 147: [9], 148: [9], 149: [9], 150: [9], 151: [9], 152: [9], 153: [9], 154: [9], 155: [9], 156: [9], 157: [9], 158: [9]}, 'freqs': {0: [1], 1: [1], 2: [1], 3: [2], 4: [1], 5: [1], 6: [1], 7: [2, 1], 8: [1, 1, 3, 1, 1, 1, 1, 2, 1, 1], 9: [1, 1, 4, 3, 1, 1, 1, 4, 1, 1], 10: [1], 11: [1], 12: [1], 13: [1], 14: [1], 15: [1, 2, 1, 1, 2, 1, 1], 16: [1, 1], 17: [1, 1, 1], 18: [1], 19: [1], 20: [1], 21: [1], 22: [1], 23: [1], 24: [1], 25: [1, 1, 1, 1, 1, 1, 1], 26: [1], 27: [1], 28: [1, 1], 29: [1, 1, 1, 1, 1], 30: [1, 1, 1, 1], 31: [1, 1, 1, 1], 32: [1], 33: [1], 34: [1], 35: [1], 36: [1], 37: [1], 38: [1, 3], 39: [1], 40: [1], 41: [1], 42: [2], 43: [1], 44: [1], 45: [2], 46: [1], 47: [1], 48: [1], 49: [1], 50: [1], 51: [1, 1], 52: [1], 53: [1, 1, 1], 54: [1, 1, 1, 1], 55: [1], 56: [1], 57: [1], 58: [1], 59: [1], 60: [1, 1], 61: [1, 1], 62: [1], 63: [1, 1], 64: [1, 2, 1], 65: [1, 1, 1], 66: [1], 67: [1, 1], 68: [1, 1], 69: [1], 70: [1, 1], 71: [1], 72: [1], 73: [1], 74: [1], 75: [2], 76: [1], 77: [1], 78: [1], 79: [1], 80: [2], 81: [2], 82: [2], 83: [1], 84: [1], 85: [1], 86: [1], 87: [1], 88: [1], 89: [1], 90: [1, 1, 2], 91: [1], 92: [1], 93: [1], 94: [1], 95: [1], 96: [1, 1], 97: [1], 98: [1], 99: [1], 100: [1], 101: [1], 102: [1], 103: [1], 104: [1], 105: [1], 106: [1], 107: [1], 108: [1], 109: [2], 110: [1], 111: [1], 112: [1], 113: [1], 114: [1], 115: [1], 116: [1], 117: [1], 118: [1], 119: [1], 120: [2, 1], 121: [1, 1], 122: [1], 123: [1], 124: [1], 125: [1, 1], 126: [1], 127: [1], 128: [1], 129: [1], 130: [1], 131: [1], 132: [1], 133: [1], 134: [1], 135: [2], 136: [1], 137: [1], 138: [1], 139: [1], 140: [1], 141: [1], 142: [1], 143: [1], 144: [1], 145: [1], 146: [1], 147: [1], 148: [1], 149: [1], 150: [1], 151: [2], 152: [1], 153: [1], 154: [1], 155: [1], 156: [1], 157: [1], 158: [1]}}\n",
            "[('0', 27), ('1', 19), ('2', 24), ('3', 30), ('4', 31), ('5', 24), ('6', 18), ('7', 48), ('8', 14), ('9', 21)]\n",
            "{'num_docs': 10, 'num_terms': 159, 'num_tokens': 256}\n"
          ]
        }
      ],
      "source": [
        "print(lex)\n",
        "print(inv)\n",
        "print(doc)\n",
        "print(stats)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "mircv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
